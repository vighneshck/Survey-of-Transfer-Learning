{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SML_Code_1103842.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ReMxEyujO7yD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVBOtRdwpJyV",
        "colab_type": "text"
      },
      "source": [
        "### Setting up colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxumBzx2pCl-",
        "colab_type": "code",
        "outputId": "c073b21d-c879-42be-e52f-26f22815cc7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8lhcTh1pN2N",
        "colab_type": "code",
        "outputId": "9b7e0825-390f-4e38-a202-b1c5f32db613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My Drive/SML/Project2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SML/Project2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYu9-4wppN_0",
        "colab_type": "code",
        "outputId": "15c8638e-1f43-4d70-fdf2-5540bdb57ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['FEMALE.csv',\n",
              " 'MALE.csv',\n",
              " 'MIXED.csv',\n",
              " 'README.md',\n",
              " 'SML_Code_1103842_Copy.ipynb',\n",
              " 'SML_Code_1103842.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcwTjhmNjJw7",
        "colab_type": "text"
      },
      "source": [
        "### Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op-BTNXAwbFC",
        "colab_type": "code",
        "outputId": "bba3c29c-de97-4090-d2e6-1549dbd630c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install xgboost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2As8G_pOfxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "#from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from statistics import mean\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzrVEkKXjWd3",
        "colab_type": "text"
      },
      "source": [
        "### Reading csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqhcx4RijYhE",
        "colab_type": "code",
        "outputId": "c708784f-6044-4601-f74a-a46ca210b046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "male = pd.read_csv('MALE.csv')\n",
        "female = pd.read_csv('FEMALE.csv')\n",
        "mixed = pd.read_csv('MIXED.csv')\n",
        "mixed = mixed[mixed['VR Band of Student']!=0].reset_index(drop=True)\n",
        "#mixed.drop(mixed.loc[mixed['VR Band of Student']==0].index, inplace = True)\n",
        "#mixed.drop(mixed[mixed['VR Band of Student']==0].index, inplace = True)\n",
        "print(male.shape)\n",
        "print(female.shape)\n",
        "print(mixed.shape)\n",
        "\n",
        "cat_columns = [\"Year\", \"VR Band of Student\", \"Ethnic group of student\", \"School denomination\"]\n",
        "male = pd.get_dummies(male, prefix_sep=\"_\", columns=cat_columns)\n",
        "female = pd.get_dummies(female, prefix_sep=\"_\", columns=cat_columns)\n",
        "mixed = pd.get_dummies(mixed, prefix_sep=\"_\", columns=cat_columns)\n",
        "print(male.head())\n",
        "print(female.head())\n",
        "print(mixed.head())\n",
        "print(male.shape)\n",
        "print(female.shape)\n",
        "print(mixed.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3654, 7)\n",
            "(4404, 7)\n",
            "(7289, 7)\n",
            "   FSM  VR1 Band  ...  School denomination_2  School denomination_3\n",
            "0   24        21  ...                      1                      0\n",
            "1   24        21  ...                      1                      0\n",
            "2   24        21  ...                      1                      0\n",
            "3   24        21  ...                      1                      0\n",
            "4   24        21  ...                      1                      0\n",
            "\n",
            "[5 rows x 23 columns]\n",
            "   FSM  VR1 Band  ...  School denomination_2  School denomination_3\n",
            "0   63        15  ...                      0                      0\n",
            "1   63        15  ...                      0                      0\n",
            "2   63        15  ...                      0                      0\n",
            "3   63        15  ...                      0                      0\n",
            "4   63        15  ...                      0                      0\n",
            "\n",
            "[5 rows x 23 columns]\n",
            "   FSM  VR1 Band  ...  School denomination_2  School denomination_3\n",
            "0   24        18  ...                      0                      0\n",
            "1   24        18  ...                      0                      0\n",
            "2   24        18  ...                      0                      0\n",
            "3   24        18  ...                      0                      0\n",
            "4   24        18  ...                      0                      0\n",
            "\n",
            "[5 rows x 23 columns]\n",
            "(3654, 23)\n",
            "(4404, 23)\n",
            "(7289, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZObaflR8816K",
        "colab_type": "code",
        "outputId": "c1263d7d-4c70-4ebe-ffb2-4f36d94ece3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "feat_col = list(mixed.columns)\n",
        "feat_col.remove('Exam Score')\n",
        "print(feat_col)\n",
        "print(len(feat_col))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['FSM', 'VR1 Band', 'Year_1', 'Year_2', 'Year_3', 'VR Band of Student_1', 'VR Band of Student_2', 'VR Band of Student_3', 'Ethnic group of student_1', 'Ethnic group of student_2', 'Ethnic group of student_3', 'Ethnic group of student_4', 'Ethnic group of student_5', 'Ethnic group of student_6', 'Ethnic group of student_7', 'Ethnic group of student_8', 'Ethnic group of student_9', 'Ethnic group of student_10', 'Ethnic group of student_11', 'School denomination_1', 'School denomination_2', 'School denomination_3']\n",
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBFrPeLGjaWp",
        "colab_type": "text"
      },
      "source": [
        "### Data partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqG1fj1Sjcz2",
        "colab_type": "code",
        "outputId": "3291f27a-ba52-46ef-e586-05f50907a47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "shuffled = male.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "male_dev, male_test, male_train = np.array_split(shuffled, [100, 200])\n",
        "male_test = male_test.reset_index(drop=True)\n",
        "male_train = male_train.reset_index(drop=True)\n",
        "male_tgt = male_train.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "print(male_train.shape)\n",
        "male_train_pred = male_train.loc[:, 'Exam Score']\n",
        "print(male_train_pred.shape)\n",
        "male_train = male_train.loc[:, feat_col]\n",
        "print(male_train.shape)\n",
        "male_dev_pred = male_dev.loc[:, 'Exam Score']\n",
        "male_dev = male_dev.loc[:, feat_col]\n",
        "male_test_pred = male_test.loc[:, 'Exam Score']\n",
        "male_test = male_test.loc[:, feat_col]\n",
        "male_tgt_pred = male_tgt.loc[:, 'Exam Score']\n",
        "male_tgt = male_tgt.loc[:, feat_col]\n",
        "\"\"\"\n",
        "print(type(male_train))\n",
        "print(male_train.head())\n",
        "print(male_train.shape)\n",
        "print(male_dev.shape)\n",
        "print(male_test.shape)\n",
        "print(gar.shape)\n",
        "\"\"\"\n",
        "\n",
        "shuffled = female.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "female_dev, female_test, female_train = np.array_split(shuffled, [100, 200])\n",
        "female_test = female_test.reset_index(drop=True)\n",
        "female_train = female_train.reset_index(drop=True)\n",
        "female_tgt = female_train.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "print(female_train.shape)\n",
        "female_train_pred = female_train.loc[:, 'Exam Score']\n",
        "print(female_train_pred.shape)\n",
        "female_train = female_train.loc[:, feat_col]\n",
        "print(female_train.shape)\n",
        "female_dev_pred = female_dev.loc[:, 'Exam Score']\n",
        "female_dev = female_dev.loc[:, feat_col]\n",
        "female_test_pred = female_test.loc[:, 'Exam Score']\n",
        "female_test = female_test.loc[:, feat_col]\n",
        "female_tgt_pred = female_tgt.loc[:, 'Exam Score']\n",
        "female_tgt = female_tgt.loc[:, feat_col]\n",
        "\"\"\"\n",
        "print(female_train.head())\n",
        "print(female_train.shape)\n",
        "print(female_dev.shape)\n",
        "print(female_test.shape)\n",
        "print(gar.shape)\n",
        "\"\"\"\n",
        "\n",
        "shuffled = mixed.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mixed_dev, mixed_test, mixed_train = np.array_split(shuffled, [100, 200])\n",
        "mixed_test = mixed_test.reset_index(drop=True)\n",
        "mixed_train = mixed_train.reset_index(drop=True)\n",
        "mixed_tgt = mixed_train.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "print(mixed_train.shape)\n",
        "mixed_train_pred = mixed_train.loc[:, 'Exam Score']\n",
        "print(mixed_train_pred.shape)\n",
        "mixed_train = mixed_train.loc[:, feat_col]\n",
        "print(mixed_train.shape)\n",
        "mixed_dev_pred = mixed_dev.loc[:, 'Exam Score']\n",
        "mixed_dev = mixed_dev.loc[:, feat_col]\n",
        "mixed_test_pred = mixed_test.loc[:, 'Exam Score']\n",
        "mixed_test = mixed_test.loc[:, feat_col]\n",
        "mixed_tgt_pred = mixed_tgt.loc[:, 'Exam Score']\n",
        "mixed_tgt = mixed_tgt.loc[:, feat_col]\n",
        "\"\"\"\n",
        "print(mixed_train.head())\n",
        "print(mixed_train.shape)\n",
        "print(mixed_dev.shape)\n",
        "print(mixed_test.shape)\n",
        "\"\"\"\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3454, 23)\n",
            "(3454,)\n",
            "(3454, 22)\n",
            "(4204, 23)\n",
            "(4204,)\n",
            "(4204, 22)\n",
            "(7089, 23)\n",
            "(7089,)\n",
            "(7089, 22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(mixed_train.head())\\nprint(mixed_train.shape)\\nprint(mixed_dev.shape)\\nprint(mixed_test.shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4rFJyj-gbU2",
        "colab_type": "code",
        "outputId": "d534cb8a-5a07-4349-8826-8271a5c0edd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\"\n",
        "print(male_dev.head())\n",
        "dev = male_dev.copy().reset_index(drop=True)\n",
        "print(dev.head())\n",
        "print(dev.loc[:, 'Year':'School denomination'].head())\n",
        "print(dev.loc[:, 'Exam Score'].head())\n",
        "print(len(dev))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\\nprint(male_dev.head())\\ndev = male_dev.copy().reset_index(drop=True)\\nprint(dev.head())\\nprint(dev.loc[:, \\'Year\\':\\'School denomination\\'].head())\\nprint(dev.loc[:, \\'Exam Score\\'].head())\\nprint(len(dev))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUi_JcHKkGx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['background-color: yellow' if v else '' for v in is_min]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReMxEyujO7yD",
        "colab_type": "text"
      },
      "source": [
        "### Task 1.1 (Implement all 6 Baseline methods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mW0rHzO_eP",
        "colab_type": "text"
      },
      "source": [
        "### SRCONLY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qnWyqyXPqTa",
        "colab_type": "code",
        "outputId": "5d81266e-15ba-43f8-f371-9b881a50efc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 128, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 128, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "ma_train = pd.concat([female_train, mixed_train], ignore_index=True)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([female_train_pred, mixed_train_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([male_train, mixed_train], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([male_train_pred, mixed_train_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([male_train, female_train], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([male_train_pred, female_train_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred]}\n",
        "src_first_dev = []\n",
        "src_second_dev = []\n",
        "src_first_test = []\n",
        "src_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "  linr = LinearRegression() #normalize=True\n",
        "  linr.fit(X_train, Y_train)\n",
        "  src_first_dev.append(mean_squared_error(Y_dev, linr.predict(X_dev)))\n",
        "  src_first_test.append(mean_squared_error(Y_test, linr.predict(X_test)))\n",
        "  \n",
        "  mlp = MLPRegressor(activation='relu', alpha=0.0001, batch_size=128, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30), (25,25,25,25)], 'activation':['relu', 'identity'], 'alpha':[0.0001, 0.01, 0.1, 1, 10], 'batch_size':[200, 128, 64, 32], 'max_iter':[1000], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30)], 'alpha':[0.0001, 0.01, 1], 'activation':['relu', 'identity'], 'max_iter':[1000], 'batch_size':[200, 128, 64], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200], 'learning_rate_init':[0.001, 0.01]}\n",
        "  #mse = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False\n",
        "  #clf = GridSearchCV(mlp, parameters, scoring = 'neg_mean_squared_error', n_jobs = -1)#, verbose=True, scoring = mse)\n",
        "  mlp.fit(X_train, Y_train)\n",
        "  #print(clf.best_params_)\n",
        "  #print(clf.cv_results_)\n",
        "  #mlp = MLPRegressor(hidden_layer_sizes=(100,), learning_rate_init=0.01, verbose=True, max_iter=1000, random_state=24)\n",
        "  #mlp.fit(X_train, Y_train)\n",
        "  src_second_dev.append(mean_squared_error(Y_dev, mlp.predict(X_dev)))\n",
        "  src_second_test.append(mean_squared_error(Y_test, mlp.predict(X_test)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 108.66570139\n",
            "Iteration 2, loss = 75.15090289\n",
            "Iteration 3, loss = 71.20661604\n",
            "Iteration 4, loss = 68.32243477\n",
            "Iteration 5, loss = 65.50988555\n",
            "Iteration 6, loss = 62.76317216\n",
            "Iteration 7, loss = 60.27149179\n",
            "Iteration 8, loss = 58.24121499\n",
            "Iteration 9, loss = 56.96747127\n",
            "Iteration 10, loss = 55.87717995\n",
            "Iteration 11, loss = 55.59715619\n",
            "Iteration 12, loss = 54.87448394\n",
            "Iteration 13, loss = 54.47425668\n",
            "Iteration 14, loss = 54.10991681\n",
            "Iteration 15, loss = 53.94279295\n",
            "Iteration 16, loss = 54.05404891\n",
            "Iteration 17, loss = 53.73277701\n",
            "Iteration 18, loss = 53.57075254\n",
            "Iteration 19, loss = 53.30802557\n",
            "Iteration 20, loss = 53.28916486\n",
            "Iteration 21, loss = 53.13240656\n",
            "Iteration 22, loss = 53.09121649\n",
            "Iteration 23, loss = 53.24988559\n",
            "Iteration 24, loss = 52.96685552\n",
            "Iteration 25, loss = 52.71211885\n",
            "Iteration 26, loss = 52.82176748\n",
            "Iteration 27, loss = 52.60537781\n",
            "Iteration 28, loss = 52.69057856\n",
            "Iteration 29, loss = 52.50889566\n",
            "Iteration 30, loss = 52.55507271\n",
            "Iteration 31, loss = 52.55317143\n",
            "Iteration 32, loss = 52.44675448\n",
            "Iteration 33, loss = 52.36817643\n",
            "Iteration 34, loss = 52.32275508\n",
            "Iteration 35, loss = 52.17152898\n",
            "Iteration 36, loss = 52.10855921\n",
            "Iteration 37, loss = 52.16158485\n",
            "Iteration 38, loss = 52.25573866\n",
            "Iteration 39, loss = 52.14177327\n",
            "Iteration 40, loss = 52.38356810\n",
            "Iteration 41, loss = 52.12278488\n",
            "Iteration 42, loss = 52.01733633\n",
            "Iteration 43, loss = 52.14998828\n",
            "Iteration 44, loss = 52.20801274\n",
            "Iteration 45, loss = 51.95123623\n",
            "Iteration 46, loss = 52.08779449\n",
            "Iteration 47, loss = 52.03848659\n",
            "Iteration 48, loss = 51.90216913\n",
            "Iteration 49, loss = 51.87196924\n",
            "Iteration 50, loss = 51.87813545\n",
            "Iteration 51, loss = 52.01788948\n",
            "Iteration 52, loss = 52.06085293\n",
            "Iteration 53, loss = 51.97538947\n",
            "Iteration 54, loss = 52.02298558\n",
            "Iteration 55, loss = 51.82953861\n",
            "Iteration 56, loss = 52.03045333\n",
            "Iteration 57, loss = 52.00050137\n",
            "Iteration 58, loss = 52.01707606\n",
            "Iteration 59, loss = 51.80971425\n",
            "Iteration 60, loss = 51.94596136\n",
            "Iteration 61, loss = 51.84460472\n",
            "Iteration 62, loss = 51.79987992\n",
            "Iteration 63, loss = 51.97751055\n",
            "Iteration 64, loss = 51.76760593\n",
            "Iteration 65, loss = 51.71353282\n",
            "Iteration 66, loss = 51.74596281\n",
            "Iteration 67, loss = 51.70875248\n",
            "Iteration 68, loss = 51.79182209\n",
            "Iteration 69, loss = 51.64894918\n",
            "Iteration 70, loss = 51.79611759\n",
            "Iteration 71, loss = 52.11632468\n",
            "Iteration 72, loss = 51.88980474\n",
            "Iteration 73, loss = 51.81215776\n",
            "Iteration 74, loss = 51.70763663\n",
            "Iteration 75, loss = 51.56613499\n",
            "Iteration 76, loss = 51.69420798\n",
            "Iteration 77, loss = 51.78356901\n",
            "Iteration 78, loss = 51.63069205\n",
            "Iteration 79, loss = 51.70287196\n",
            "Iteration 80, loss = 51.74207812\n",
            "Iteration 81, loss = 51.81889144\n",
            "Iteration 82, loss = 51.75607301\n",
            "Iteration 83, loss = 51.71991753\n",
            "Iteration 84, loss = 51.59510753\n",
            "Iteration 85, loss = 51.73265339\n",
            "Iteration 86, loss = 51.93874611\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 99.01069369\n",
            "Iteration 2, loss = 73.71141505\n",
            "Iteration 3, loss = 70.86178063\n",
            "Iteration 4, loss = 68.28547412\n",
            "Iteration 5, loss = 65.61959271\n",
            "Iteration 6, loss = 63.21761429\n",
            "Iteration 7, loss = 61.14256727\n",
            "Iteration 8, loss = 59.27407805\n",
            "Iteration 9, loss = 58.09947771\n",
            "Iteration 10, loss = 57.05236085\n",
            "Iteration 11, loss = 56.44022338\n",
            "Iteration 12, loss = 56.02927777\n",
            "Iteration 13, loss = 55.64109836\n",
            "Iteration 14, loss = 55.45416143\n",
            "Iteration 15, loss = 55.34718122\n",
            "Iteration 16, loss = 55.12527849\n",
            "Iteration 17, loss = 54.79116078\n",
            "Iteration 18, loss = 54.62580020\n",
            "Iteration 19, loss = 54.56571435\n",
            "Iteration 20, loss = 54.49882143\n",
            "Iteration 21, loss = 54.20391590\n",
            "Iteration 22, loss = 54.12582521\n",
            "Iteration 23, loss = 54.15638365\n",
            "Iteration 24, loss = 54.06268325\n",
            "Iteration 25, loss = 53.90812761\n",
            "Iteration 26, loss = 53.66223635\n",
            "Iteration 27, loss = 53.65952198\n",
            "Iteration 28, loss = 53.66281267\n",
            "Iteration 29, loss = 53.67606813\n",
            "Iteration 30, loss = 53.46342592\n",
            "Iteration 31, loss = 53.50831150\n",
            "Iteration 32, loss = 53.63751599\n",
            "Iteration 33, loss = 53.38859956\n",
            "Iteration 34, loss = 53.60476255\n",
            "Iteration 35, loss = 53.31719799\n",
            "Iteration 36, loss = 53.22415155\n",
            "Iteration 37, loss = 53.36332916\n",
            "Iteration 38, loss = 53.31133415\n",
            "Iteration 39, loss = 53.45345456\n",
            "Iteration 40, loss = 53.16925397\n",
            "Iteration 41, loss = 53.18575915\n",
            "Iteration 42, loss = 53.49457705\n",
            "Iteration 43, loss = 53.26926487\n",
            "Iteration 44, loss = 53.12156723\n",
            "Iteration 45, loss = 53.07649859\n",
            "Iteration 46, loss = 53.14568524\n",
            "Iteration 47, loss = 53.02778475\n",
            "Iteration 48, loss = 53.09440023\n",
            "Iteration 49, loss = 53.35736541\n",
            "Iteration 50, loss = 52.91561258\n",
            "Iteration 51, loss = 53.08507017\n",
            "Iteration 52, loss = 53.02500899\n",
            "Iteration 53, loss = 53.07691052\n",
            "Iteration 54, loss = 52.98555416\n",
            "Iteration 55, loss = 52.91419318\n",
            "Iteration 56, loss = 53.25081736\n",
            "Iteration 57, loss = 53.09424013\n",
            "Iteration 58, loss = 53.30108149\n",
            "Iteration 59, loss = 53.03898854\n",
            "Iteration 60, loss = 53.01412155\n",
            "Iteration 61, loss = 52.90098732\n",
            "Iteration 62, loss = 52.94875823\n",
            "Iteration 63, loss = 53.04961141\n",
            "Iteration 64, loss = 52.79415991\n",
            "Iteration 65, loss = 53.24689371\n",
            "Iteration 66, loss = 52.81217523\n",
            "Iteration 67, loss = 52.97062489\n",
            "Iteration 68, loss = 52.81927808\n",
            "Iteration 69, loss = 52.91678025\n",
            "Iteration 70, loss = 52.83687073\n",
            "Iteration 71, loss = 52.71926539\n",
            "Iteration 72, loss = 52.74930435\n",
            "Iteration 73, loss = 52.80679014\n",
            "Iteration 74, loss = 52.75576375\n",
            "Iteration 75, loss = 52.96559238\n",
            "Iteration 76, loss = 53.13925695\n",
            "Iteration 77, loss = 53.02654073\n",
            "Iteration 78, loss = 52.74589817\n",
            "Iteration 79, loss = 52.78420810\n",
            "Iteration 80, loss = 52.68462370\n",
            "Iteration 81, loss = 52.78071655\n",
            "Iteration 82, loss = 52.91313925\n",
            "Iteration 83, loss = 52.73131328\n",
            "Iteration 84, loss = 52.64064893\n",
            "Iteration 85, loss = 52.78431739\n",
            "Iteration 86, loss = 53.08225866\n",
            "Iteration 87, loss = 52.63010170\n",
            "Iteration 88, loss = 52.77417009\n",
            "Iteration 89, loss = 52.77112837\n",
            "Iteration 90, loss = 52.85195421\n",
            "Iteration 91, loss = 52.75056445\n",
            "Iteration 92, loss = 52.62538539\n",
            "Iteration 93, loss = 52.60810648\n",
            "Iteration 94, loss = 52.71083631\n",
            "Iteration 95, loss = 52.72949508\n",
            "Iteration 96, loss = 52.56895101\n",
            "Iteration 97, loss = 52.68889121\n",
            "Iteration 98, loss = 52.47327926\n",
            "Iteration 99, loss = 52.76481801\n",
            "Iteration 100, loss = 52.62039370\n",
            "Iteration 101, loss = 52.62796251\n",
            "Iteration 102, loss = 52.45813682\n",
            "Iteration 103, loss = 52.48771458\n",
            "Iteration 104, loss = 52.62561322\n",
            "Iteration 105, loss = 52.46309331\n",
            "Iteration 106, loss = 52.70338134\n",
            "Iteration 107, loss = 52.45224486\n",
            "Iteration 108, loss = 52.65987827\n",
            "Iteration 109, loss = 52.63414344\n",
            "Iteration 110, loss = 52.66541641\n",
            "Iteration 111, loss = 52.49754414\n",
            "Iteration 112, loss = 52.42343267\n",
            "Iteration 113, loss = 52.45652713\n",
            "Iteration 114, loss = 52.50485007\n",
            "Iteration 115, loss = 52.53090076\n",
            "Iteration 116, loss = 52.41168090\n",
            "Iteration 117, loss = 52.31558421\n",
            "Iteration 118, loss = 52.47407732\n",
            "Iteration 119, loss = 52.48864844\n",
            "Iteration 120, loss = 52.76583180\n",
            "Iteration 121, loss = 52.47044698\n",
            "Iteration 122, loss = 52.33378017\n",
            "Iteration 123, loss = 52.30094913\n",
            "Iteration 124, loss = 52.49851085\n",
            "Iteration 125, loss = 52.39901359\n",
            "Iteration 126, loss = 52.22840089\n",
            "Iteration 127, loss = 52.39218692\n",
            "Iteration 128, loss = 52.43137212\n",
            "Iteration 129, loss = 52.35877359\n",
            "Iteration 130, loss = 52.40798823\n",
            "Iteration 131, loss = 52.34084681\n",
            "Iteration 132, loss = 52.27662718\n",
            "Iteration 133, loss = 52.52046265\n",
            "Iteration 134, loss = 52.30909623\n",
            "Iteration 135, loss = 52.28427597\n",
            "Iteration 136, loss = 52.42385146\n",
            "Iteration 137, loss = 52.32426535\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 127.74384173\n",
            "Iteration 2, loss = 83.94601727\n",
            "Iteration 3, loss = 79.98543357\n",
            "Iteration 4, loss = 77.58639587\n",
            "Iteration 5, loss = 75.55203860\n",
            "Iteration 6, loss = 73.62517772\n",
            "Iteration 7, loss = 71.90101031\n",
            "Iteration 8, loss = 69.98603312\n",
            "Iteration 9, loss = 68.26312508\n",
            "Iteration 10, loss = 66.72885517\n",
            "Iteration 11, loss = 65.24609803\n",
            "Iteration 12, loss = 64.04897889\n",
            "Iteration 13, loss = 62.83093399\n",
            "Iteration 14, loss = 61.85246632\n",
            "Iteration 15, loss = 61.07155259\n",
            "Iteration 16, loss = 60.62446341\n",
            "Iteration 17, loss = 59.93892857\n",
            "Iteration 18, loss = 59.65501626\n",
            "Iteration 19, loss = 59.24610441\n",
            "Iteration 20, loss = 59.00681351\n",
            "Iteration 21, loss = 58.82614833\n",
            "Iteration 22, loss = 58.64710627\n",
            "Iteration 23, loss = 58.82520426\n",
            "Iteration 24, loss = 58.27061148\n",
            "Iteration 25, loss = 58.19630507\n",
            "Iteration 26, loss = 57.91325328\n",
            "Iteration 27, loss = 57.82351370\n",
            "Iteration 28, loss = 58.04975643\n",
            "Iteration 29, loss = 57.59243025\n",
            "Iteration 30, loss = 57.49515801\n",
            "Iteration 31, loss = 57.32298032\n",
            "Iteration 32, loss = 57.40734065\n",
            "Iteration 33, loss = 57.26091486\n",
            "Iteration 34, loss = 57.17815681\n",
            "Iteration 35, loss = 56.89894127\n",
            "Iteration 36, loss = 56.98015829\n",
            "Iteration 37, loss = 56.82889020\n",
            "Iteration 38, loss = 57.00266880\n",
            "Iteration 39, loss = 56.79278704\n",
            "Iteration 40, loss = 56.62418131\n",
            "Iteration 41, loss = 56.41455455\n",
            "Iteration 42, loss = 56.57753266\n",
            "Iteration 43, loss = 56.47885734\n",
            "Iteration 44, loss = 56.34654266\n",
            "Iteration 45, loss = 56.17005680\n",
            "Iteration 46, loss = 56.27963942\n",
            "Iteration 47, loss = 56.28245818\n",
            "Iteration 48, loss = 56.01703776\n",
            "Iteration 49, loss = 56.06689556\n",
            "Iteration 50, loss = 56.21034683\n",
            "Iteration 51, loss = 55.95087209\n",
            "Iteration 52, loss = 55.88450060\n",
            "Iteration 53, loss = 55.85253428\n",
            "Iteration 54, loss = 55.98422882\n",
            "Iteration 55, loss = 55.83056065\n",
            "Iteration 56, loss = 55.76084594\n",
            "Iteration 57, loss = 55.76752719\n",
            "Iteration 58, loss = 55.67892206\n",
            "Iteration 59, loss = 55.66255419\n",
            "Iteration 60, loss = 55.52777405\n",
            "Iteration 61, loss = 55.60642427\n",
            "Iteration 62, loss = 55.52136441\n",
            "Iteration 63, loss = 55.54412429\n",
            "Iteration 64, loss = 55.61900528\n",
            "Iteration 65, loss = 55.62035440\n",
            "Iteration 66, loss = 55.57699220\n",
            "Iteration 67, loss = 55.39964445\n",
            "Iteration 68, loss = 55.90314393\n",
            "Iteration 69, loss = 55.58732791\n",
            "Iteration 70, loss = 55.45968194\n",
            "Iteration 71, loss = 55.41645419\n",
            "Iteration 72, loss = 55.64616345\n",
            "Iteration 73, loss = 55.21369085\n",
            "Iteration 74, loss = 55.43590562\n",
            "Iteration 75, loss = 55.24396439\n",
            "Iteration 76, loss = 55.27177086\n",
            "Iteration 77, loss = 55.17406039\n",
            "Iteration 78, loss = 55.29177362\n",
            "Iteration 79, loss = 55.29510996\n",
            "Iteration 80, loss = 55.63754585\n",
            "Iteration 81, loss = 55.22755910\n",
            "Iteration 82, loss = 55.25519281\n",
            "Iteration 83, loss = 55.01709401\n",
            "Iteration 84, loss = 55.10641109\n",
            "Iteration 85, loss = 55.02343355\n",
            "Iteration 86, loss = 55.12504029\n",
            "Iteration 87, loss = 55.17032179\n",
            "Iteration 88, loss = 55.00143726\n",
            "Iteration 89, loss = 54.93287873\n",
            "Iteration 90, loss = 55.06521843\n",
            "Iteration 91, loss = 55.11728877\n",
            "Iteration 92, loss = 54.92778801\n",
            "Iteration 93, loss = 54.84381146\n",
            "Iteration 94, loss = 55.04961875\n",
            "Iteration 95, loss = 54.84874147\n",
            "Iteration 96, loss = 54.76404347\n",
            "Iteration 97, loss = 55.07849211\n",
            "Iteration 98, loss = 54.69077439\n",
            "Iteration 99, loss = 54.79679717\n",
            "Iteration 100, loss = 54.69822233\n",
            "Iteration 101, loss = 54.80428517\n",
            "Iteration 102, loss = 55.22368050\n",
            "Iteration 103, loss = 54.67912908\n",
            "Iteration 104, loss = 54.69278019\n",
            "Iteration 105, loss = 54.71875550\n",
            "Iteration 106, loss = 54.59099888\n",
            "Iteration 107, loss = 54.81358398\n",
            "Iteration 108, loss = 54.69871435\n",
            "Iteration 109, loss = 54.64558260\n",
            "Iteration 110, loss = 54.72474232\n",
            "Iteration 111, loss = 54.59854445\n",
            "Iteration 112, loss = 54.70242508\n",
            "Iteration 113, loss = 54.56711489\n",
            "Iteration 114, loss = 54.63170240\n",
            "Iteration 115, loss = 54.55278828\n",
            "Iteration 116, loss = 54.52670082\n",
            "Iteration 117, loss = 54.69840741\n",
            "Iteration 118, loss = 54.55946256\n",
            "Iteration 119, loss = 54.38698617\n",
            "Iteration 120, loss = 54.89859622\n",
            "Iteration 121, loss = 54.46105554\n",
            "Iteration 122, loss = 54.50272179\n",
            "Iteration 123, loss = 54.38610671\n",
            "Iteration 124, loss = 54.46100640\n",
            "Iteration 125, loss = 54.40381777\n",
            "Iteration 126, loss = 54.43681269\n",
            "Iteration 127, loss = 54.35432354\n",
            "Iteration 128, loss = 54.51734425\n",
            "Iteration 129, loss = 55.06217853\n",
            "Iteration 130, loss = 54.47861417\n",
            "Iteration 131, loss = 54.32666796\n",
            "Iteration 132, loss = 54.29832778\n",
            "Iteration 133, loss = 54.34380326\n",
            "Iteration 134, loss = 54.27376376\n",
            "Iteration 135, loss = 54.21818533\n",
            "Iteration 136, loss = 54.26681323\n",
            "Iteration 137, loss = 54.30816461\n",
            "Iteration 138, loss = 54.52155437\n",
            "Iteration 139, loss = 54.31550861\n",
            "Iteration 140, loss = 54.41670808\n",
            "Iteration 141, loss = 54.30817937\n",
            "Iteration 142, loss = 54.23816654\n",
            "Iteration 143, loss = 54.40172135\n",
            "Iteration 144, loss = 54.40084860\n",
            "Iteration 145, loss = 54.29044230\n",
            "Iteration 146, loss = 54.29773147\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbVIWPLQGa1j",
        "colab_type": "code",
        "outputId": "c15bbaba-cfd6-43d7-d5ab-41055ff9e106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(src_first_dev)\n",
        "print(src_first_test)\n",
        "print(src_second_dev)\n",
        "print(src_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[113.27534658892085, 132.89938223552974, 116.86139025375269]\n",
            "[97.8275385322878, 128.92214442252535, 105.00928474693275]\n",
            "[116.6041364235833, 154.2215724672641, 121.69415753124206]\n",
            "[99.07345402152727, 134.57657223542978, 105.74110839186143]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdB-VotcaRLr",
        "colab_type": "code",
        "outputId": "d0fb7253-c57c-49c4-daa3-06060ebc21ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "src_test_mse = pd.DataFrame(list(zip(src_first_test, src_second_test)), columns=['SRCONLY_REG','SRCONLY_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "src_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_897aef9e_9eec_11ea_9f06_0242ac1c0002row0_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_897aef9e_9eec_11ea_9f06_0242ac1c0002row1_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_897aef9e_9eec_11ea_9f06_0242ac1c0002row2_col0 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >SRCONLY_REG</th>        <th class=\"col_heading level0 col1\" >SRCONLY_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002row0_col0\" class=\"data row0 col0\" >97.827539</td>\n",
              "                        <td id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002row0_col1\" class=\"data row0 col1\" >99.073454</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002row1_col0\" class=\"data row1 col0\" >128.922144</td>\n",
              "                        <td id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002row1_col1\" class=\"data row1 col1\" >134.576572</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002row2_col0\" class=\"data row2 col0\" >105.009285</td>\n",
              "                        <td id=\"T_897aef9e_9eec_11ea_9f06_0242ac1c0002row2_col1\" class=\"data row2 col1\" >105.741108</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28f26b05f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XskRKmj9PCWW",
        "colab_type": "text"
      },
      "source": [
        "### TGTONLY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBrIq2flVaO6",
        "colab_type": "code",
        "outputId": "7b8dc0f3-da94-487d-b0e9-573e4e7b906b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#{'activation': 'identity', 'alpha': 1, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'identity', 'alpha': 0.01, 'batch_size': 64, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01, 'max_iter': 1000}\n",
        "#0.001 converged for atleast 1 of the 3 domains, but 0.01 didnt converge for any, 0.1 converged for all domains.\n",
        "ma_train = pd.concat([male_tgt], ignore_index=True)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([male_tgt_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([female_tgt], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([female_tgt_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([mixed_tgt], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([mixed_tgt_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred]}\n",
        "\n",
        "tgt_first_dev = []\n",
        "tgt_second_dev = []\n",
        "tgt_first_test = []\n",
        "tgt_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "  linr = LinearRegression() #normalize=True\n",
        "  linr.fit(X_train, Y_train)\n",
        "  tgt_first_dev.append(mean_squared_error(Y_dev, linr.predict(X_dev)))\n",
        "  tgt_first_test.append(mean_squared_error(Y_test, linr.predict(X_test)))\n",
        "\n",
        "  mlp = MLPRegressor(activation='identity', alpha=0.0001, batch_size=64, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  #mlp = MLPRegressor(random_state=24)\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30), (25,25,25,25)], 'activation':['relu', 'identity'], 'alpha':[0.0001, 0.01, 0.1, 1, 10], 'batch_size':[200, 128, 64, 32], 'max_iter':[1000], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'solver':['adam', 'lbfgs'], 'hidden_layer_sizes':[(100,), (50,50), (30,30,30)], 'alpha':[0.0001, 0.01, 1], 'activation':['relu', 'identity'], 'max_iter':[1000], 'batch_size':[200, 128, 64], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200], 'learning_rate_init':[0.001, 0.01]}\n",
        "  #mse = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False\n",
        "  #clf = GridSearchCV(mlp, parameters, scoring = 'neg_mean_squared_error', n_jobs = -1)#, verbose=True, scoring = mse)\n",
        "  mlp.fit(X_train, Y_train)\n",
        "  #print(clf.best_params_)\n",
        "  #mlp = MLPRegressor(hidden_layer_sizes=(100,), learning_rate_init=0.1, verbose=True, max_iter=1000, random_state=24)\n",
        "  #mlp.fit(X_train, Y_train)\n",
        "  tgt_second_dev.append(mean_squared_error(Y_dev, mlp.predict(X_dev)))\n",
        "  tgt_second_test.append(mean_squared_error(Y_test, mlp.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 281.79653501\n",
            "Iteration 2, loss = 233.25873066\n",
            "Iteration 3, loss = 192.79598434\n",
            "Iteration 4, loss = 161.57151159\n",
            "Iteration 5, loss = 138.06078808\n",
            "Iteration 6, loss = 124.03595368\n",
            "Iteration 7, loss = 114.26131012\n",
            "Iteration 8, loss = 111.76166502\n",
            "Iteration 9, loss = 111.70198090\n",
            "Iteration 10, loss = 111.54186145\n",
            "Iteration 11, loss = 112.29727990\n",
            "Iteration 12, loss = 112.30106095\n",
            "Iteration 13, loss = 111.06258230\n",
            "Iteration 14, loss = 108.85379034\n",
            "Iteration 15, loss = 106.03427221\n",
            "Iteration 16, loss = 103.12511373\n",
            "Iteration 17, loss = 100.33988657\n",
            "Iteration 18, loss = 98.48271229\n",
            "Iteration 19, loss = 97.12407678\n",
            "Iteration 20, loss = 95.75886726\n",
            "Iteration 21, loss = 95.01278341\n",
            "Iteration 22, loss = 94.24073859\n",
            "Iteration 23, loss = 93.50866049\n",
            "Iteration 24, loss = 92.97146925\n",
            "Iteration 25, loss = 92.04491480\n",
            "Iteration 26, loss = 91.08551013\n",
            "Iteration 27, loss = 90.27594862\n",
            "Iteration 28, loss = 89.33301236\n",
            "Iteration 29, loss = 88.67290227\n",
            "Iteration 30, loss = 88.04159671\n",
            "Iteration 31, loss = 87.62257175\n",
            "Iteration 32, loss = 87.09943798\n",
            "Iteration 33, loss = 86.62756125\n",
            "Iteration 34, loss = 86.22228242\n",
            "Iteration 35, loss = 85.91246482\n",
            "Iteration 36, loss = 85.53381995\n",
            "Iteration 37, loss = 85.23335146\n",
            "Iteration 38, loss = 84.87690260\n",
            "Iteration 39, loss = 84.52492725\n",
            "Iteration 40, loss = 84.16231495\n",
            "Iteration 41, loss = 83.93836725\n",
            "Iteration 42, loss = 83.69674640\n",
            "Iteration 43, loss = 83.38129986\n",
            "Iteration 44, loss = 83.20891458\n",
            "Iteration 45, loss = 82.99815001\n",
            "Iteration 46, loss = 82.82028380\n",
            "Iteration 47, loss = 82.69460719\n",
            "Iteration 48, loss = 82.49240802\n",
            "Iteration 49, loss = 82.32215995\n",
            "Iteration 50, loss = 82.15183362\n",
            "Iteration 51, loss = 81.97703575\n",
            "Iteration 52, loss = 81.82173831\n",
            "Iteration 53, loss = 81.62402838\n",
            "Iteration 54, loss = 81.48132497\n",
            "Iteration 55, loss = 81.33362960\n",
            "Iteration 56, loss = 81.16349949\n",
            "Iteration 57, loss = 81.08461064\n",
            "Iteration 58, loss = 80.89083498\n",
            "Iteration 59, loss = 80.72630491\n",
            "Iteration 60, loss = 80.56461900\n",
            "Iteration 61, loss = 80.49719775\n",
            "Iteration 62, loss = 80.30161392\n",
            "Iteration 63, loss = 80.14479521\n",
            "Iteration 64, loss = 80.00626008\n",
            "Iteration 65, loss = 79.86819463\n",
            "Iteration 66, loss = 79.69742562\n",
            "Iteration 67, loss = 79.65540937\n",
            "Iteration 68, loss = 79.41718036\n",
            "Iteration 69, loss = 79.27654364\n",
            "Iteration 70, loss = 79.13928682\n",
            "Iteration 71, loss = 79.24603102\n",
            "Iteration 72, loss = 78.86219500\n",
            "Iteration 73, loss = 78.73049225\n",
            "Iteration 74, loss = 78.61170451\n",
            "Iteration 75, loss = 78.48439654\n",
            "Iteration 76, loss = 78.35198688\n",
            "Iteration 77, loss = 78.38760707\n",
            "Iteration 78, loss = 78.05238116\n",
            "Iteration 79, loss = 77.90835292\n",
            "Iteration 80, loss = 77.79661278\n",
            "Iteration 81, loss = 77.75066254\n",
            "Iteration 82, loss = 77.49736255\n",
            "Iteration 83, loss = 77.37623941\n",
            "Iteration 84, loss = 77.29377286\n",
            "Iteration 85, loss = 77.25101153\n",
            "Iteration 86, loss = 77.17164733\n",
            "Iteration 87, loss = 76.92762742\n",
            "Iteration 88, loss = 77.03896511\n",
            "Iteration 89, loss = 76.64378172\n",
            "Iteration 90, loss = 76.53160454\n",
            "Iteration 91, loss = 76.43337059\n",
            "Iteration 92, loss = 76.32970004\n",
            "Iteration 93, loss = 76.11774425\n",
            "Iteration 94, loss = 75.98991042\n",
            "Iteration 95, loss = 75.96680343\n",
            "Iteration 96, loss = 75.72412066\n",
            "Iteration 97, loss = 75.65036591\n",
            "Iteration 98, loss = 75.58927005\n",
            "Iteration 99, loss = 75.51473248\n",
            "Iteration 100, loss = 75.44204917\n",
            "Iteration 101, loss = 75.31046446\n",
            "Iteration 102, loss = 75.14854910\n",
            "Iteration 103, loss = 74.88409386\n",
            "Iteration 104, loss = 75.06156676\n",
            "Iteration 105, loss = 74.66669053\n",
            "Iteration 106, loss = 74.49331046\n",
            "Iteration 107, loss = 74.37876676\n",
            "Iteration 108, loss = 74.26614731\n",
            "Iteration 109, loss = 74.21891569\n",
            "Iteration 110, loss = 74.06250734\n",
            "Iteration 111, loss = 73.88704090\n",
            "Iteration 112, loss = 73.73100060\n",
            "Iteration 113, loss = 73.60728129\n",
            "Iteration 114, loss = 73.57385639\n",
            "Iteration 115, loss = 73.53261085\n",
            "Iteration 116, loss = 73.45244431\n",
            "Iteration 117, loss = 73.30446608\n",
            "Iteration 118, loss = 73.13045623\n",
            "Iteration 119, loss = 72.93536722\n",
            "Iteration 120, loss = 72.89850700\n",
            "Iteration 121, loss = 72.70835939\n",
            "Iteration 122, loss = 72.57810928\n",
            "Iteration 123, loss = 72.47561798\n",
            "Iteration 124, loss = 72.32417487\n",
            "Iteration 125, loss = 72.23438553\n",
            "Iteration 126, loss = 72.10107094\n",
            "Iteration 127, loss = 71.99498747\n",
            "Iteration 128, loss = 71.90675257\n",
            "Iteration 129, loss = 71.81840640\n",
            "Iteration 130, loss = 71.67442209\n",
            "Iteration 131, loss = 71.54974018\n",
            "Iteration 132, loss = 71.45670135\n",
            "Iteration 133, loss = 71.29054187\n",
            "Iteration 134, loss = 71.15792855\n",
            "Iteration 135, loss = 71.08019055\n",
            "Iteration 136, loss = 70.92296744\n",
            "Iteration 137, loss = 70.82183214\n",
            "Iteration 138, loss = 70.68604453\n",
            "Iteration 139, loss = 70.80783485\n",
            "Iteration 140, loss = 70.49016462\n",
            "Iteration 141, loss = 70.47359410\n",
            "Iteration 142, loss = 70.39935055\n",
            "Iteration 143, loss = 70.22460620\n",
            "Iteration 144, loss = 70.07697867\n",
            "Iteration 145, loss = 69.89406003\n",
            "Iteration 146, loss = 69.81353107\n",
            "Iteration 147, loss = 69.77507249\n",
            "Iteration 148, loss = 69.66652229\n",
            "Iteration 149, loss = 69.54033802\n",
            "Iteration 150, loss = 69.39735616\n",
            "Iteration 151, loss = 69.26733485\n",
            "Iteration 152, loss = 69.14799476\n",
            "Iteration 153, loss = 69.09826870\n",
            "Iteration 154, loss = 68.99840541\n",
            "Iteration 155, loss = 68.87731969\n",
            "Iteration 156, loss = 68.75400110\n",
            "Iteration 157, loss = 68.62391090\n",
            "Iteration 158, loss = 68.53990088\n",
            "Iteration 159, loss = 68.41505010\n",
            "Iteration 160, loss = 68.32346607\n",
            "Iteration 161, loss = 68.26077201\n",
            "Iteration 162, loss = 68.10581577\n",
            "Iteration 163, loss = 68.01169935\n",
            "Iteration 164, loss = 68.02320483\n",
            "Iteration 165, loss = 67.84324033\n",
            "Iteration 166, loss = 67.71166209\n",
            "Iteration 167, loss = 67.62933664\n",
            "Iteration 168, loss = 67.63271178\n",
            "Iteration 169, loss = 67.43031035\n",
            "Iteration 170, loss = 67.36916248\n",
            "Iteration 171, loss = 67.27052803\n",
            "Iteration 172, loss = 67.13275443\n",
            "Iteration 173, loss = 67.22834130\n",
            "Iteration 174, loss = 67.19522705\n",
            "Iteration 175, loss = 66.87070859\n",
            "Iteration 176, loss = 66.77007915\n",
            "Iteration 177, loss = 66.65739631\n",
            "Iteration 178, loss = 66.56448306\n",
            "Iteration 179, loss = 66.59471916\n",
            "Iteration 180, loss = 66.42316835\n",
            "Iteration 181, loss = 66.29296731\n",
            "Iteration 182, loss = 66.26650439\n",
            "Iteration 183, loss = 66.15213002\n",
            "Iteration 184, loss = 66.00737677\n",
            "Iteration 185, loss = 65.91780614\n",
            "Iteration 186, loss = 65.90194618\n",
            "Iteration 187, loss = 65.95454835\n",
            "Iteration 188, loss = 65.81450075\n",
            "Iteration 189, loss = 65.64663250\n",
            "Iteration 190, loss = 65.59340638\n",
            "Iteration 191, loss = 65.49691734\n",
            "Iteration 192, loss = 65.44021532\n",
            "Iteration 193, loss = 65.41896212\n",
            "Iteration 194, loss = 65.22726063\n",
            "Iteration 195, loss = 65.24113742\n",
            "Iteration 196, loss = 64.97683811\n",
            "Iteration 197, loss = 65.13329796\n",
            "Iteration 198, loss = 64.77699635\n",
            "Iteration 199, loss = 65.12150497\n",
            "Iteration 200, loss = 64.61800829\n",
            "Iteration 201, loss = 64.59877813\n",
            "Iteration 202, loss = 64.61347365\n",
            "Iteration 203, loss = 64.56746268\n",
            "Iteration 204, loss = 64.51823739\n",
            "Iteration 205, loss = 64.22237161\n",
            "Iteration 206, loss = 64.15352509\n",
            "Iteration 207, loss = 64.10661531\n",
            "Iteration 208, loss = 63.96016226\n",
            "Iteration 209, loss = 63.85645331\n",
            "Iteration 210, loss = 63.89428628\n",
            "Iteration 211, loss = 63.77431467\n",
            "Iteration 212, loss = 63.79022257\n",
            "Iteration 213, loss = 63.59718876\n",
            "Iteration 214, loss = 63.52359320\n",
            "Iteration 215, loss = 63.56335328\n",
            "Iteration 216, loss = 63.34010861\n",
            "Iteration 217, loss = 63.28327165\n",
            "Iteration 218, loss = 63.23975164\n",
            "Iteration 219, loss = 63.16445545\n",
            "Iteration 220, loss = 63.06050149\n",
            "Iteration 221, loss = 62.95273466\n",
            "Iteration 222, loss = 62.93237311\n",
            "Iteration 223, loss = 62.89932020\n",
            "Iteration 224, loss = 62.95368723\n",
            "Iteration 225, loss = 62.98850544\n",
            "Iteration 226, loss = 62.67362375\n",
            "Iteration 227, loss = 62.56424181\n",
            "Iteration 228, loss = 62.52042739\n",
            "Iteration 229, loss = 62.44045403\n",
            "Iteration 230, loss = 62.60011249\n",
            "Iteration 231, loss = 62.34698876\n",
            "Iteration 232, loss = 62.24206742\n",
            "Iteration 233, loss = 62.14515163\n",
            "Iteration 234, loss = 62.23648130\n",
            "Iteration 235, loss = 62.21825725\n",
            "Iteration 236, loss = 62.14138280\n",
            "Iteration 237, loss = 61.95998089\n",
            "Iteration 238, loss = 61.88513080\n",
            "Iteration 239, loss = 61.81073848\n",
            "Iteration 240, loss = 61.73054268\n",
            "Iteration 241, loss = 61.69107965\n",
            "Iteration 242, loss = 61.72809879\n",
            "Iteration 243, loss = 61.60853171\n",
            "Iteration 244, loss = 61.60068484\n",
            "Iteration 245, loss = 61.51344924\n",
            "Iteration 246, loss = 61.29372073\n",
            "Iteration 247, loss = 61.20525898\n",
            "Iteration 248, loss = 61.16432041\n",
            "Iteration 249, loss = 61.15018220\n",
            "Iteration 250, loss = 61.12879848\n",
            "Iteration 251, loss = 61.08088543\n",
            "Iteration 252, loss = 60.96583278\n",
            "Iteration 253, loss = 60.79576887\n",
            "Iteration 254, loss = 60.73612497\n",
            "Iteration 255, loss = 60.76626506\n",
            "Iteration 256, loss = 60.83717588\n",
            "Iteration 257, loss = 60.80700738\n",
            "Iteration 258, loss = 60.68035921\n",
            "Iteration 259, loss = 60.52297393\n",
            "Iteration 260, loss = 60.47992646\n",
            "Iteration 261, loss = 60.43364362\n",
            "Iteration 262, loss = 60.44376784\n",
            "Iteration 263, loss = 60.38233296\n",
            "Iteration 264, loss = 60.20885779\n",
            "Iteration 265, loss = 60.14519897\n",
            "Iteration 266, loss = 60.05128820\n",
            "Iteration 267, loss = 60.16867367\n",
            "Iteration 268, loss = 60.23024900\n",
            "Iteration 269, loss = 60.14771272\n",
            "Iteration 270, loss = 59.98711690\n",
            "Iteration 271, loss = 60.03053178\n",
            "Iteration 272, loss = 59.79318547\n",
            "Iteration 273, loss = 59.88747200\n",
            "Iteration 274, loss = 59.68621176\n",
            "Iteration 275, loss = 59.70288464\n",
            "Iteration 276, loss = 59.66696622\n",
            "Iteration 277, loss = 59.63268242\n",
            "Iteration 278, loss = 59.48379118\n",
            "Iteration 279, loss = 59.47179838\n",
            "Iteration 280, loss = 59.39070000\n",
            "Iteration 281, loss = 59.45394983\n",
            "Iteration 282, loss = 59.26949112\n",
            "Iteration 283, loss = 59.39571322\n",
            "Iteration 284, loss = 59.29284058\n",
            "Iteration 285, loss = 59.07026985\n",
            "Iteration 286, loss = 59.02822007\n",
            "Iteration 287, loss = 59.14812808\n",
            "Iteration 288, loss = 59.12720061\n",
            "Iteration 289, loss = 59.06204696\n",
            "Iteration 290, loss = 58.92321222\n",
            "Iteration 291, loss = 58.78333854\n",
            "Iteration 292, loss = 58.91243431\n",
            "Iteration 293, loss = 58.99394349\n",
            "Iteration 294, loss = 58.77948896\n",
            "Iteration 295, loss = 58.56933601\n",
            "Iteration 296, loss = 58.72972529\n",
            "Iteration 297, loss = 58.78792770\n",
            "Iteration 298, loss = 58.70786348\n",
            "Iteration 299, loss = 58.61717528\n",
            "Iteration 300, loss = 58.51308640\n",
            "Iteration 301, loss = 58.58218927\n",
            "Iteration 302, loss = 58.61948999\n",
            "Iteration 303, loss = 58.41137764\n",
            "Iteration 304, loss = 58.36386834\n",
            "Iteration 305, loss = 58.21493777\n",
            "Iteration 306, loss = 58.17704935\n",
            "Iteration 307, loss = 58.16010821\n",
            "Iteration 308, loss = 58.08023446\n",
            "Iteration 309, loss = 58.19791311\n",
            "Iteration 310, loss = 57.96409630\n",
            "Iteration 311, loss = 58.35918310\n",
            "Iteration 312, loss = 58.29703472\n",
            "Iteration 313, loss = 58.20457730\n",
            "Iteration 314, loss = 57.85227381\n",
            "Iteration 315, loss = 57.82286733\n",
            "Iteration 316, loss = 57.72652890\n",
            "Iteration 317, loss = 57.80548871\n",
            "Iteration 318, loss = 58.01542167\n",
            "Iteration 319, loss = 57.89039879\n",
            "Iteration 320, loss = 57.60444762\n",
            "Iteration 321, loss = 57.99831697\n",
            "Iteration 322, loss = 57.71206697\n",
            "Iteration 323, loss = 57.82385233\n",
            "Iteration 324, loss = 57.52552789\n",
            "Iteration 325, loss = 57.45303546\n",
            "Iteration 326, loss = 57.49697803\n",
            "Iteration 327, loss = 57.36979176\n",
            "Iteration 328, loss = 57.35551933\n",
            "Iteration 329, loss = 57.28768402\n",
            "Iteration 330, loss = 57.28135367\n",
            "Iteration 331, loss = 57.45455232\n",
            "Iteration 332, loss = 57.27174207\n",
            "Iteration 333, loss = 57.08824614\n",
            "Iteration 334, loss = 57.24004171\n",
            "Iteration 335, loss = 57.29155036\n",
            "Iteration 336, loss = 57.33064839\n",
            "Iteration 337, loss = 57.18043242\n",
            "Iteration 338, loss = 57.05942524\n",
            "Iteration 339, loss = 56.88655680\n",
            "Iteration 340, loss = 56.91637390\n",
            "Iteration 341, loss = 57.11633713\n",
            "Iteration 342, loss = 57.28658253\n",
            "Iteration 343, loss = 57.07307527\n",
            "Iteration 344, loss = 56.93819310\n",
            "Iteration 345, loss = 56.78128805\n",
            "Iteration 346, loss = 56.71223435\n",
            "Iteration 347, loss = 56.70003942\n",
            "Iteration 348, loss = 57.03337947\n",
            "Iteration 349, loss = 56.89257698\n",
            "Iteration 350, loss = 56.65866254\n",
            "Iteration 351, loss = 56.55161861\n",
            "Iteration 352, loss = 56.55484254\n",
            "Iteration 353, loss = 56.58348968\n",
            "Iteration 354, loss = 56.54151350\n",
            "Iteration 355, loss = 56.46646320\n",
            "Iteration 356, loss = 56.53065814\n",
            "Iteration 357, loss = 56.66522510\n",
            "Iteration 358, loss = 56.83670305\n",
            "Iteration 359, loss = 56.58448614\n",
            "Iteration 360, loss = 56.38547699\n",
            "Iteration 361, loss = 56.44845315\n",
            "Iteration 362, loss = 56.43529491\n",
            "Iteration 363, loss = 56.39710648\n",
            "Iteration 364, loss = 56.27877383\n",
            "Iteration 365, loss = 56.24266506\n",
            "Iteration 366, loss = 56.23259633\n",
            "Iteration 367, loss = 56.19011171\n",
            "Iteration 368, loss = 56.55284451\n",
            "Iteration 369, loss = 56.08727965\n",
            "Iteration 370, loss = 56.08361671\n",
            "Iteration 371, loss = 56.16632120\n",
            "Iteration 372, loss = 56.24320398\n",
            "Iteration 373, loss = 56.21770391\n",
            "Iteration 374, loss = 56.11861955\n",
            "Iteration 375, loss = 56.06699984\n",
            "Iteration 376, loss = 55.92078722\n",
            "Iteration 377, loss = 55.99277473\n",
            "Iteration 378, loss = 56.22523634\n",
            "Iteration 379, loss = 55.94431138\n",
            "Iteration 380, loss = 55.95750155\n",
            "Iteration 381, loss = 56.08815352\n",
            "Iteration 382, loss = 55.88667340\n",
            "Iteration 383, loss = 55.76178532\n",
            "Iteration 384, loss = 55.85189195\n",
            "Iteration 385, loss = 55.87480716\n",
            "Iteration 386, loss = 55.86082153\n",
            "Iteration 387, loss = 55.73753181\n",
            "Iteration 388, loss = 55.69065305\n",
            "Iteration 389, loss = 55.72968293\n",
            "Iteration 390, loss = 55.74872327\n",
            "Iteration 391, loss = 55.77468854\n",
            "Iteration 392, loss = 55.67149739\n",
            "Iteration 393, loss = 55.63537087\n",
            "Iteration 394, loss = 55.86277732\n",
            "Iteration 395, loss = 55.52927935\n",
            "Iteration 396, loss = 55.86599231\n",
            "Iteration 397, loss = 55.81002298\n",
            "Iteration 398, loss = 55.56321902\n",
            "Iteration 399, loss = 55.43190066\n",
            "Iteration 400, loss = 55.62726419\n",
            "Iteration 401, loss = 55.81138548\n",
            "Iteration 402, loss = 55.94841604\n",
            "Iteration 403, loss = 55.85027745\n",
            "Iteration 404, loss = 55.40540402\n",
            "Iteration 405, loss = 55.48771275\n",
            "Iteration 406, loss = 55.48834529\n",
            "Iteration 407, loss = 55.37367458\n",
            "Iteration 408, loss = 55.55397213\n",
            "Iteration 409, loss = 55.33216127\n",
            "Iteration 410, loss = 55.33037284\n",
            "Iteration 411, loss = 55.38471044\n",
            "Iteration 412, loss = 55.29904116\n",
            "Iteration 413, loss = 55.27503490\n",
            "Iteration 414, loss = 55.30715709\n",
            "Iteration 415, loss = 55.15763679\n",
            "Iteration 416, loss = 55.26814558\n",
            "Iteration 417, loss = 55.33369185\n",
            "Iteration 418, loss = 55.21128087\n",
            "Iteration 419, loss = 55.17633666\n",
            "Iteration 420, loss = 55.13674815\n",
            "Iteration 421, loss = 55.09792489\n",
            "Iteration 422, loss = 55.07940320\n",
            "Iteration 423, loss = 55.15232493\n",
            "Iteration 424, loss = 55.26902966\n",
            "Iteration 425, loss = 55.11677090\n",
            "Iteration 426, loss = 55.06140355\n",
            "Iteration 427, loss = 55.10808869\n",
            "Iteration 428, loss = 55.32692028\n",
            "Iteration 429, loss = 55.24520238\n",
            "Iteration 430, loss = 55.06785796\n",
            "Iteration 431, loss = 54.83831473\n",
            "Iteration 432, loss = 55.06406807\n",
            "Iteration 433, loss = 55.54030335\n",
            "Iteration 434, loss = 55.13728719\n",
            "Iteration 435, loss = 54.90808063\n",
            "Iteration 436, loss = 54.88889543\n",
            "Iteration 437, loss = 55.31054685\n",
            "Iteration 438, loss = 54.99064796\n",
            "Iteration 439, loss = 54.65601508\n",
            "Iteration 440, loss = 55.29213230\n",
            "Iteration 441, loss = 55.27135995\n",
            "Iteration 442, loss = 55.28745420\n",
            "Iteration 443, loss = 54.82878222\n",
            "Iteration 444, loss = 54.86839527\n",
            "Iteration 445, loss = 54.92213400\n",
            "Iteration 446, loss = 54.78972661\n",
            "Iteration 447, loss = 54.68461687\n",
            "Iteration 448, loss = 54.69846312\n",
            "Iteration 449, loss = 54.73024414\n",
            "Iteration 450, loss = 54.84075991\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 347.68554214\n",
            "Iteration 2, loss = 289.99854047\n",
            "Iteration 3, loss = 241.76590603\n",
            "Iteration 4, loss = 199.52363890\n",
            "Iteration 5, loss = 166.45349682\n",
            "Iteration 6, loss = 142.17280316\n",
            "Iteration 7, loss = 121.68667995\n",
            "Iteration 8, loss = 113.25954275\n",
            "Iteration 9, loss = 104.80267046\n",
            "Iteration 10, loss = 101.92200253\n",
            "Iteration 11, loss = 100.71067857\n",
            "Iteration 12, loss = 101.22685123\n",
            "Iteration 13, loss = 101.31229318\n",
            "Iteration 14, loss = 101.18316534\n",
            "Iteration 15, loss = 99.90586052\n",
            "Iteration 16, loss = 97.92241143\n",
            "Iteration 17, loss = 95.30872029\n",
            "Iteration 18, loss = 92.46718488\n",
            "Iteration 19, loss = 89.31887104\n",
            "Iteration 20, loss = 87.07905011\n",
            "Iteration 21, loss = 84.06501991\n",
            "Iteration 22, loss = 82.25719043\n",
            "Iteration 23, loss = 80.76341349\n",
            "Iteration 24, loss = 79.61155503\n",
            "Iteration 25, loss = 78.63255637\n",
            "Iteration 26, loss = 77.68857946\n",
            "Iteration 27, loss = 76.47271801\n",
            "Iteration 28, loss = 75.28472650\n",
            "Iteration 29, loss = 74.05134244\n",
            "Iteration 30, loss = 72.87970283\n",
            "Iteration 31, loss = 71.47281274\n",
            "Iteration 32, loss = 70.41501472\n",
            "Iteration 33, loss = 69.47745547\n",
            "Iteration 34, loss = 68.60437885\n",
            "Iteration 35, loss = 67.70459927\n",
            "Iteration 36, loss = 66.84917458\n",
            "Iteration 37, loss = 66.12733399\n",
            "Iteration 38, loss = 65.33996014\n",
            "Iteration 39, loss = 64.68174548\n",
            "Iteration 40, loss = 64.06401067\n",
            "Iteration 41, loss = 63.38256474\n",
            "Iteration 42, loss = 62.78232335\n",
            "Iteration 43, loss = 62.22655614\n",
            "Iteration 44, loss = 61.70751775\n",
            "Iteration 45, loss = 61.21668217\n",
            "Iteration 46, loss = 60.83597660\n",
            "Iteration 47, loss = 60.32550357\n",
            "Iteration 48, loss = 59.97884775\n",
            "Iteration 49, loss = 59.58812108\n",
            "Iteration 50, loss = 59.16977921\n",
            "Iteration 51, loss = 58.87454061\n",
            "Iteration 52, loss = 58.58620115\n",
            "Iteration 53, loss = 58.24881218\n",
            "Iteration 54, loss = 58.05860283\n",
            "Iteration 55, loss = 57.75253113\n",
            "Iteration 56, loss = 57.52758394\n",
            "Iteration 57, loss = 57.22189043\n",
            "Iteration 58, loss = 57.07339216\n",
            "Iteration 59, loss = 56.85931850\n",
            "Iteration 60, loss = 56.67775837\n",
            "Iteration 61, loss = 56.52135784\n",
            "Iteration 62, loss = 56.32456278\n",
            "Iteration 63, loss = 56.20468816\n",
            "Iteration 64, loss = 56.05566349\n",
            "Iteration 65, loss = 55.90812289\n",
            "Iteration 66, loss = 55.76100884\n",
            "Iteration 67, loss = 55.63542299\n",
            "Iteration 68, loss = 55.52684138\n",
            "Iteration 69, loss = 55.44744009\n",
            "Iteration 70, loss = 55.44594982\n",
            "Iteration 71, loss = 55.25532944\n",
            "Iteration 72, loss = 55.18864339\n",
            "Iteration 73, loss = 55.14712841\n",
            "Iteration 74, loss = 55.05703106\n",
            "Iteration 75, loss = 54.97236268\n",
            "Iteration 76, loss = 55.01768744\n",
            "Iteration 77, loss = 54.78755171\n",
            "Iteration 78, loss = 54.68482133\n",
            "Iteration 79, loss = 54.54819166\n",
            "Iteration 80, loss = 54.46430349\n",
            "Iteration 81, loss = 54.37820441\n",
            "Iteration 82, loss = 54.32362365\n",
            "Iteration 83, loss = 54.28380996\n",
            "Iteration 84, loss = 54.14237626\n",
            "Iteration 85, loss = 54.13391824\n",
            "Iteration 86, loss = 54.03898134\n",
            "Iteration 87, loss = 53.95561433\n",
            "Iteration 88, loss = 53.92614056\n",
            "Iteration 89, loss = 53.80554586\n",
            "Iteration 90, loss = 53.73078304\n",
            "Iteration 91, loss = 53.65275590\n",
            "Iteration 92, loss = 53.58578048\n",
            "Iteration 93, loss = 53.50462475\n",
            "Iteration 94, loss = 53.44938409\n",
            "Iteration 95, loss = 53.38172585\n",
            "Iteration 96, loss = 53.33166568\n",
            "Iteration 97, loss = 53.26068444\n",
            "Iteration 98, loss = 53.18578136\n",
            "Iteration 99, loss = 53.13830353\n",
            "Iteration 100, loss = 53.03176447\n",
            "Iteration 101, loss = 52.96880303\n",
            "Iteration 102, loss = 52.92254751\n",
            "Iteration 103, loss = 52.91050926\n",
            "Iteration 104, loss = 52.85946220\n",
            "Iteration 105, loss = 52.75006941\n",
            "Iteration 106, loss = 52.64972384\n",
            "Iteration 107, loss = 52.55619784\n",
            "Iteration 108, loss = 52.57049025\n",
            "Iteration 109, loss = 52.48010913\n",
            "Iteration 110, loss = 52.41881592\n",
            "Iteration 111, loss = 52.37849741\n",
            "Iteration 112, loss = 52.30489104\n",
            "Iteration 113, loss = 52.24893379\n",
            "Iteration 114, loss = 52.14644199\n",
            "Iteration 115, loss = 52.06108666\n",
            "Iteration 116, loss = 52.04061036\n",
            "Iteration 117, loss = 51.97399901\n",
            "Iteration 118, loss = 51.91868695\n",
            "Iteration 119, loss = 51.89716906\n",
            "Iteration 120, loss = 51.82959628\n",
            "Iteration 121, loss = 51.70013491\n",
            "Iteration 122, loss = 51.63354274\n",
            "Iteration 123, loss = 51.60925177\n",
            "Iteration 124, loss = 51.60925611\n",
            "Iteration 125, loss = 51.57215747\n",
            "Iteration 126, loss = 51.57541191\n",
            "Iteration 127, loss = 51.56903822\n",
            "Iteration 128, loss = 51.48330187\n",
            "Iteration 129, loss = 51.47685268\n",
            "Iteration 130, loss = 51.34863475\n",
            "Iteration 131, loss = 51.15139560\n",
            "Iteration 132, loss = 51.09637549\n",
            "Iteration 133, loss = 51.03222300\n",
            "Iteration 134, loss = 51.00016867\n",
            "Iteration 135, loss = 50.92096346\n",
            "Iteration 136, loss = 50.86418893\n",
            "Iteration 137, loss = 50.79612811\n",
            "Iteration 138, loss = 50.75485150\n",
            "Iteration 139, loss = 50.71322927\n",
            "Iteration 140, loss = 50.66877705\n",
            "Iteration 141, loss = 50.59131075\n",
            "Iteration 142, loss = 50.52303668\n",
            "Iteration 143, loss = 50.47056999\n",
            "Iteration 144, loss = 50.48847808\n",
            "Iteration 145, loss = 50.40028821\n",
            "Iteration 146, loss = 50.32624583\n",
            "Iteration 147, loss = 50.27171077\n",
            "Iteration 148, loss = 50.27474392\n",
            "Iteration 149, loss = 50.17773433\n",
            "Iteration 150, loss = 50.14861587\n",
            "Iteration 151, loss = 50.04783282\n",
            "Iteration 152, loss = 49.99984022\n",
            "Iteration 153, loss = 49.94705771\n",
            "Iteration 154, loss = 49.89996013\n",
            "Iteration 155, loss = 49.84529821\n",
            "Iteration 156, loss = 49.81830270\n",
            "Iteration 157, loss = 49.75363563\n",
            "Iteration 158, loss = 49.72846493\n",
            "Iteration 159, loss = 49.65216110\n",
            "Iteration 160, loss = 49.59735864\n",
            "Iteration 161, loss = 49.52771083\n",
            "Iteration 162, loss = 49.49247606\n",
            "Iteration 163, loss = 49.44097409\n",
            "Iteration 164, loss = 49.43688245\n",
            "Iteration 165, loss = 49.36014495\n",
            "Iteration 166, loss = 49.28224455\n",
            "Iteration 167, loss = 49.25315138\n",
            "Iteration 168, loss = 49.23604978\n",
            "Iteration 169, loss = 49.16182049\n",
            "Iteration 170, loss = 49.10202730\n",
            "Iteration 171, loss = 49.05153523\n",
            "Iteration 172, loss = 48.96959344\n",
            "Iteration 173, loss = 48.92524679\n",
            "Iteration 174, loss = 49.03005886\n",
            "Iteration 175, loss = 48.97199115\n",
            "Iteration 176, loss = 48.91647491\n",
            "Iteration 177, loss = 48.86336226\n",
            "Iteration 178, loss = 48.74689714\n",
            "Iteration 179, loss = 48.67984363\n",
            "Iteration 180, loss = 48.62578549\n",
            "Iteration 181, loss = 48.62407060\n",
            "Iteration 182, loss = 48.55471057\n",
            "Iteration 183, loss = 48.46978674\n",
            "Iteration 184, loss = 48.47912614\n",
            "Iteration 185, loss = 48.42287503\n",
            "Iteration 186, loss = 48.36727466\n",
            "Iteration 187, loss = 48.35649503\n",
            "Iteration 188, loss = 48.26417348\n",
            "Iteration 189, loss = 48.21162033\n",
            "Iteration 190, loss = 48.22380629\n",
            "Iteration 191, loss = 48.12452798\n",
            "Iteration 192, loss = 48.07942495\n",
            "Iteration 193, loss = 48.07269625\n",
            "Iteration 194, loss = 48.02978855\n",
            "Iteration 195, loss = 48.03156409\n",
            "Iteration 196, loss = 47.91501636\n",
            "Iteration 197, loss = 47.90837486\n",
            "Iteration 198, loss = 47.83481598\n",
            "Iteration 199, loss = 47.94395560\n",
            "Iteration 200, loss = 47.76259802\n",
            "Iteration 201, loss = 47.70768160\n",
            "Iteration 202, loss = 47.69091091\n",
            "Iteration 203, loss = 47.64463707\n",
            "Iteration 204, loss = 47.65365667\n",
            "Iteration 205, loss = 47.65727064\n",
            "Iteration 206, loss = 47.68087626\n",
            "Iteration 207, loss = 47.54520873\n",
            "Iteration 208, loss = 47.55908214\n",
            "Iteration 209, loss = 47.49849771\n",
            "Iteration 210, loss = 47.44558251\n",
            "Iteration 211, loss = 47.34671353\n",
            "Iteration 212, loss = 47.34641633\n",
            "Iteration 213, loss = 47.27456850\n",
            "Iteration 214, loss = 47.20648700\n",
            "Iteration 215, loss = 47.17394221\n",
            "Iteration 216, loss = 47.13623461\n",
            "Iteration 217, loss = 47.33485904\n",
            "Iteration 218, loss = 47.19716907\n",
            "Iteration 219, loss = 47.03878913\n",
            "Iteration 220, loss = 47.07964807\n",
            "Iteration 221, loss = 46.98566518\n",
            "Iteration 222, loss = 46.91195585\n",
            "Iteration 223, loss = 46.86695628\n",
            "Iteration 224, loss = 46.90746012\n",
            "Iteration 225, loss = 46.87060296\n",
            "Iteration 226, loss = 46.80397511\n",
            "Iteration 227, loss = 46.75653698\n",
            "Iteration 228, loss = 46.72668183\n",
            "Iteration 229, loss = 46.64936189\n",
            "Iteration 230, loss = 46.64213562\n",
            "Iteration 231, loss = 46.65778593\n",
            "Iteration 232, loss = 46.65160646\n",
            "Iteration 233, loss = 46.59934897\n",
            "Iteration 234, loss = 46.50035971\n",
            "Iteration 235, loss = 46.47774539\n",
            "Iteration 236, loss = 46.50144307\n",
            "Iteration 237, loss = 46.43471382\n",
            "Iteration 238, loss = 46.38393734\n",
            "Iteration 239, loss = 46.44947510\n",
            "Iteration 240, loss = 46.31279020\n",
            "Iteration 241, loss = 46.33735099\n",
            "Iteration 242, loss = 46.22605001\n",
            "Iteration 243, loss = 46.24983369\n",
            "Iteration 244, loss = 46.18966606\n",
            "Iteration 245, loss = 46.16643254\n",
            "Iteration 246, loss = 46.19135481\n",
            "Iteration 247, loss = 46.11304640\n",
            "Iteration 248, loss = 46.08801763\n",
            "Iteration 249, loss = 46.02193795\n",
            "Iteration 250, loss = 45.98539616\n",
            "Iteration 251, loss = 45.93898136\n",
            "Iteration 252, loss = 45.93228937\n",
            "Iteration 253, loss = 45.91130459\n",
            "Iteration 254, loss = 45.84220769\n",
            "Iteration 255, loss = 45.80610838\n",
            "Iteration 256, loss = 45.78262201\n",
            "Iteration 257, loss = 45.75834001\n",
            "Iteration 258, loss = 45.71431530\n",
            "Iteration 259, loss = 45.71491735\n",
            "Iteration 260, loss = 45.67118937\n",
            "Iteration 261, loss = 45.64840046\n",
            "Iteration 262, loss = 45.61709152\n",
            "Iteration 263, loss = 45.58740128\n",
            "Iteration 264, loss = 45.57220787\n",
            "Iteration 265, loss = 45.51524011\n",
            "Iteration 266, loss = 45.49376277\n",
            "Iteration 267, loss = 45.48835927\n",
            "Iteration 268, loss = 45.46953332\n",
            "Iteration 269, loss = 45.46741168\n",
            "Iteration 270, loss = 45.40786856\n",
            "Iteration 271, loss = 45.35594051\n",
            "Iteration 272, loss = 45.34649843\n",
            "Iteration 273, loss = 45.29625486\n",
            "Iteration 274, loss = 45.28857118\n",
            "Iteration 275, loss = 45.25060462\n",
            "Iteration 276, loss = 45.22469391\n",
            "Iteration 277, loss = 45.21651138\n",
            "Iteration 278, loss = 45.18509755\n",
            "Iteration 279, loss = 45.16162415\n",
            "Iteration 280, loss = 45.12914696\n",
            "Iteration 281, loss = 45.11284557\n",
            "Iteration 282, loss = 45.08528952\n",
            "Iteration 283, loss = 45.05046005\n",
            "Iteration 284, loss = 45.01517042\n",
            "Iteration 285, loss = 45.02887555\n",
            "Iteration 286, loss = 45.04347580\n",
            "Iteration 287, loss = 45.00694649\n",
            "Iteration 288, loss = 44.92562690\n",
            "Iteration 289, loss = 44.91646739\n",
            "Iteration 290, loss = 44.88918836\n",
            "Iteration 291, loss = 44.89570641\n",
            "Iteration 292, loss = 44.88412156\n",
            "Iteration 293, loss = 44.94754315\n",
            "Iteration 294, loss = 44.82190082\n",
            "Iteration 295, loss = 44.81920193\n",
            "Iteration 296, loss = 44.78981648\n",
            "Iteration 297, loss = 44.86454381\n",
            "Iteration 298, loss = 44.69476258\n",
            "Iteration 299, loss = 44.67108127\n",
            "Iteration 300, loss = 44.65038359\n",
            "Iteration 301, loss = 44.61922366\n",
            "Iteration 302, loss = 44.61177072\n",
            "Iteration 303, loss = 44.59386083\n",
            "Iteration 304, loss = 44.58024660\n",
            "Iteration 305, loss = 44.53494866\n",
            "Iteration 306, loss = 44.57310281\n",
            "Iteration 307, loss = 44.58876594\n",
            "Iteration 308, loss = 44.59454158\n",
            "Iteration 309, loss = 44.55639025\n",
            "Iteration 310, loss = 44.47927692\n",
            "Iteration 311, loss = 44.44284548\n",
            "Iteration 312, loss = 44.38652764\n",
            "Iteration 313, loss = 44.41918203\n",
            "Iteration 314, loss = 44.48295010\n",
            "Iteration 315, loss = 44.44923031\n",
            "Iteration 316, loss = 44.38219462\n",
            "Iteration 317, loss = 44.28886463\n",
            "Iteration 318, loss = 44.27124940\n",
            "Iteration 319, loss = 44.27320458\n",
            "Iteration 320, loss = 44.33652285\n",
            "Iteration 321, loss = 44.46567434\n",
            "Iteration 322, loss = 44.18140609\n",
            "Iteration 323, loss = 44.40740943\n",
            "Iteration 324, loss = 44.23170814\n",
            "Iteration 325, loss = 44.16440216\n",
            "Iteration 326, loss = 44.08459968\n",
            "Iteration 327, loss = 44.07139435\n",
            "Iteration 328, loss = 44.23246114\n",
            "Iteration 329, loss = 44.39998114\n",
            "Iteration 330, loss = 44.31942145\n",
            "Iteration 331, loss = 44.23289277\n",
            "Iteration 332, loss = 44.10969271\n",
            "Iteration 333, loss = 43.96629418\n",
            "Iteration 334, loss = 43.99108230\n",
            "Iteration 335, loss = 44.14591788\n",
            "Iteration 336, loss = 44.15086742\n",
            "Iteration 337, loss = 44.21772928\n",
            "Iteration 338, loss = 44.11407241\n",
            "Iteration 339, loss = 43.94571473\n",
            "Iteration 340, loss = 43.94647195\n",
            "Iteration 341, loss = 43.90091290\n",
            "Iteration 342, loss = 43.95798504\n",
            "Iteration 343, loss = 44.00644454\n",
            "Iteration 344, loss = 43.93660711\n",
            "Iteration 345, loss = 43.86717575\n",
            "Iteration 346, loss = 43.84059833\n",
            "Iteration 347, loss = 43.80398653\n",
            "Iteration 348, loss = 43.76650483\n",
            "Iteration 349, loss = 43.92346320\n",
            "Iteration 350, loss = 43.86737114\n",
            "Iteration 351, loss = 43.74229163\n",
            "Iteration 352, loss = 43.81152073\n",
            "Iteration 353, loss = 43.71957957\n",
            "Iteration 354, loss = 43.72068782\n",
            "Iteration 355, loss = 43.86050369\n",
            "Iteration 356, loss = 43.64884136\n",
            "Iteration 357, loss = 43.67929447\n",
            "Iteration 358, loss = 43.67076367\n",
            "Iteration 359, loss = 43.66165576\n",
            "Iteration 360, loss = 43.67537295\n",
            "Iteration 361, loss = 43.56466828\n",
            "Iteration 362, loss = 43.57070376\n",
            "Iteration 363, loss = 43.65334889\n",
            "Iteration 364, loss = 43.76749305\n",
            "Iteration 365, loss = 43.66097651\n",
            "Iteration 366, loss = 43.61582114\n",
            "Iteration 367, loss = 43.51407540\n",
            "Iteration 368, loss = 43.44359081\n",
            "Iteration 369, loss = 43.53247774\n",
            "Iteration 370, loss = 43.65586778\n",
            "Iteration 371, loss = 43.64455350\n",
            "Iteration 372, loss = 43.52773962\n",
            "Iteration 373, loss = 43.53713756\n",
            "Iteration 374, loss = 43.41589045\n",
            "Iteration 375, loss = 43.54181424\n",
            "Iteration 376, loss = 43.51778705\n",
            "Iteration 377, loss = 43.38926891\n",
            "Iteration 378, loss = 43.40800774\n",
            "Iteration 379, loss = 43.37367219\n",
            "Iteration 380, loss = 43.42560056\n",
            "Iteration 381, loss = 43.35249889\n",
            "Iteration 382, loss = 43.48873423\n",
            "Iteration 383, loss = 43.37344967\n",
            "Iteration 384, loss = 43.29586426\n",
            "Iteration 385, loss = 43.27423468\n",
            "Iteration 386, loss = 43.39871830\n",
            "Iteration 387, loss = 43.47247189\n",
            "Iteration 388, loss = 43.41385969\n",
            "Iteration 389, loss = 43.32271135\n",
            "Iteration 390, loss = 43.28929660\n",
            "Iteration 391, loss = 43.23464548\n",
            "Iteration 392, loss = 43.26100508\n",
            "Iteration 393, loss = 43.27844424\n",
            "Iteration 394, loss = 43.23595617\n",
            "Iteration 395, loss = 43.26239521\n",
            "Iteration 396, loss = 43.16029725\n",
            "Iteration 397, loss = 43.16988172\n",
            "Iteration 398, loss = 43.20590048\n",
            "Iteration 399, loss = 43.18094752\n",
            "Iteration 400, loss = 43.15361112\n",
            "Iteration 401, loss = 43.12101816\n",
            "Iteration 402, loss = 43.16998655\n",
            "Iteration 403, loss = 43.13690577\n",
            "Iteration 404, loss = 43.12810688\n",
            "Iteration 405, loss = 43.06582091\n",
            "Iteration 406, loss = 43.06407209\n",
            "Iteration 407, loss = 43.09319201\n",
            "Iteration 408, loss = 43.10269315\n",
            "Iteration 409, loss = 42.99881666\n",
            "Iteration 410, loss = 43.01106788\n",
            "Iteration 411, loss = 43.13214473\n",
            "Iteration 412, loss = 43.26459781\n",
            "Iteration 413, loss = 43.19688240\n",
            "Iteration 414, loss = 43.11809491\n",
            "Iteration 415, loss = 43.12448318\n",
            "Iteration 416, loss = 42.96169021\n",
            "Iteration 417, loss = 42.96115442\n",
            "Iteration 418, loss = 42.96233451\n",
            "Iteration 419, loss = 42.97332490\n",
            "Iteration 420, loss = 42.99597159\n",
            "Iteration 421, loss = 42.93586223\n",
            "Iteration 422, loss = 42.91782082\n",
            "Iteration 423, loss = 42.90049774\n",
            "Iteration 424, loss = 42.92909706\n",
            "Iteration 425, loss = 42.89026351\n",
            "Iteration 426, loss = 43.06302904\n",
            "Iteration 427, loss = 42.91143312\n",
            "Iteration 428, loss = 42.82211188\n",
            "Iteration 429, loss = 42.95948265\n",
            "Iteration 430, loss = 43.02088525\n",
            "Iteration 431, loss = 43.04573981\n",
            "Iteration 432, loss = 42.98652738\n",
            "Iteration 433, loss = 42.88972061\n",
            "Iteration 434, loss = 43.00907200\n",
            "Iteration 435, loss = 42.92484420\n",
            "Iteration 436, loss = 42.86197766\n",
            "Iteration 437, loss = 42.78605926\n",
            "Iteration 438, loss = 42.81414296\n",
            "Iteration 439, loss = 42.83349464\n",
            "Iteration 440, loss = 42.82829494\n",
            "Iteration 441, loss = 42.76648115\n",
            "Iteration 442, loss = 42.76310485\n",
            "Iteration 443, loss = 42.79475587\n",
            "Iteration 444, loss = 42.83961974\n",
            "Iteration 445, loss = 42.84412880\n",
            "Iteration 446, loss = 42.82696782\n",
            "Iteration 447, loss = 42.78316902\n",
            "Iteration 448, loss = 42.72931264\n",
            "Iteration 449, loss = 42.75745381\n",
            "Iteration 450, loss = 42.75794690\n",
            "Iteration 451, loss = 43.10819388\n",
            "Iteration 452, loss = 42.71069335\n",
            "Iteration 453, loss = 42.70665728\n",
            "Iteration 454, loss = 42.64565515\n",
            "Iteration 455, loss = 42.82789803\n",
            "Iteration 456, loss = 42.65890086\n",
            "Iteration 457, loss = 42.69737681\n",
            "Iteration 458, loss = 42.65876689\n",
            "Iteration 459, loss = 42.65033702\n",
            "Iteration 460, loss = 42.63670120\n",
            "Iteration 461, loss = 42.71006834\n",
            "Iteration 462, loss = 42.73207867\n",
            "Iteration 463, loss = 42.73608843\n",
            "Iteration 464, loss = 42.62691358\n",
            "Iteration 465, loss = 42.58756245\n",
            "Iteration 466, loss = 42.63268736\n",
            "Iteration 467, loss = 42.69024327\n",
            "Iteration 468, loss = 42.66052493\n",
            "Iteration 469, loss = 42.61615916\n",
            "Iteration 470, loss = 42.63622605\n",
            "Iteration 471, loss = 42.57836743\n",
            "Iteration 472, loss = 42.56393715\n",
            "Iteration 473, loss = 42.54957885\n",
            "Iteration 474, loss = 42.55820461\n",
            "Iteration 475, loss = 42.56318313\n",
            "Iteration 476, loss = 42.52494071\n",
            "Iteration 477, loss = 42.53590493\n",
            "Iteration 478, loss = 42.55673547\n",
            "Iteration 479, loss = 42.62175956\n",
            "Iteration 480, loss = 42.50904620\n",
            "Iteration 481, loss = 42.46446097\n",
            "Iteration 482, loss = 42.56776090\n",
            "Iteration 483, loss = 42.49285737\n",
            "Iteration 484, loss = 42.47458569\n",
            "Iteration 485, loss = 42.45336061\n",
            "Iteration 486, loss = 42.54550474\n",
            "Iteration 487, loss = 42.62978798\n",
            "Iteration 488, loss = 42.54989951\n",
            "Iteration 489, loss = 42.48576886\n",
            "Iteration 490, loss = 42.43362235\n",
            "Iteration 491, loss = 42.51379282\n",
            "Iteration 492, loss = 42.56747856\n",
            "Iteration 493, loss = 42.57222344\n",
            "Iteration 494, loss = 42.43135187\n",
            "Iteration 495, loss = 42.46299080\n",
            "Iteration 496, loss = 42.47032345\n",
            "Iteration 497, loss = 42.64462286\n",
            "Iteration 498, loss = 42.48230647\n",
            "Iteration 499, loss = 42.55060586\n",
            "Iteration 500, loss = 42.38891216\n",
            "Iteration 501, loss = 42.38219097\n",
            "Iteration 502, loss = 42.34270306\n",
            "Iteration 503, loss = 42.35323212\n",
            "Iteration 504, loss = 42.48271504\n",
            "Iteration 505, loss = 42.50893646\n",
            "Iteration 506, loss = 42.50098883\n",
            "Iteration 507, loss = 42.38285551\n",
            "Iteration 508, loss = 42.35838965\n",
            "Iteration 509, loss = 42.35196986\n",
            "Iteration 510, loss = 42.31776138\n",
            "Iteration 511, loss = 42.30692041\n",
            "Iteration 512, loss = 42.36058939\n",
            "Iteration 513, loss = 42.29784041\n",
            "Iteration 514, loss = 42.45645732\n",
            "Iteration 515, loss = 42.29540263\n",
            "Iteration 516, loss = 42.39259991\n",
            "Iteration 517, loss = 42.32054239\n",
            "Iteration 518, loss = 42.32456579\n",
            "Iteration 519, loss = 42.29556517\n",
            "Iteration 520, loss = 42.26548160\n",
            "Iteration 521, loss = 42.43850163\n",
            "Iteration 522, loss = 42.36282460\n",
            "Iteration 523, loss = 42.32947750\n",
            "Iteration 524, loss = 42.27151721\n",
            "Iteration 525, loss = 42.25798844\n",
            "Iteration 526, loss = 42.39739043\n",
            "Iteration 527, loss = 42.34992505\n",
            "Iteration 528, loss = 42.25513879\n",
            "Iteration 529, loss = 42.22536702\n",
            "Iteration 530, loss = 42.27046055\n",
            "Iteration 531, loss = 42.24046379\n",
            "Iteration 532, loss = 42.21577743\n",
            "Iteration 533, loss = 42.26936293\n",
            "Iteration 534, loss = 42.20197066\n",
            "Iteration 535, loss = 42.21815205\n",
            "Iteration 536, loss = 42.22455713\n",
            "Iteration 537, loss = 42.20592452\n",
            "Iteration 538, loss = 42.25605161\n",
            "Iteration 539, loss = 42.23290976\n",
            "Iteration 540, loss = 42.37206767\n",
            "Iteration 541, loss = 42.20895191\n",
            "Iteration 542, loss = 42.20403176\n",
            "Iteration 543, loss = 42.26030547\n",
            "Iteration 544, loss = 42.24541913\n",
            "Iteration 545, loss = 42.23537314\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 275.86124446\n",
            "Iteration 2, loss = 226.05695459\n",
            "Iteration 3, loss = 186.90457137\n",
            "Iteration 4, loss = 150.97658040\n",
            "Iteration 5, loss = 129.28321758\n",
            "Iteration 6, loss = 109.14093986\n",
            "Iteration 7, loss = 98.59841260\n",
            "Iteration 8, loss = 93.75237776\n",
            "Iteration 9, loss = 91.38573289\n",
            "Iteration 10, loss = 91.37102316\n",
            "Iteration 11, loss = 91.94823589\n",
            "Iteration 12, loss = 93.25783060\n",
            "Iteration 13, loss = 92.80284646\n",
            "Iteration 14, loss = 91.50538633\n",
            "Iteration 15, loss = 89.76499692\n",
            "Iteration 16, loss = 87.39824190\n",
            "Iteration 17, loss = 84.70271188\n",
            "Iteration 18, loss = 82.52468980\n",
            "Iteration 19, loss = 80.70500663\n",
            "Iteration 20, loss = 79.90343078\n",
            "Iteration 21, loss = 78.73605741\n",
            "Iteration 22, loss = 77.90510668\n",
            "Iteration 23, loss = 77.46660038\n",
            "Iteration 24, loss = 76.67174905\n",
            "Iteration 25, loss = 76.07360409\n",
            "Iteration 26, loss = 75.43970815\n",
            "Iteration 27, loss = 74.83095037\n",
            "Iteration 28, loss = 74.11846920\n",
            "Iteration 29, loss = 73.51617170\n",
            "Iteration 30, loss = 73.07958167\n",
            "Iteration 31, loss = 72.65379264\n",
            "Iteration 32, loss = 72.10578174\n",
            "Iteration 33, loss = 71.76857336\n",
            "Iteration 34, loss = 71.40009938\n",
            "Iteration 35, loss = 71.12618507\n",
            "Iteration 36, loss = 70.79241748\n",
            "Iteration 37, loss = 70.46908674\n",
            "Iteration 38, loss = 70.22867971\n",
            "Iteration 39, loss = 69.94509667\n",
            "Iteration 40, loss = 69.69999186\n",
            "Iteration 41, loss = 69.45756061\n",
            "Iteration 42, loss = 69.22222157\n",
            "Iteration 43, loss = 69.07472058\n",
            "Iteration 44, loss = 68.86269632\n",
            "Iteration 45, loss = 68.68047745\n",
            "Iteration 46, loss = 68.51381420\n",
            "Iteration 47, loss = 68.29236637\n",
            "Iteration 48, loss = 68.15504238\n",
            "Iteration 49, loss = 67.97807009\n",
            "Iteration 50, loss = 67.85022303\n",
            "Iteration 51, loss = 67.68853996\n",
            "Iteration 52, loss = 67.54737771\n",
            "Iteration 53, loss = 67.44318087\n",
            "Iteration 54, loss = 67.26455642\n",
            "Iteration 55, loss = 67.17280696\n",
            "Iteration 56, loss = 67.00833653\n",
            "Iteration 57, loss = 66.93976652\n",
            "Iteration 58, loss = 66.78879235\n",
            "Iteration 59, loss = 66.64729245\n",
            "Iteration 60, loss = 66.56354490\n",
            "Iteration 61, loss = 66.38788841\n",
            "Iteration 62, loss = 66.28630834\n",
            "Iteration 63, loss = 66.16927166\n",
            "Iteration 64, loss = 66.09862079\n",
            "Iteration 65, loss = 65.96040359\n",
            "Iteration 66, loss = 65.84992718\n",
            "Iteration 67, loss = 65.68978183\n",
            "Iteration 68, loss = 65.56468925\n",
            "Iteration 69, loss = 65.50505319\n",
            "Iteration 70, loss = 65.42219783\n",
            "Iteration 71, loss = 65.23415396\n",
            "Iteration 72, loss = 65.12373524\n",
            "Iteration 73, loss = 65.05994494\n",
            "Iteration 74, loss = 64.96723987\n",
            "Iteration 75, loss = 64.83942813\n",
            "Iteration 76, loss = 64.68611070\n",
            "Iteration 77, loss = 64.54459892\n",
            "Iteration 78, loss = 64.46769422\n",
            "Iteration 79, loss = 64.37082778\n",
            "Iteration 80, loss = 64.26279445\n",
            "Iteration 81, loss = 64.11875457\n",
            "Iteration 82, loss = 63.97309810\n",
            "Iteration 83, loss = 63.91864825\n",
            "Iteration 84, loss = 63.79753304\n",
            "Iteration 85, loss = 63.64941262\n",
            "Iteration 86, loss = 63.54466514\n",
            "Iteration 87, loss = 63.44127113\n",
            "Iteration 88, loss = 63.32228276\n",
            "Iteration 89, loss = 63.22946952\n",
            "Iteration 90, loss = 63.10039556\n",
            "Iteration 91, loss = 63.04697612\n",
            "Iteration 92, loss = 62.88561418\n",
            "Iteration 93, loss = 62.76550886\n",
            "Iteration 94, loss = 62.64234253\n",
            "Iteration 95, loss = 62.52905373\n",
            "Iteration 96, loss = 62.62584526\n",
            "Iteration 97, loss = 62.36908881\n",
            "Iteration 98, loss = 62.23169027\n",
            "Iteration 99, loss = 62.20465463\n",
            "Iteration 100, loss = 62.15052915\n",
            "Iteration 101, loss = 61.90565484\n",
            "Iteration 102, loss = 61.79239840\n",
            "Iteration 103, loss = 61.72840000\n",
            "Iteration 104, loss = 61.59811867\n",
            "Iteration 105, loss = 61.48585272\n",
            "Iteration 106, loss = 61.35624476\n",
            "Iteration 107, loss = 61.26547658\n",
            "Iteration 108, loss = 61.16799354\n",
            "Iteration 109, loss = 61.08118941\n",
            "Iteration 110, loss = 60.97147140\n",
            "Iteration 111, loss = 60.81825323\n",
            "Iteration 112, loss = 60.71738274\n",
            "Iteration 113, loss = 60.65248258\n",
            "Iteration 114, loss = 60.63382942\n",
            "Iteration 115, loss = 60.50770237\n",
            "Iteration 116, loss = 60.49994297\n",
            "Iteration 117, loss = 60.29733711\n",
            "Iteration 118, loss = 60.16131440\n",
            "Iteration 119, loss = 59.99627225\n",
            "Iteration 120, loss = 59.89497551\n",
            "Iteration 121, loss = 59.83147203\n",
            "Iteration 122, loss = 59.70405271\n",
            "Iteration 123, loss = 59.66540882\n",
            "Iteration 124, loss = 59.59597857\n",
            "Iteration 125, loss = 59.48122993\n",
            "Iteration 126, loss = 59.34749964\n",
            "Iteration 127, loss = 59.20287574\n",
            "Iteration 128, loss = 59.07218427\n",
            "Iteration 129, loss = 59.13280314\n",
            "Iteration 130, loss = 58.90532983\n",
            "Iteration 131, loss = 58.78751032\n",
            "Iteration 132, loss = 58.75285152\n",
            "Iteration 133, loss = 58.64223285\n",
            "Iteration 134, loss = 58.66450062\n",
            "Iteration 135, loss = 58.41112705\n",
            "Iteration 136, loss = 58.28846093\n",
            "Iteration 137, loss = 58.21635114\n",
            "Iteration 138, loss = 58.11440888\n",
            "Iteration 139, loss = 58.08586338\n",
            "Iteration 140, loss = 57.88623896\n",
            "Iteration 141, loss = 57.79577721\n",
            "Iteration 142, loss = 57.69579807\n",
            "Iteration 143, loss = 57.61735384\n",
            "Iteration 144, loss = 57.52722397\n",
            "Iteration 145, loss = 57.44503828\n",
            "Iteration 146, loss = 57.32610095\n",
            "Iteration 147, loss = 57.23066026\n",
            "Iteration 148, loss = 57.11725958\n",
            "Iteration 149, loss = 57.00784772\n",
            "Iteration 150, loss = 56.96862899\n",
            "Iteration 151, loss = 56.85353659\n",
            "Iteration 152, loss = 56.80634673\n",
            "Iteration 153, loss = 56.78067773\n",
            "Iteration 154, loss = 56.75766512\n",
            "Iteration 155, loss = 56.66779309\n",
            "Iteration 156, loss = 56.37772097\n",
            "Iteration 157, loss = 56.27409514\n",
            "Iteration 158, loss = 56.16978902\n",
            "Iteration 159, loss = 56.04906663\n",
            "Iteration 160, loss = 56.14879535\n",
            "Iteration 161, loss = 55.89318578\n",
            "Iteration 162, loss = 55.75281385\n",
            "Iteration 163, loss = 55.66142352\n",
            "Iteration 164, loss = 55.90822874\n",
            "Iteration 165, loss = 55.56819281\n",
            "Iteration 166, loss = 55.36541124\n",
            "Iteration 167, loss = 55.26958275\n",
            "Iteration 168, loss = 55.19911611\n",
            "Iteration 169, loss = 55.12391573\n",
            "Iteration 170, loss = 55.06568434\n",
            "Iteration 171, loss = 54.97585339\n",
            "Iteration 172, loss = 54.86128927\n",
            "Iteration 173, loss = 54.74836577\n",
            "Iteration 174, loss = 54.70973445\n",
            "Iteration 175, loss = 54.58971848\n",
            "Iteration 176, loss = 54.48641534\n",
            "Iteration 177, loss = 54.40412584\n",
            "Iteration 178, loss = 54.29558227\n",
            "Iteration 179, loss = 54.20180018\n",
            "Iteration 180, loss = 54.17454872\n",
            "Iteration 181, loss = 53.99981741\n",
            "Iteration 182, loss = 53.91887681\n",
            "Iteration 183, loss = 53.85037555\n",
            "Iteration 184, loss = 53.76753709\n",
            "Iteration 185, loss = 53.70339563\n",
            "Iteration 186, loss = 53.61030356\n",
            "Iteration 187, loss = 53.49797260\n",
            "Iteration 188, loss = 53.44747839\n",
            "Iteration 189, loss = 53.32920450\n",
            "Iteration 190, loss = 53.21561208\n",
            "Iteration 191, loss = 53.14189200\n",
            "Iteration 192, loss = 53.17705319\n",
            "Iteration 193, loss = 53.05987094\n",
            "Iteration 194, loss = 52.90026777\n",
            "Iteration 195, loss = 52.80528177\n",
            "Iteration 196, loss = 52.72641582\n",
            "Iteration 197, loss = 52.91600289\n",
            "Iteration 198, loss = 52.63797801\n",
            "Iteration 199, loss = 52.47281137\n",
            "Iteration 200, loss = 52.44634517\n",
            "Iteration 201, loss = 52.33523445\n",
            "Iteration 202, loss = 52.28549337\n",
            "Iteration 203, loss = 52.21699718\n",
            "Iteration 204, loss = 52.12102816\n",
            "Iteration 205, loss = 52.06034601\n",
            "Iteration 206, loss = 51.90284472\n",
            "Iteration 207, loss = 51.83459217\n",
            "Iteration 208, loss = 51.75480486\n",
            "Iteration 209, loss = 51.67168766\n",
            "Iteration 210, loss = 51.63260654\n",
            "Iteration 211, loss = 51.53703758\n",
            "Iteration 212, loss = 51.44096265\n",
            "Iteration 213, loss = 51.40110516\n",
            "Iteration 214, loss = 51.31573131\n",
            "Iteration 215, loss = 51.21943323\n",
            "Iteration 216, loss = 51.16805725\n",
            "Iteration 217, loss = 51.05264202\n",
            "Iteration 218, loss = 50.93754416\n",
            "Iteration 219, loss = 50.86469697\n",
            "Iteration 220, loss = 51.00929960\n",
            "Iteration 221, loss = 50.99334254\n",
            "Iteration 222, loss = 50.84468798\n",
            "Iteration 223, loss = 50.69877525\n",
            "Iteration 224, loss = 50.64364899\n",
            "Iteration 225, loss = 50.51271934\n",
            "Iteration 226, loss = 50.39658860\n",
            "Iteration 227, loss = 50.30140363\n",
            "Iteration 228, loss = 50.24735197\n",
            "Iteration 229, loss = 50.20998046\n",
            "Iteration 230, loss = 50.08235826\n",
            "Iteration 231, loss = 50.07534027\n",
            "Iteration 232, loss = 50.01733188\n",
            "Iteration 233, loss = 49.95703094\n",
            "Iteration 234, loss = 49.96819653\n",
            "Iteration 235, loss = 49.82276343\n",
            "Iteration 236, loss = 49.75090047\n",
            "Iteration 237, loss = 49.62064920\n",
            "Iteration 238, loss = 49.53552876\n",
            "Iteration 239, loss = 49.53854324\n",
            "Iteration 240, loss = 49.63606138\n",
            "Iteration 241, loss = 49.41209451\n",
            "Iteration 242, loss = 49.41783148\n",
            "Iteration 243, loss = 49.34136304\n",
            "Iteration 244, loss = 49.23920279\n",
            "Iteration 245, loss = 49.08435155\n",
            "Iteration 246, loss = 49.25405655\n",
            "Iteration 247, loss = 49.14893511\n",
            "Iteration 248, loss = 49.01734518\n",
            "Iteration 249, loss = 48.86331956\n",
            "Iteration 250, loss = 48.80697258\n",
            "Iteration 251, loss = 48.80770409\n",
            "Iteration 252, loss = 48.81990944\n",
            "Iteration 253, loss = 48.70011222\n",
            "Iteration 254, loss = 48.65229381\n",
            "Iteration 255, loss = 48.58695422\n",
            "Iteration 256, loss = 48.42359749\n",
            "Iteration 257, loss = 48.38406973\n",
            "Iteration 258, loss = 48.55169487\n",
            "Iteration 259, loss = 48.46993925\n",
            "Iteration 260, loss = 48.19619521\n",
            "Iteration 261, loss = 48.12586774\n",
            "Iteration 262, loss = 48.21512936\n",
            "Iteration 263, loss = 48.34410130\n",
            "Iteration 264, loss = 48.25849292\n",
            "Iteration 265, loss = 48.12430912\n",
            "Iteration 266, loss = 47.89154441\n",
            "Iteration 267, loss = 48.17340009\n",
            "Iteration 268, loss = 47.81327964\n",
            "Iteration 269, loss = 47.76055809\n",
            "Iteration 270, loss = 47.61113811\n",
            "Iteration 271, loss = 47.84937287\n",
            "Iteration 272, loss = 47.65357901\n",
            "Iteration 273, loss = 47.63369380\n",
            "Iteration 274, loss = 47.42322492\n",
            "Iteration 275, loss = 47.65346029\n",
            "Iteration 276, loss = 47.41598999\n",
            "Iteration 277, loss = 47.28154022\n",
            "Iteration 278, loss = 47.26414687\n",
            "Iteration 279, loss = 47.18904289\n",
            "Iteration 280, loss = 47.19816588\n",
            "Iteration 281, loss = 47.09547768\n",
            "Iteration 282, loss = 47.03974751\n",
            "Iteration 283, loss = 47.04227652\n",
            "Iteration 284, loss = 46.98975620\n",
            "Iteration 285, loss = 46.93369369\n",
            "Iteration 286, loss = 46.87577743\n",
            "Iteration 287, loss = 46.82780865\n",
            "Iteration 288, loss = 46.90456634\n",
            "Iteration 289, loss = 46.76222944\n",
            "Iteration 290, loss = 46.69506110\n",
            "Iteration 291, loss = 46.61629332\n",
            "Iteration 292, loss = 46.59814878\n",
            "Iteration 293, loss = 46.71874123\n",
            "Iteration 294, loss = 46.47682575\n",
            "Iteration 295, loss = 46.37875943\n",
            "Iteration 296, loss = 46.43627688\n",
            "Iteration 297, loss = 46.64165225\n",
            "Iteration 298, loss = 46.71022478\n",
            "Iteration 299, loss = 46.49904359\n",
            "Iteration 300, loss = 46.44785711\n",
            "Iteration 301, loss = 46.20708625\n",
            "Iteration 302, loss = 46.23675210\n",
            "Iteration 303, loss = 46.19101798\n",
            "Iteration 304, loss = 46.28073478\n",
            "Iteration 305, loss = 46.08198957\n",
            "Iteration 306, loss = 46.04841182\n",
            "Iteration 307, loss = 46.01188117\n",
            "Iteration 308, loss = 45.93795065\n",
            "Iteration 309, loss = 45.89754908\n",
            "Iteration 310, loss = 45.84286888\n",
            "Iteration 311, loss = 45.91032096\n",
            "Iteration 312, loss = 45.77018735\n",
            "Iteration 313, loss = 45.76759058\n",
            "Iteration 314, loss = 45.78528843\n",
            "Iteration 315, loss = 45.79788611\n",
            "Iteration 316, loss = 45.72973813\n",
            "Iteration 317, loss = 45.60332779\n",
            "Iteration 318, loss = 45.58682128\n",
            "Iteration 319, loss = 45.52453357\n",
            "Iteration 320, loss = 45.50263712\n",
            "Iteration 321, loss = 45.45654354\n",
            "Iteration 322, loss = 45.47745405\n",
            "Iteration 323, loss = 45.39868701\n",
            "Iteration 324, loss = 45.48755524\n",
            "Iteration 325, loss = 45.28775789\n",
            "Iteration 326, loss = 45.28191257\n",
            "Iteration 327, loss = 45.54319873\n",
            "Iteration 328, loss = 45.61498011\n",
            "Iteration 329, loss = 45.38513380\n",
            "Iteration 330, loss = 45.17278824\n",
            "Iteration 331, loss = 45.31305609\n",
            "Iteration 332, loss = 45.23491646\n",
            "Iteration 333, loss = 45.16225097\n",
            "Iteration 334, loss = 45.04118733\n",
            "Iteration 335, loss = 44.93723358\n",
            "Iteration 336, loss = 44.95893178\n",
            "Iteration 337, loss = 45.15628134\n",
            "Iteration 338, loss = 45.14433597\n",
            "Iteration 339, loss = 44.93995172\n",
            "Iteration 340, loss = 45.03362202\n",
            "Iteration 341, loss = 44.88017310\n",
            "Iteration 342, loss = 44.91253676\n",
            "Iteration 343, loss = 44.85847546\n",
            "Iteration 344, loss = 44.73787151\n",
            "Iteration 345, loss = 44.90641017\n",
            "Iteration 346, loss = 44.76771940\n",
            "Iteration 347, loss = 44.72461656\n",
            "Iteration 348, loss = 44.66632395\n",
            "Iteration 349, loss = 44.70632115\n",
            "Iteration 350, loss = 44.59865222\n",
            "Iteration 351, loss = 44.62858694\n",
            "Iteration 352, loss = 44.75764567\n",
            "Iteration 353, loss = 44.55933043\n",
            "Iteration 354, loss = 44.50526217\n",
            "Iteration 355, loss = 44.57850692\n",
            "Iteration 356, loss = 44.45331049\n",
            "Iteration 357, loss = 44.52035244\n",
            "Iteration 358, loss = 44.49200290\n",
            "Iteration 359, loss = 44.33360473\n",
            "Iteration 360, loss = 44.33789896\n",
            "Iteration 361, loss = 44.29285344\n",
            "Iteration 362, loss = 44.30384401\n",
            "Iteration 363, loss = 44.62103687\n",
            "Iteration 364, loss = 44.52876667\n",
            "Iteration 365, loss = 44.25365491\n",
            "Iteration 366, loss = 44.17101628\n",
            "Iteration 367, loss = 44.33676777\n",
            "Iteration 368, loss = 44.30843071\n",
            "Iteration 369, loss = 44.26382569\n",
            "Iteration 370, loss = 44.08535271\n",
            "Iteration 371, loss = 44.09521574\n",
            "Iteration 372, loss = 44.20477366\n",
            "Iteration 373, loss = 44.11332656\n",
            "Iteration 374, loss = 44.10889981\n",
            "Iteration 375, loss = 44.02729504\n",
            "Iteration 376, loss = 44.08856924\n",
            "Iteration 377, loss = 44.05931183\n",
            "Iteration 378, loss = 43.96849449\n",
            "Iteration 379, loss = 43.89459005\n",
            "Iteration 380, loss = 43.92495714\n",
            "Iteration 381, loss = 43.96142563\n",
            "Iteration 382, loss = 43.98152273\n",
            "Iteration 383, loss = 43.88094844\n",
            "Iteration 384, loss = 43.82368820\n",
            "Iteration 385, loss = 43.81903785\n",
            "Iteration 386, loss = 43.78155126\n",
            "Iteration 387, loss = 43.77770830\n",
            "Iteration 388, loss = 43.77927547\n",
            "Iteration 389, loss = 43.74511738\n",
            "Iteration 390, loss = 43.71919719\n",
            "Iteration 391, loss = 43.69892677\n",
            "Iteration 392, loss = 43.69022348\n",
            "Iteration 393, loss = 43.67920305\n",
            "Iteration 394, loss = 43.71808179\n",
            "Iteration 395, loss = 43.65686537\n",
            "Iteration 396, loss = 43.67710961\n",
            "Iteration 397, loss = 43.71183040\n",
            "Iteration 398, loss = 43.63873820\n",
            "Iteration 399, loss = 43.55574223\n",
            "Iteration 400, loss = 43.61290063\n",
            "Iteration 401, loss = 43.78011334\n",
            "Iteration 402, loss = 43.53254558\n",
            "Iteration 403, loss = 43.52472299\n",
            "Iteration 404, loss = 43.54385266\n",
            "Iteration 405, loss = 43.48971229\n",
            "Iteration 406, loss = 43.42430050\n",
            "Iteration 407, loss = 43.65826440\n",
            "Iteration 408, loss = 43.51327893\n",
            "Iteration 409, loss = 43.46772103\n",
            "Iteration 410, loss = 43.35656407\n",
            "Iteration 411, loss = 43.60254209\n",
            "Iteration 412, loss = 43.38767266\n",
            "Iteration 413, loss = 43.35512941\n",
            "Iteration 414, loss = 43.38240162\n",
            "Iteration 415, loss = 43.36992716\n",
            "Iteration 416, loss = 43.27875101\n",
            "Iteration 417, loss = 43.25904500\n",
            "Iteration 418, loss = 43.32393834\n",
            "Iteration 419, loss = 43.34654503\n",
            "Iteration 420, loss = 43.32330133\n",
            "Iteration 421, loss = 43.24079019\n",
            "Iteration 422, loss = 43.21632494\n",
            "Iteration 423, loss = 43.19970844\n",
            "Iteration 424, loss = 43.17268578\n",
            "Iteration 425, loss = 43.32128672\n",
            "Iteration 426, loss = 43.22004791\n",
            "Iteration 427, loss = 43.14068644\n",
            "Iteration 428, loss = 43.09979826\n",
            "Iteration 429, loss = 43.08120337\n",
            "Iteration 430, loss = 43.24409031\n",
            "Iteration 431, loss = 43.17363501\n",
            "Iteration 432, loss = 43.18483674\n",
            "Iteration 433, loss = 43.12116257\n",
            "Iteration 434, loss = 43.08519608\n",
            "Iteration 435, loss = 43.07772857\n",
            "Iteration 436, loss = 43.00377479\n",
            "Iteration 437, loss = 43.03078694\n",
            "Iteration 438, loss = 43.16744361\n",
            "Iteration 439, loss = 43.12478805\n",
            "Iteration 440, loss = 42.91207326\n",
            "Iteration 441, loss = 42.98817624\n",
            "Iteration 442, loss = 43.43419261\n",
            "Iteration 443, loss = 43.28637852\n",
            "Iteration 444, loss = 43.10947302\n",
            "Iteration 445, loss = 42.87839000\n",
            "Iteration 446, loss = 42.95619495\n",
            "Iteration 447, loss = 43.01799044\n",
            "Iteration 448, loss = 42.98100326\n",
            "Iteration 449, loss = 42.86289636\n",
            "Iteration 450, loss = 42.95685019\n",
            "Iteration 451, loss = 42.91114070\n",
            "Iteration 452, loss = 42.83279057\n",
            "Iteration 453, loss = 42.78881461\n",
            "Iteration 454, loss = 42.82558802\n",
            "Iteration 455, loss = 43.01083208\n",
            "Iteration 456, loss = 42.86455582\n",
            "Iteration 457, loss = 42.75885471\n",
            "Iteration 458, loss = 42.73558052\n",
            "Iteration 459, loss = 42.97044133\n",
            "Iteration 460, loss = 43.07509530\n",
            "Iteration 461, loss = 42.97538134\n",
            "Iteration 462, loss = 43.00768107\n",
            "Iteration 463, loss = 42.68869057\n",
            "Iteration 464, loss = 42.68643394\n",
            "Iteration 465, loss = 42.75210456\n",
            "Iteration 466, loss = 42.86904389\n",
            "Iteration 467, loss = 42.67512371\n",
            "Iteration 468, loss = 42.61532168\n",
            "Iteration 469, loss = 42.64916510\n",
            "Iteration 470, loss = 42.78237057\n",
            "Iteration 471, loss = 42.61296800\n",
            "Iteration 472, loss = 42.51124144\n",
            "Iteration 473, loss = 42.70425483\n",
            "Iteration 474, loss = 42.80040509\n",
            "Iteration 475, loss = 42.70996916\n",
            "Iteration 476, loss = 42.59184982\n",
            "Iteration 477, loss = 42.57025571\n",
            "Iteration 478, loss = 42.57966337\n",
            "Iteration 479, loss = 42.67415837\n",
            "Iteration 480, loss = 42.49528465\n",
            "Iteration 481, loss = 42.54849125\n",
            "Iteration 482, loss = 42.72553059\n",
            "Iteration 483, loss = 42.50136966\n",
            "Iteration 484, loss = 42.53440496\n",
            "Iteration 485, loss = 42.48031780\n",
            "Iteration 486, loss = 42.47428560\n",
            "Iteration 487, loss = 42.49519374\n",
            "Iteration 488, loss = 42.46265508\n",
            "Iteration 489, loss = 42.43549556\n",
            "Iteration 490, loss = 42.45225978\n",
            "Iteration 491, loss = 42.41868289\n",
            "Iteration 492, loss = 42.52147608\n",
            "Iteration 493, loss = 42.41254759\n",
            "Iteration 494, loss = 42.49096870\n",
            "Iteration 495, loss = 42.44478634\n",
            "Iteration 496, loss = 42.31560058\n",
            "Iteration 497, loss = 42.53329020\n",
            "Iteration 498, loss = 42.64653321\n",
            "Iteration 499, loss = 42.33776835\n",
            "Iteration 500, loss = 42.60117434\n",
            "Iteration 501, loss = 42.44870936\n",
            "Iteration 502, loss = 42.38003907\n",
            "Iteration 503, loss = 42.24402511\n",
            "Iteration 504, loss = 42.24940949\n",
            "Iteration 505, loss = 42.53643654\n",
            "Iteration 506, loss = 42.42746001\n",
            "Iteration 507, loss = 42.16560850\n",
            "Iteration 508, loss = 42.48284614\n",
            "Iteration 509, loss = 42.55342553\n",
            "Iteration 510, loss = 42.46203707\n",
            "Iteration 511, loss = 42.21559740\n",
            "Iteration 512, loss = 42.23258124\n",
            "Iteration 513, loss = 42.26393109\n",
            "Iteration 514, loss = 42.26888684\n",
            "Iteration 515, loss = 42.20607204\n",
            "Iteration 516, loss = 42.15367785\n",
            "Iteration 517, loss = 42.19308397\n",
            "Iteration 518, loss = 42.58880278\n",
            "Iteration 519, loss = 42.30641062\n",
            "Iteration 520, loss = 42.13522828\n",
            "Iteration 521, loss = 42.20075999\n",
            "Iteration 522, loss = 42.23752997\n",
            "Iteration 523, loss = 42.38862579\n",
            "Iteration 524, loss = 42.11347532\n",
            "Iteration 525, loss = 42.12035892\n",
            "Iteration 526, loss = 42.08610490\n",
            "Iteration 527, loss = 42.17480391\n",
            "Iteration 528, loss = 42.47842939\n",
            "Iteration 529, loss = 42.15974375\n",
            "Iteration 530, loss = 42.27470572\n",
            "Iteration 531, loss = 42.19030206\n",
            "Iteration 532, loss = 42.05591927\n",
            "Iteration 533, loss = 42.19120437\n",
            "Iteration 534, loss = 42.11443050\n",
            "Iteration 535, loss = 42.03987852\n",
            "Iteration 536, loss = 42.04410267\n",
            "Iteration 537, loss = 42.01174380\n",
            "Iteration 538, loss = 41.97409743\n",
            "Iteration 539, loss = 42.06619287\n",
            "Iteration 540, loss = 42.03397776\n",
            "Iteration 541, loss = 41.94441081\n",
            "Iteration 542, loss = 42.00451818\n",
            "Iteration 543, loss = 41.96507064\n",
            "Iteration 544, loss = 41.95417691\n",
            "Iteration 545, loss = 41.91178857\n",
            "Iteration 546, loss = 41.88100448\n",
            "Iteration 547, loss = 41.89030095\n",
            "Iteration 548, loss = 42.39409806\n",
            "Iteration 549, loss = 42.37456531\n",
            "Iteration 550, loss = 41.95746511\n",
            "Iteration 551, loss = 41.87850626\n",
            "Iteration 552, loss = 41.88258623\n",
            "Iteration 553, loss = 41.84939112\n",
            "Iteration 554, loss = 41.86514760\n",
            "Iteration 555, loss = 41.85552170\n",
            "Iteration 556, loss = 42.05060953\n",
            "Iteration 557, loss = 41.80643878\n",
            "Iteration 558, loss = 41.91066358\n",
            "Iteration 559, loss = 41.88841036\n",
            "Iteration 560, loss = 41.88960968\n",
            "Iteration 561, loss = 41.77054701\n",
            "Iteration 562, loss = 41.85137720\n",
            "Iteration 563, loss = 41.80842998\n",
            "Iteration 564, loss = 41.81603260\n",
            "Iteration 565, loss = 42.06867214\n",
            "Iteration 566, loss = 41.70932771\n",
            "Iteration 567, loss = 41.75742213\n",
            "Iteration 568, loss = 42.05031711\n",
            "Iteration 569, loss = 42.05274322\n",
            "Iteration 570, loss = 42.15325401\n",
            "Iteration 571, loss = 41.68359808\n",
            "Iteration 572, loss = 41.76661877\n",
            "Iteration 573, loss = 41.77314514\n",
            "Iteration 574, loss = 41.82837965\n",
            "Iteration 575, loss = 41.70741932\n",
            "Iteration 576, loss = 41.68538480\n",
            "Iteration 577, loss = 41.75531583\n",
            "Iteration 578, loss = 41.71459420\n",
            "Iteration 579, loss = 41.62252851\n",
            "Iteration 580, loss = 41.66818165\n",
            "Iteration 581, loss = 41.66393175\n",
            "Iteration 582, loss = 41.60922158\n",
            "Iteration 583, loss = 41.62777322\n",
            "Iteration 584, loss = 41.65231536\n",
            "Iteration 585, loss = 41.66801375\n",
            "Iteration 586, loss = 41.60066409\n",
            "Iteration 587, loss = 41.69346574\n",
            "Iteration 588, loss = 41.50420423\n",
            "Iteration 589, loss = 41.55082162\n",
            "Iteration 590, loss = 41.76380836\n",
            "Iteration 591, loss = 41.82041814\n",
            "Iteration 592, loss = 41.56994253\n",
            "Iteration 593, loss = 41.55098326\n",
            "Iteration 594, loss = 41.68219793\n",
            "Iteration 595, loss = 41.86623877\n",
            "Iteration 596, loss = 41.62702973\n",
            "Iteration 597, loss = 41.70491330\n",
            "Iteration 598, loss = 41.68658938\n",
            "Iteration 599, loss = 41.60112911\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAl76AneVagX",
        "colab_type": "code",
        "outputId": "0c94a96c-1de4-4257-f3ce-6dd8463b9ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(tgt_first_dev)\n",
        "print(tgt_first_test)\n",
        "print(tgt_second_dev)\n",
        "print(tgt_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[178.44935124396977, 144.94778932485468, 119.44926488824116]\n",
            "[132.91106596637482, 126.74614554665905, 106.81075073833799]\n",
            "[160.05877365623783, 145.8450266135486, 129.37027559142803]\n",
            "[121.07140954075066, 130.1072141749327, 102.25809954294813]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59XyX3KaqqO",
        "colab_type": "code",
        "outputId": "bbb06ac6-d89d-4a5b-9c81-a18e844bb642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "tgt_test_mse = pd.DataFrame(list(zip(tgt_first_test, tgt_second_test)), columns=['TGTONLY_REG','TGTONLY_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "tgt_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row1_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >TGTONLY_REG</th>        <th class=\"col_heading level0 col1\" >TGTONLY_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row0_col0\" class=\"data row0 col0\" >132.911066</td>\n",
              "                        <td id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row0_col1\" class=\"data row0 col1\" >121.071410</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row1_col0\" class=\"data row1 col0\" >126.746146</td>\n",
              "                        <td id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row1_col1\" class=\"data row1 col1\" >130.107214</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row2_col0\" class=\"data row2 col0\" >106.810751</td>\n",
              "                        <td id=\"T_8aa8ee5c_9eec_11ea_9f06_0242ac1c0002row2_col1\" class=\"data row2 col1\" >102.258100</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28f26b3a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN2Z5VftPfN0",
        "colab_type": "text"
      },
      "source": [
        "### ALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ2Bi9PhVef_",
        "colab_type": "code",
        "outputId": "eb36fb94-8faa-4e7c-dcc0-341be635df85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 64, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 128, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "ma_train = pd.concat([female_train, mixed_train, male_tgt], ignore_index=True)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([female_train_pred, mixed_train_pred, male_tgt_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([male_train, mixed_train, female_tgt], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([male_train_pred, mixed_train_pred, female_tgt_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([male_train, female_train, mixed_tgt], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([male_train_pred, female_train_pred, mixed_tgt_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred]}\n",
        "\n",
        "all_first_dev = []\n",
        "all_second_dev = []\n",
        "all_first_test = []\n",
        "all_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "  linr = LinearRegression() #normalize=True\n",
        "  linr.fit(X_train, Y_train)\n",
        "  all_first_dev.append(mean_squared_error(Y_dev, linr.predict(X_dev)))\n",
        "  all_first_test.append(mean_squared_error(Y_test, linr.predict(X_test)))\n",
        "  \n",
        "  mlp = MLPRegressor(activation='relu', alpha=0.0001, batch_size=64, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  mlp.fit(X_train, Y_train)\n",
        "  #mlp = MLPRegressor(random_state=24)\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30), (25,25,25,25)], 'activation':['relu', 'identity'], 'alpha':[0.0001, 0.01, 0.1, 1, 10], 'batch_size':[200, 128, 64, 32], 'max_iter':[1000], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30)], 'alpha':[0.0001, 0.01, 1], 'activation':['relu', 'identity'], 'max_iter':[1000], 'batch_size':[200, 128, 64], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200], 'learning_rate_init':[0.001, 0.01]}\n",
        "  #mse = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False\n",
        "  #clf = GridSearchCV(mlp, parameters, scoring = 'neg_mean_squared_error', n_jobs = -1)#, verbose=True, scoring = mse)\n",
        "  #clf.fit(X_train, Y_train)\n",
        "  #print(clf.best_params_)\n",
        "  all_second_dev.append(mean_squared_error(Y_dev, mlp.predict(X_dev)))\n",
        "  all_second_test.append(mean_squared_error(Y_test, mlp.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 90.57087376\n",
            "Iteration 2, loss = 70.66524092\n",
            "Iteration 3, loss = 65.66969275\n",
            "Iteration 4, loss = 61.48277766\n",
            "Iteration 5, loss = 58.26308728\n",
            "Iteration 6, loss = 56.78273483\n",
            "Iteration 7, loss = 55.61869881\n",
            "Iteration 8, loss = 55.09261635\n",
            "Iteration 9, loss = 55.13816783\n",
            "Iteration 10, loss = 54.66204726\n",
            "Iteration 11, loss = 54.46584194\n",
            "Iteration 12, loss = 54.12426807\n",
            "Iteration 13, loss = 53.96487082\n",
            "Iteration 14, loss = 53.59281066\n",
            "Iteration 15, loss = 53.55545876\n",
            "Iteration 16, loss = 53.27520410\n",
            "Iteration 17, loss = 53.33190693\n",
            "Iteration 18, loss = 53.16687480\n",
            "Iteration 19, loss = 53.45027587\n",
            "Iteration 20, loss = 53.76713174\n",
            "Iteration 21, loss = 52.75089942\n",
            "Iteration 22, loss = 52.67303329\n",
            "Iteration 23, loss = 52.71645600\n",
            "Iteration 24, loss = 52.78494090\n",
            "Iteration 25, loss = 54.01756460\n",
            "Iteration 26, loss = 52.53711401\n",
            "Iteration 27, loss = 52.40098890\n",
            "Iteration 28, loss = 52.60755068\n",
            "Iteration 29, loss = 52.88218699\n",
            "Iteration 30, loss = 52.49398524\n",
            "Iteration 31, loss = 52.46566885\n",
            "Iteration 32, loss = 52.66156160\n",
            "Iteration 33, loss = 52.70362273\n",
            "Iteration 34, loss = 52.80592161\n",
            "Iteration 35, loss = 52.44487379\n",
            "Iteration 36, loss = 52.51284358\n",
            "Iteration 37, loss = 52.39785466\n",
            "Iteration 38, loss = 52.40361155\n",
            "Iteration 39, loss = 52.43010923\n",
            "Iteration 40, loss = 52.38303891\n",
            "Iteration 41, loss = 52.56809664\n",
            "Iteration 42, loss = 52.58389584\n",
            "Iteration 43, loss = 52.35068074\n",
            "Iteration 44, loss = 52.37886699\n",
            "Iteration 45, loss = 52.45513980\n",
            "Iteration 46, loss = 52.15601618\n",
            "Iteration 47, loss = 52.16324071\n",
            "Iteration 48, loss = 52.48471301\n",
            "Iteration 49, loss = 52.26710372\n",
            "Iteration 50, loss = 52.21636827\n",
            "Iteration 51, loss = 52.25482344\n",
            "Iteration 52, loss = 52.06241480\n",
            "Iteration 53, loss = 52.35830815\n",
            "Iteration 54, loss = 52.06822410\n",
            "Iteration 55, loss = 52.32860561\n",
            "Iteration 56, loss = 52.17899428\n",
            "Iteration 57, loss = 51.94112947\n",
            "Iteration 58, loss = 52.50374385\n",
            "Iteration 59, loss = 51.99611023\n",
            "Iteration 60, loss = 52.04199307\n",
            "Iteration 61, loss = 52.03243824\n",
            "Iteration 62, loss = 52.05397398\n",
            "Iteration 63, loss = 52.40478384\n",
            "Iteration 64, loss = 51.80521178\n",
            "Iteration 65, loss = 51.99949966\n",
            "Iteration 66, loss = 51.90217345\n",
            "Iteration 67, loss = 52.45595490\n",
            "Iteration 68, loss = 51.77559635\n",
            "Iteration 69, loss = 51.91954280\n",
            "Iteration 70, loss = 51.79059678\n",
            "Iteration 71, loss = 51.92088841\n",
            "Iteration 72, loss = 52.02646094\n",
            "Iteration 73, loss = 52.42979579\n",
            "Iteration 74, loss = 51.79975033\n",
            "Iteration 75, loss = 51.82275205\n",
            "Iteration 76, loss = 52.31658967\n",
            "Iteration 77, loss = 51.78966583\n",
            "Iteration 78, loss = 51.79884168\n",
            "Iteration 79, loss = 51.85059631\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 87.47438110\n",
            "Iteration 2, loss = 70.18824633\n",
            "Iteration 3, loss = 65.62855365\n",
            "Iteration 4, loss = 61.78484295\n",
            "Iteration 5, loss = 58.97772720\n",
            "Iteration 6, loss = 57.20719411\n",
            "Iteration 7, loss = 56.39912200\n",
            "Iteration 8, loss = 55.79353468\n",
            "Iteration 9, loss = 55.73043082\n",
            "Iteration 10, loss = 55.38559124\n",
            "Iteration 11, loss = 54.98227974\n",
            "Iteration 12, loss = 54.71172944\n",
            "Iteration 13, loss = 54.67378140\n",
            "Iteration 14, loss = 54.64901143\n",
            "Iteration 15, loss = 54.40422683\n",
            "Iteration 16, loss = 54.20856508\n",
            "Iteration 17, loss = 53.99005154\n",
            "Iteration 18, loss = 53.94978299\n",
            "Iteration 19, loss = 53.73182016\n",
            "Iteration 20, loss = 53.70744874\n",
            "Iteration 21, loss = 53.60346545\n",
            "Iteration 22, loss = 53.84463216\n",
            "Iteration 23, loss = 53.59975987\n",
            "Iteration 24, loss = 53.52699936\n",
            "Iteration 25, loss = 53.38122827\n",
            "Iteration 26, loss = 53.71874086\n",
            "Iteration 27, loss = 53.38450764\n",
            "Iteration 28, loss = 53.39437809\n",
            "Iteration 29, loss = 53.44486989\n",
            "Iteration 30, loss = 53.26691106\n",
            "Iteration 31, loss = 53.30796803\n",
            "Iteration 32, loss = 53.39177030\n",
            "Iteration 33, loss = 53.16814494\n",
            "Iteration 34, loss = 53.35248826\n",
            "Iteration 35, loss = 53.28454898\n",
            "Iteration 36, loss = 53.12877876\n",
            "Iteration 37, loss = 53.24984389\n",
            "Iteration 38, loss = 53.34555755\n",
            "Iteration 39, loss = 53.48217900\n",
            "Iteration 40, loss = 53.18158625\n",
            "Iteration 41, loss = 53.13502132\n",
            "Iteration 42, loss = 53.20093945\n",
            "Iteration 43, loss = 53.51398649\n",
            "Iteration 44, loss = 52.98974584\n",
            "Iteration 45, loss = 53.19612316\n",
            "Iteration 46, loss = 52.90014334\n",
            "Iteration 47, loss = 53.17288236\n",
            "Iteration 48, loss = 53.05310056\n",
            "Iteration 49, loss = 53.02600967\n",
            "Iteration 50, loss = 53.08776675\n",
            "Iteration 51, loss = 52.97818866\n",
            "Iteration 52, loss = 52.94869053\n",
            "Iteration 53, loss = 52.94957478\n",
            "Iteration 54, loss = 52.93602347\n",
            "Iteration 55, loss = 52.94016936\n",
            "Iteration 56, loss = 52.84767021\n",
            "Iteration 57, loss = 53.08142274\n",
            "Iteration 58, loss = 52.83648031\n",
            "Iteration 59, loss = 52.90579144\n",
            "Iteration 60, loss = 52.94925083\n",
            "Iteration 61, loss = 52.76460563\n",
            "Iteration 62, loss = 52.82098207\n",
            "Iteration 63, loss = 52.85615443\n",
            "Iteration 64, loss = 52.84898001\n",
            "Iteration 65, loss = 52.81346263\n",
            "Iteration 66, loss = 52.70082295\n",
            "Iteration 67, loss = 52.83063725\n",
            "Iteration 68, loss = 52.89903569\n",
            "Iteration 69, loss = 52.65626341\n",
            "Iteration 70, loss = 52.75143464\n",
            "Iteration 71, loss = 52.78910935\n",
            "Iteration 72, loss = 52.59752087\n",
            "Iteration 73, loss = 52.91912020\n",
            "Iteration 74, loss = 52.78066576\n",
            "Iteration 75, loss = 52.73324959\n",
            "Iteration 76, loss = 52.86736898\n",
            "Iteration 77, loss = 52.74788940\n",
            "Iteration 78, loss = 52.76767457\n",
            "Iteration 79, loss = 52.72017083\n",
            "Iteration 80, loss = 52.98388392\n",
            "Iteration 81, loss = 52.83761332\n",
            "Iteration 82, loss = 52.60531399\n",
            "Iteration 83, loss = 52.68558004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 105.80127110\n",
            "Iteration 2, loss = 78.93561082\n",
            "Iteration 3, loss = 75.01192413\n",
            "Iteration 4, loss = 71.28592690\n",
            "Iteration 5, loss = 68.03746392\n",
            "Iteration 6, loss = 65.17548297\n",
            "Iteration 7, loss = 62.86148818\n",
            "Iteration 8, loss = 61.44077798\n",
            "Iteration 9, loss = 60.41797336\n",
            "Iteration 10, loss = 59.67132212\n",
            "Iteration 11, loss = 59.42560832\n",
            "Iteration 12, loss = 58.85789904\n",
            "Iteration 13, loss = 58.63672726\n",
            "Iteration 14, loss = 58.36861046\n",
            "Iteration 15, loss = 58.04292739\n",
            "Iteration 16, loss = 58.12706948\n",
            "Iteration 17, loss = 58.32028719\n",
            "Iteration 18, loss = 57.54699443\n",
            "Iteration 19, loss = 57.58445422\n",
            "Iteration 20, loss = 57.19888122\n",
            "Iteration 21, loss = 57.05328948\n",
            "Iteration 22, loss = 57.04618059\n",
            "Iteration 23, loss = 56.73654273\n",
            "Iteration 24, loss = 56.65529239\n",
            "Iteration 25, loss = 56.58454192\n",
            "Iteration 26, loss = 56.57618904\n",
            "Iteration 27, loss = 56.65943170\n",
            "Iteration 28, loss = 56.39281659\n",
            "Iteration 29, loss = 56.17143424\n",
            "Iteration 30, loss = 56.11081144\n",
            "Iteration 31, loss = 56.00347406\n",
            "Iteration 32, loss = 55.93995473\n",
            "Iteration 33, loss = 55.89528110\n",
            "Iteration 34, loss = 56.15286128\n",
            "Iteration 35, loss = 55.73613854\n",
            "Iteration 36, loss = 55.88329431\n",
            "Iteration 37, loss = 56.10649730\n",
            "Iteration 38, loss = 55.69770250\n",
            "Iteration 39, loss = 55.80940824\n",
            "Iteration 40, loss = 55.82473329\n",
            "Iteration 41, loss = 55.88812020\n",
            "Iteration 42, loss = 55.71478247\n",
            "Iteration 43, loss = 55.57504653\n",
            "Iteration 44, loss = 55.63943586\n",
            "Iteration 45, loss = 55.42222051\n",
            "Iteration 46, loss = 55.57054205\n",
            "Iteration 47, loss = 55.47751239\n",
            "Iteration 48, loss = 55.47512274\n",
            "Iteration 49, loss = 55.20806630\n",
            "Iteration 50, loss = 55.29606295\n",
            "Iteration 51, loss = 55.47833819\n",
            "Iteration 52, loss = 55.22034096\n",
            "Iteration 53, loss = 55.22427826\n",
            "Iteration 54, loss = 55.35909184\n",
            "Iteration 55, loss = 55.47697478\n",
            "Iteration 56, loss = 55.39717196\n",
            "Iteration 57, loss = 54.96256923\n",
            "Iteration 58, loss = 54.85139087\n",
            "Iteration 59, loss = 55.15524897\n",
            "Iteration 60, loss = 54.86451957\n",
            "Iteration 61, loss = 55.30916831\n",
            "Iteration 62, loss = 55.18526231\n",
            "Iteration 63, loss = 55.45549183\n",
            "Iteration 64, loss = 55.04353765\n",
            "Iteration 65, loss = 54.94062804\n",
            "Iteration 66, loss = 55.08564267\n",
            "Iteration 67, loss = 54.92967783\n",
            "Iteration 68, loss = 55.00993134\n",
            "Iteration 69, loss = 54.97025048\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74EKsgrVVfAP",
        "colab_type": "code",
        "outputId": "af024970-5227-4def-f9b0-27e136bcd40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(all_first_dev)\n",
        "print(all_first_test)\n",
        "print(all_second_dev)\n",
        "print(all_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[113.4607742265285, 132.54585991326113, 116.45332237053626]\n",
            "[97.50095830199635, 128.5449591098929, 104.76716171617446]\n",
            "[111.97982092426429, 137.9519159736161, 121.9613100137307]\n",
            "[92.23586416486971, 125.68273536311342, 101.46935231677621]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtWhkuAwb1ez",
        "colab_type": "code",
        "outputId": "9ec628a1-4ee7-4188-e660-f9cf22e00ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "all_test_mse = pd.DataFrame(list(zip(all_first_test, all_second_test)), columns=['ALL_REG','ALL_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "all_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_94bb728e_9eec_11ea_9f06_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_94bb728e_9eec_11ea_9f06_0242ac1c0002row1_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_94bb728e_9eec_11ea_9f06_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_REG</th>        <th class=\"col_heading level0 col1\" >ALL_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002row0_col0\" class=\"data row0 col0\" >97.500958</td>\n",
              "                        <td id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002row0_col1\" class=\"data row0 col1\" >92.235864</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002row1_col0\" class=\"data row1 col0\" >128.544959</td>\n",
              "                        <td id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002row1_col1\" class=\"data row1 col1\" >125.682735</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002row2_col0\" class=\"data row2 col0\" >104.767162</td>\n",
              "                        <td id=\"T_94bb728e_9eec_11ea_9f06_0242ac1c0002row2_col1\" class=\"data row2 col1\" >101.469352</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28f26b33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-i_PcAwPg6w",
        "colab_type": "text"
      },
      "source": [
        "### WEIGHTED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKk82PpBVfe-",
        "colab_type": "code",
        "outputId": "978eb584-5890-46fe-f724-dff0989087ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "ma_inter = pd.concat([female_train, mixed_train], ignore_index=True)\n",
        "n = len(ma_inter)//len(male_tgt)\n",
        "tgt_inter = pd.concat([male_tgt]*n, ignore_index=True)\n",
        "ma_train = pd.concat([ma_inter, tgt_inter], ignore_index=True)\n",
        "#print(ma_inter.shape, n, tgt_inter.shape, ma_train.shape)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_inter_pred = pd.concat([female_train_pred, mixed_train_pred], ignore_index=True)\n",
        "n = len(ma_inter_pred)//len(male_tgt_pred)\n",
        "tgt_inter_pred = pd.concat([male_tgt_pred]*n, ignore_index=True)\n",
        "ma_train_pred = pd.concat([ma_inter_pred, tgt_inter_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_inter = pd.concat([male_train, mixed_train], ignore_index=True)\n",
        "n = len(fe_inter)//len(female_tgt)\n",
        "tgt_inter = pd.concat([female_tgt]*n, ignore_index=True)\n",
        "fe_train = pd.concat([fe_inter, tgt_inter], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_inter_pred = pd.concat([male_train_pred, mixed_train_pred], ignore_index=True)\n",
        "n = len(fe_inter_pred)//len(female_tgt_pred)\n",
        "tgt_inter_pred = pd.concat([female_tgt_pred]*n, ignore_index=True)\n",
        "fe_train_pred = pd.concat([fe_inter_pred, tgt_inter_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_inter = pd.concat([male_train, female_train], ignore_index=True)\n",
        "n = len(mi_inter)//len(mixed_tgt)\n",
        "tgt_inter = pd.concat([mixed_tgt]*n, ignore_index=True)\n",
        "mi_train = pd.concat([mi_inter, tgt_inter], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_inter_pred = pd.concat([male_train_pred, female_train_pred], ignore_index=True)\n",
        "n = len(mi_inter_pred)//len(mixed_tgt_pred)\n",
        "tgt_inter_pred = pd.concat([mixed_tgt_pred]*n, ignore_index=True)\n",
        "mi_train_pred = pd.concat([mi_inter_pred, tgt_inter_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred]}\n",
        "\n",
        "wei_first_dev = []\n",
        "wei_second_dev = []\n",
        "wei_first_test = []\n",
        "wei_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "  linr = LinearRegression() #normalize=True\n",
        "  linr.fit(X_train, Y_train)\n",
        "  wei_first_dev.append(mean_squared_error(Y_dev, linr.predict(X_dev)))\n",
        "  wei_first_test.append(mean_squared_error(Y_test, linr.predict(X_test)))\n",
        "  \n",
        "  mlp = MLPRegressor(activation='relu', alpha=0.01, batch_size=64, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  mlp.fit(X_train, Y_train)\n",
        "  #print(\"Before GridSearch\")\n",
        "  #mlp = MLPRegressor(random_state=24)\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30), (25,25,25,25)], 'activation':['relu', 'identity'], 'alpha':[0.0001, 0.01, 0.1, 1, 10], 'batch_size':[200, 128, 64, 32], 'max_iter':[1000], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001, 0.01, 1], 'activation':['relu', 'identity'], 'max_iter':[1000], 'batch_size':[200, 128, 64], 'learning_rate_init':[0.001]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200], 'learning_rate_init':[0.001]}\n",
        "  #mse = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False\n",
        "  #clf = GridSearchCV(mlp, parameters, scoring = 'neg_mean_squared_error', n_jobs = -1)#, verbose=True, scoring = mse)\n",
        "  #clf.fit(X_train, Y_train)\n",
        "  #print(\"After GridSearch\")\n",
        "  #print(clf.best_params_)\n",
        "  wei_second_dev.append(mean_squared_error(Y_dev, mlp.predict(X_dev)))\n",
        "  wei_second_test.append(mean_squared_error(Y_test, mlp.predict(X_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 84.91151425\n",
            "Iteration 2, loss = 67.98788826\n",
            "Iteration 3, loss = 61.52862150\n",
            "Iteration 4, loss = 59.11651456\n",
            "Iteration 5, loss = 57.93287856\n",
            "Iteration 6, loss = 56.99084990\n",
            "Iteration 7, loss = 56.36272827\n",
            "Iteration 8, loss = 55.56938252\n",
            "Iteration 9, loss = 54.97020709\n",
            "Iteration 10, loss = 54.35500019\n",
            "Iteration 11, loss = 53.80221282\n",
            "Iteration 12, loss = 53.27842850\n",
            "Iteration 13, loss = 52.73492989\n",
            "Iteration 14, loss = 52.41009871\n",
            "Iteration 15, loss = 51.99277815\n",
            "Iteration 16, loss = 51.43602676\n",
            "Iteration 17, loss = 51.09914932\n",
            "Iteration 18, loss = 50.61947813\n",
            "Iteration 19, loss = 50.33278382\n",
            "Iteration 20, loss = 49.89320399\n",
            "Iteration 21, loss = 49.50259653\n",
            "Iteration 22, loss = 49.23195131\n",
            "Iteration 23, loss = 48.79916420\n",
            "Iteration 24, loss = 48.46803616\n",
            "Iteration 25, loss = 48.25448498\n",
            "Iteration 26, loss = 48.04121646\n",
            "Iteration 27, loss = 47.77378615\n",
            "Iteration 28, loss = 47.28182978\n",
            "Iteration 29, loss = 47.11430586\n",
            "Iteration 30, loss = 46.78483848\n",
            "Iteration 31, loss = 46.73111681\n",
            "Iteration 32, loss = 46.46415477\n",
            "Iteration 33, loss = 46.26619636\n",
            "Iteration 34, loss = 46.10116699\n",
            "Iteration 35, loss = 45.92583643\n",
            "Iteration 36, loss = 45.72427093\n",
            "Iteration 37, loss = 45.61858192\n",
            "Iteration 38, loss = 45.41267759\n",
            "Iteration 39, loss = 45.25012832\n",
            "Iteration 40, loss = 45.01174555\n",
            "Iteration 41, loss = 44.97012106\n",
            "Iteration 42, loss = 44.83856191\n",
            "Iteration 43, loss = 44.72554678\n",
            "Iteration 44, loss = 44.60551299\n",
            "Iteration 45, loss = 44.49566711\n",
            "Iteration 46, loss = 44.50305941\n",
            "Iteration 47, loss = 44.27603015\n",
            "Iteration 48, loss = 43.98442046\n",
            "Iteration 49, loss = 43.91088895\n",
            "Iteration 50, loss = 43.81191426\n",
            "Iteration 51, loss = 43.62081287\n",
            "Iteration 52, loss = 43.75746536\n",
            "Iteration 53, loss = 43.47571759\n",
            "Iteration 54, loss = 43.36060769\n",
            "Iteration 55, loss = 43.40043040\n",
            "Iteration 56, loss = 43.08704161\n",
            "Iteration 57, loss = 42.93078141\n",
            "Iteration 58, loss = 43.03438003\n",
            "Iteration 59, loss = 42.81688293\n",
            "Iteration 60, loss = 42.85836173\n",
            "Iteration 61, loss = 42.51327344\n",
            "Iteration 62, loss = 42.38854462\n",
            "Iteration 63, loss = 42.32271108\n",
            "Iteration 64, loss = 42.19961105\n",
            "Iteration 65, loss = 42.01826971\n",
            "Iteration 66, loss = 42.06590220\n",
            "Iteration 67, loss = 41.96711733\n",
            "Iteration 68, loss = 41.91709768\n",
            "Iteration 69, loss = 41.75683708\n",
            "Iteration 70, loss = 41.62664350\n",
            "Iteration 71, loss = 41.64134108\n",
            "Iteration 72, loss = 41.46980740\n",
            "Iteration 73, loss = 41.33741488\n",
            "Iteration 74, loss = 41.32541679\n",
            "Iteration 75, loss = 41.30239475\n",
            "Iteration 76, loss = 41.09446103\n",
            "Iteration 77, loss = 41.22368615\n",
            "Iteration 78, loss = 40.97451082\n",
            "Iteration 79, loss = 41.01206010\n",
            "Iteration 80, loss = 40.86687819\n",
            "Iteration 81, loss = 40.88033094\n",
            "Iteration 82, loss = 40.71249162\n",
            "Iteration 83, loss = 40.42840154\n",
            "Iteration 84, loss = 40.34833322\n",
            "Iteration 85, loss = 40.40961665\n",
            "Iteration 86, loss = 40.64183139\n",
            "Iteration 87, loss = 40.00830258\n",
            "Iteration 88, loss = 40.13994399\n",
            "Iteration 89, loss = 39.97117639\n",
            "Iteration 90, loss = 39.97206060\n",
            "Iteration 91, loss = 39.80948177\n",
            "Iteration 92, loss = 39.93962727\n",
            "Iteration 93, loss = 39.87877714\n",
            "Iteration 94, loss = 39.59255956\n",
            "Iteration 95, loss = 39.46859672\n",
            "Iteration 96, loss = 39.43884009\n",
            "Iteration 97, loss = 39.31107743\n",
            "Iteration 98, loss = 39.29266297\n",
            "Iteration 99, loss = 39.30785636\n",
            "Iteration 100, loss = 39.09599119\n",
            "Iteration 101, loss = 39.24063635\n",
            "Iteration 102, loss = 38.93458236\n",
            "Iteration 103, loss = 39.00531764\n",
            "Iteration 104, loss = 38.96198932\n",
            "Iteration 105, loss = 38.91370758\n",
            "Iteration 106, loss = 38.82419560\n",
            "Iteration 107, loss = 38.74502710\n",
            "Iteration 108, loss = 38.87810305\n",
            "Iteration 109, loss = 38.68033220\n",
            "Iteration 110, loss = 38.51353717\n",
            "Iteration 111, loss = 38.45140600\n",
            "Iteration 112, loss = 38.62672340\n",
            "Iteration 113, loss = 38.34355485\n",
            "Iteration 114, loss = 38.34212690\n",
            "Iteration 115, loss = 38.44984890\n",
            "Iteration 116, loss = 38.52862238\n",
            "Iteration 117, loss = 38.22234031\n",
            "Iteration 118, loss = 38.14160199\n",
            "Iteration 119, loss = 38.18895269\n",
            "Iteration 120, loss = 38.08236917\n",
            "Iteration 121, loss = 38.13027386\n",
            "Iteration 122, loss = 38.04041549\n",
            "Iteration 123, loss = 38.04718374\n",
            "Iteration 124, loss = 38.05462769\n",
            "Iteration 125, loss = 37.79802984\n",
            "Iteration 126, loss = 37.94695790\n",
            "Iteration 127, loss = 37.86216571\n",
            "Iteration 128, loss = 37.84916591\n",
            "Iteration 129, loss = 37.97985193\n",
            "Iteration 130, loss = 37.73093479\n",
            "Iteration 131, loss = 37.68550459\n",
            "Iteration 132, loss = 37.62418190\n",
            "Iteration 133, loss = 37.76679565\n",
            "Iteration 134, loss = 37.76125603\n",
            "Iteration 135, loss = 37.74955501\n",
            "Iteration 136, loss = 37.43354739\n",
            "Iteration 137, loss = 37.61643661\n",
            "Iteration 138, loss = 37.41432873\n",
            "Iteration 139, loss = 37.24163212\n",
            "Iteration 140, loss = 37.25884690\n",
            "Iteration 141, loss = 37.33288080\n",
            "Iteration 142, loss = 37.31857647\n",
            "Iteration 143, loss = 37.28458267\n",
            "Iteration 144, loss = 37.38909764\n",
            "Iteration 145, loss = 37.23275829\n",
            "Iteration 146, loss = 36.97855863\n",
            "Iteration 147, loss = 37.26318643\n",
            "Iteration 148, loss = 37.11739858\n",
            "Iteration 149, loss = 37.20623747\n",
            "Iteration 150, loss = 37.06371823\n",
            "Iteration 151, loss = 37.10064125\n",
            "Iteration 152, loss = 36.95344786\n",
            "Iteration 153, loss = 37.03747269\n",
            "Iteration 154, loss = 36.89066936\n",
            "Iteration 155, loss = 36.83360086\n",
            "Iteration 156, loss = 36.96503962\n",
            "Iteration 157, loss = 36.78324210\n",
            "Iteration 158, loss = 36.80171762\n",
            "Iteration 159, loss = 36.83999445\n",
            "Iteration 160, loss = 36.71176578\n",
            "Iteration 161, loss = 36.70823330\n",
            "Iteration 162, loss = 36.55457848\n",
            "Iteration 163, loss = 36.63392755\n",
            "Iteration 164, loss = 36.57932515\n",
            "Iteration 165, loss = 36.51131454\n",
            "Iteration 166, loss = 36.51141194\n",
            "Iteration 167, loss = 36.68602965\n",
            "Iteration 168, loss = 36.50287921\n",
            "Iteration 169, loss = 36.40722816\n",
            "Iteration 170, loss = 36.47405173\n",
            "Iteration 171, loss = 36.27824850\n",
            "Iteration 172, loss = 36.31804475\n",
            "Iteration 173, loss = 36.54932579\n",
            "Iteration 174, loss = 36.35650206\n",
            "Iteration 175, loss = 36.35420525\n",
            "Iteration 176, loss = 36.21425575\n",
            "Iteration 177, loss = 36.24790646\n",
            "Iteration 178, loss = 36.57892406\n",
            "Iteration 179, loss = 36.20105168\n",
            "Iteration 180, loss = 36.03469841\n",
            "Iteration 181, loss = 36.18256036\n",
            "Iteration 182, loss = 36.08368474\n",
            "Iteration 183, loss = 36.17682418\n",
            "Iteration 184, loss = 36.10404758\n",
            "Iteration 185, loss = 36.02572675\n",
            "Iteration 186, loss = 36.04742847\n",
            "Iteration 187, loss = 35.89932871\n",
            "Iteration 188, loss = 36.03376281\n",
            "Iteration 189, loss = 36.03394862\n",
            "Iteration 190, loss = 36.01068135\n",
            "Iteration 191, loss = 35.88189615\n",
            "Iteration 192, loss = 35.96779454\n",
            "Iteration 193, loss = 36.03970885\n",
            "Iteration 194, loss = 35.84171681\n",
            "Iteration 195, loss = 35.78966306\n",
            "Iteration 196, loss = 35.79319501\n",
            "Iteration 197, loss = 35.75837696\n",
            "Iteration 198, loss = 35.97127377\n",
            "Iteration 199, loss = 35.78554624\n",
            "Iteration 200, loss = 35.78308165\n",
            "Iteration 201, loss = 35.80872752\n",
            "Iteration 202, loss = 35.67894440\n",
            "Iteration 203, loss = 35.63788734\n",
            "Iteration 204, loss = 35.62831718\n",
            "Iteration 205, loss = 35.67121642\n",
            "Iteration 206, loss = 35.71809231\n",
            "Iteration 207, loss = 35.49969886\n",
            "Iteration 208, loss = 35.48375843\n",
            "Iteration 209, loss = 35.46449502\n",
            "Iteration 210, loss = 35.52073221\n",
            "Iteration 211, loss = 35.55677785\n",
            "Iteration 212, loss = 35.66442553\n",
            "Iteration 213, loss = 35.48443711\n",
            "Iteration 214, loss = 35.47133757\n",
            "Iteration 215, loss = 35.56854848\n",
            "Iteration 216, loss = 35.53346753\n",
            "Iteration 217, loss = 35.39227539\n",
            "Iteration 218, loss = 35.55529139\n",
            "Iteration 219, loss = 35.46710772\n",
            "Iteration 220, loss = 35.48742213\n",
            "Iteration 221, loss = 35.43264390\n",
            "Iteration 222, loss = 35.44584321\n",
            "Iteration 223, loss = 35.19945335\n",
            "Iteration 224, loss = 35.20556361\n",
            "Iteration 225, loss = 35.37937699\n",
            "Iteration 226, loss = 35.29540840\n",
            "Iteration 227, loss = 35.30332610\n",
            "Iteration 228, loss = 35.35615615\n",
            "Iteration 229, loss = 35.58926108\n",
            "Iteration 230, loss = 35.14848675\n",
            "Iteration 231, loss = 35.19697020\n",
            "Iteration 232, loss = 35.15331734\n",
            "Iteration 233, loss = 35.30409316\n",
            "Iteration 234, loss = 35.12393288\n",
            "Iteration 235, loss = 35.04473611\n",
            "Iteration 236, loss = 35.31956970\n",
            "Iteration 237, loss = 35.02280865\n",
            "Iteration 238, loss = 34.98989179\n",
            "Iteration 239, loss = 34.95619880\n",
            "Iteration 240, loss = 35.17951307\n",
            "Iteration 241, loss = 35.18218291\n",
            "Iteration 242, loss = 35.14222389\n",
            "Iteration 243, loss = 34.97094879\n",
            "Iteration 244, loss = 35.09849633\n",
            "Iteration 245, loss = 35.01394434\n",
            "Iteration 246, loss = 35.08111225\n",
            "Iteration 247, loss = 34.95893473\n",
            "Iteration 248, loss = 35.01297802\n",
            "Iteration 249, loss = 34.99843176\n",
            "Iteration 250, loss = 34.98690008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 74.51992193\n",
            "Iteration 2, loss = 57.28105836\n",
            "Iteration 3, loss = 51.92344694\n",
            "Iteration 4, loss = 49.43541185\n",
            "Iteration 5, loss = 48.06168435\n",
            "Iteration 6, loss = 47.15801311\n",
            "Iteration 7, loss = 46.55780526\n",
            "Iteration 8, loss = 46.03717432\n",
            "Iteration 9, loss = 45.55243824\n",
            "Iteration 10, loss = 45.17952960\n",
            "Iteration 11, loss = 44.97767692\n",
            "Iteration 12, loss = 44.67719787\n",
            "Iteration 13, loss = 44.26261972\n",
            "Iteration 14, loss = 44.22383928\n",
            "Iteration 15, loss = 43.97501162\n",
            "Iteration 16, loss = 43.74905621\n",
            "Iteration 17, loss = 43.57346283\n",
            "Iteration 18, loss = 43.38034155\n",
            "Iteration 19, loss = 43.03739012\n",
            "Iteration 20, loss = 42.94255207\n",
            "Iteration 21, loss = 42.72114958\n",
            "Iteration 22, loss = 42.63023581\n",
            "Iteration 23, loss = 42.54049853\n",
            "Iteration 24, loss = 42.23134018\n",
            "Iteration 25, loss = 42.24040753\n",
            "Iteration 26, loss = 42.15535217\n",
            "Iteration 27, loss = 41.99989184\n",
            "Iteration 28, loss = 41.97885720\n",
            "Iteration 29, loss = 41.78835286\n",
            "Iteration 30, loss = 41.55732306\n",
            "Iteration 31, loss = 41.53112059\n",
            "Iteration 32, loss = 41.50305068\n",
            "Iteration 33, loss = 41.26652897\n",
            "Iteration 34, loss = 41.47881140\n",
            "Iteration 35, loss = 41.37646236\n",
            "Iteration 36, loss = 41.25375744\n",
            "Iteration 37, loss = 41.17704117\n",
            "Iteration 38, loss = 41.01984492\n",
            "Iteration 39, loss = 40.93890093\n",
            "Iteration 40, loss = 40.93154696\n",
            "Iteration 41, loss = 40.86837902\n",
            "Iteration 42, loss = 40.85012846\n",
            "Iteration 43, loss = 40.71585258\n",
            "Iteration 44, loss = 40.69163928\n",
            "Iteration 45, loss = 40.62549005\n",
            "Iteration 46, loss = 40.50395882\n",
            "Iteration 47, loss = 40.47792038\n",
            "Iteration 48, loss = 40.43910521\n",
            "Iteration 49, loss = 40.32789235\n",
            "Iteration 50, loss = 40.19127593\n",
            "Iteration 51, loss = 40.12769694\n",
            "Iteration 52, loss = 40.12522330\n",
            "Iteration 53, loss = 39.84289948\n",
            "Iteration 54, loss = 39.97708131\n",
            "Iteration 55, loss = 39.76349051\n",
            "Iteration 56, loss = 39.67463995\n",
            "Iteration 57, loss = 39.53448382\n",
            "Iteration 58, loss = 39.41591169\n",
            "Iteration 59, loss = 39.48368925\n",
            "Iteration 60, loss = 39.20825734\n",
            "Iteration 61, loss = 39.09348123\n",
            "Iteration 62, loss = 39.04487622\n",
            "Iteration 63, loss = 38.92940424\n",
            "Iteration 64, loss = 38.87938709\n",
            "Iteration 65, loss = 38.83691714\n",
            "Iteration 66, loss = 38.72649846\n",
            "Iteration 67, loss = 38.68662449\n",
            "Iteration 68, loss = 38.55217577\n",
            "Iteration 69, loss = 38.39784251\n",
            "Iteration 70, loss = 38.44722045\n",
            "Iteration 71, loss = 38.40641988\n",
            "Iteration 72, loss = 38.36579673\n",
            "Iteration 73, loss = 38.12794478\n",
            "Iteration 74, loss = 38.26185749\n",
            "Iteration 75, loss = 38.12098106\n",
            "Iteration 76, loss = 38.21161287\n",
            "Iteration 77, loss = 37.98407734\n",
            "Iteration 78, loss = 37.90569814\n",
            "Iteration 79, loss = 37.81919337\n",
            "Iteration 80, loss = 37.76684733\n",
            "Iteration 81, loss = 37.74733937\n",
            "Iteration 82, loss = 37.64016884\n",
            "Iteration 83, loss = 37.58161293\n",
            "Iteration 84, loss = 37.42467163\n",
            "Iteration 85, loss = 37.40090909\n",
            "Iteration 86, loss = 37.32048812\n",
            "Iteration 87, loss = 37.21717698\n",
            "Iteration 88, loss = 37.15591866\n",
            "Iteration 89, loss = 37.16713822\n",
            "Iteration 90, loss = 37.17560105\n",
            "Iteration 91, loss = 36.94903680\n",
            "Iteration 92, loss = 36.89022574\n",
            "Iteration 93, loss = 36.80748006\n",
            "Iteration 94, loss = 36.73003376\n",
            "Iteration 95, loss = 36.77508726\n",
            "Iteration 96, loss = 36.55009918\n",
            "Iteration 97, loss = 36.55695984\n",
            "Iteration 98, loss = 36.49497646\n",
            "Iteration 99, loss = 36.27679255\n",
            "Iteration 100, loss = 36.41256912\n",
            "Iteration 101, loss = 36.11783625\n",
            "Iteration 102, loss = 36.14966361\n",
            "Iteration 103, loss = 36.03599872\n",
            "Iteration 104, loss = 36.03615792\n",
            "Iteration 105, loss = 35.96027372\n",
            "Iteration 106, loss = 35.71625985\n",
            "Iteration 107, loss = 35.85871416\n",
            "Iteration 108, loss = 35.71071885\n",
            "Iteration 109, loss = 35.71225105\n",
            "Iteration 110, loss = 35.53901820\n",
            "Iteration 111, loss = 35.53519500\n",
            "Iteration 112, loss = 35.51670284\n",
            "Iteration 113, loss = 35.43102897\n",
            "Iteration 114, loss = 35.38750108\n",
            "Iteration 115, loss = 35.41236575\n",
            "Iteration 116, loss = 35.23173950\n",
            "Iteration 117, loss = 35.33313922\n",
            "Iteration 118, loss = 35.20873243\n",
            "Iteration 119, loss = 35.11119339\n",
            "Iteration 120, loss = 35.07891704\n",
            "Iteration 121, loss = 35.05954716\n",
            "Iteration 122, loss = 35.14630428\n",
            "Iteration 123, loss = 35.02062655\n",
            "Iteration 124, loss = 35.04771905\n",
            "Iteration 125, loss = 34.93042013\n",
            "Iteration 126, loss = 34.99562872\n",
            "Iteration 127, loss = 34.85562021\n",
            "Iteration 128, loss = 35.10029269\n",
            "Iteration 129, loss = 34.82763147\n",
            "Iteration 130, loss = 34.69668843\n",
            "Iteration 131, loss = 34.65330778\n",
            "Iteration 132, loss = 34.67425084\n",
            "Iteration 133, loss = 34.74924499\n",
            "Iteration 134, loss = 34.66832963\n",
            "Iteration 135, loss = 34.65806355\n",
            "Iteration 136, loss = 34.64096118\n",
            "Iteration 137, loss = 34.47397111\n",
            "Iteration 138, loss = 34.42029640\n",
            "Iteration 139, loss = 34.53463144\n",
            "Iteration 140, loss = 34.45389244\n",
            "Iteration 141, loss = 34.43723568\n",
            "Iteration 142, loss = 34.42236780\n",
            "Iteration 143, loss = 34.36655343\n",
            "Iteration 144, loss = 34.36797283\n",
            "Iteration 145, loss = 34.42636734\n",
            "Iteration 146, loss = 34.23665169\n",
            "Iteration 147, loss = 34.30031900\n",
            "Iteration 148, loss = 34.23815253\n",
            "Iteration 149, loss = 34.11314334\n",
            "Iteration 150, loss = 34.33137524\n",
            "Iteration 151, loss = 34.18652858\n",
            "Iteration 152, loss = 34.17261008\n",
            "Iteration 153, loss = 34.18696002\n",
            "Iteration 154, loss = 34.04483229\n",
            "Iteration 155, loss = 34.20374762\n",
            "Iteration 156, loss = 34.08281174\n",
            "Iteration 157, loss = 34.06700835\n",
            "Iteration 158, loss = 34.02379459\n",
            "Iteration 159, loss = 34.10616393\n",
            "Iteration 160, loss = 33.99615237\n",
            "Iteration 161, loss = 33.86418578\n",
            "Iteration 162, loss = 33.97034168\n",
            "Iteration 163, loss = 33.86791021\n",
            "Iteration 164, loss = 34.00917213\n",
            "Iteration 165, loss = 33.96181645\n",
            "Iteration 166, loss = 33.87735476\n",
            "Iteration 167, loss = 33.80543226\n",
            "Iteration 168, loss = 33.91364818\n",
            "Iteration 169, loss = 33.72805348\n",
            "Iteration 170, loss = 33.84735163\n",
            "Iteration 171, loss = 33.86084256\n",
            "Iteration 172, loss = 33.69705353\n",
            "Iteration 173, loss = 33.71414586\n",
            "Iteration 174, loss = 33.67528343\n",
            "Iteration 175, loss = 33.64881240\n",
            "Iteration 176, loss = 33.67978938\n",
            "Iteration 177, loss = 33.53142577\n",
            "Iteration 178, loss = 33.54788167\n",
            "Iteration 179, loss = 33.66334515\n",
            "Iteration 180, loss = 33.62047369\n",
            "Iteration 181, loss = 33.54711497\n",
            "Iteration 182, loss = 33.49405825\n",
            "Iteration 183, loss = 33.56986827\n",
            "Iteration 184, loss = 33.48017263\n",
            "Iteration 185, loss = 33.45389868\n",
            "Iteration 186, loss = 33.43863196\n",
            "Iteration 187, loss = 33.39997315\n",
            "Iteration 188, loss = 33.40300080\n",
            "Iteration 189, loss = 33.50181100\n",
            "Iteration 190, loss = 33.33861227\n",
            "Iteration 191, loss = 33.38530608\n",
            "Iteration 192, loss = 33.29188334\n",
            "Iteration 193, loss = 33.41753883\n",
            "Iteration 194, loss = 33.23518453\n",
            "Iteration 195, loss = 33.48757513\n",
            "Iteration 196, loss = 33.21164109\n",
            "Iteration 197, loss = 33.15772061\n",
            "Iteration 198, loss = 33.18812481\n",
            "Iteration 199, loss = 33.24520734\n",
            "Iteration 200, loss = 33.20063294\n",
            "Iteration 201, loss = 33.16239685\n",
            "Iteration 202, loss = 33.07164436\n",
            "Iteration 203, loss = 33.31203946\n",
            "Iteration 204, loss = 33.17619440\n",
            "Iteration 205, loss = 33.21362826\n",
            "Iteration 206, loss = 33.05717633\n",
            "Iteration 207, loss = 32.95868264\n",
            "Iteration 208, loss = 33.09420462\n",
            "Iteration 209, loss = 32.94121219\n",
            "Iteration 210, loss = 33.06984210\n",
            "Iteration 211, loss = 33.01962682\n",
            "Iteration 212, loss = 33.03049781\n",
            "Iteration 213, loss = 33.12608460\n",
            "Iteration 214, loss = 32.99288114\n",
            "Iteration 215, loss = 33.11909799\n",
            "Iteration 216, loss = 32.85059138\n",
            "Iteration 217, loss = 32.90430577\n",
            "Iteration 218, loss = 32.89276861\n",
            "Iteration 219, loss = 32.86941088\n",
            "Iteration 220, loss = 32.95135124\n",
            "Iteration 221, loss = 32.86095532\n",
            "Iteration 222, loss = 32.88379546\n",
            "Iteration 223, loss = 32.86378388\n",
            "Iteration 224, loss = 32.96527379\n",
            "Iteration 225, loss = 33.00260254\n",
            "Iteration 226, loss = 32.82072809\n",
            "Iteration 227, loss = 32.76321935\n",
            "Iteration 228, loss = 32.86040512\n",
            "Iteration 229, loss = 32.77502085\n",
            "Iteration 230, loss = 32.81110760\n",
            "Iteration 231, loss = 32.70810211\n",
            "Iteration 232, loss = 32.69516783\n",
            "Iteration 233, loss = 32.81856432\n",
            "Iteration 234, loss = 32.72341242\n",
            "Iteration 235, loss = 32.87245272\n",
            "Iteration 236, loss = 32.49671714\n",
            "Iteration 237, loss = 32.67077381\n",
            "Iteration 238, loss = 32.65143672\n",
            "Iteration 239, loss = 32.63583517\n",
            "Iteration 240, loss = 32.79348463\n",
            "Iteration 241, loss = 32.60623834\n",
            "Iteration 242, loss = 32.68464352\n",
            "Iteration 243, loss = 32.62901582\n",
            "Iteration 244, loss = 32.63129987\n",
            "Iteration 245, loss = 32.75206166\n",
            "Iteration 246, loss = 32.62165676\n",
            "Iteration 247, loss = 32.56697286\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 86.37314995\n",
            "Iteration 2, loss = 67.27066463\n",
            "Iteration 3, loss = 60.15399301\n",
            "Iteration 4, loss = 55.31749473\n",
            "Iteration 5, loss = 52.58605640\n",
            "Iteration 6, loss = 51.63670577\n",
            "Iteration 7, loss = 50.68051830\n",
            "Iteration 8, loss = 50.11443878\n",
            "Iteration 9, loss = 49.67392328\n",
            "Iteration 10, loss = 49.25964655\n",
            "Iteration 11, loss = 48.90723345\n",
            "Iteration 12, loss = 48.59709376\n",
            "Iteration 13, loss = 48.23117114\n",
            "Iteration 14, loss = 47.94208056\n",
            "Iteration 15, loss = 48.04262764\n",
            "Iteration 16, loss = 47.76325810\n",
            "Iteration 17, loss = 47.68997863\n",
            "Iteration 18, loss = 47.38693597\n",
            "Iteration 19, loss = 47.34887413\n",
            "Iteration 20, loss = 47.19489167\n",
            "Iteration 21, loss = 47.07323803\n",
            "Iteration 22, loss = 46.90280438\n",
            "Iteration 23, loss = 46.80141431\n",
            "Iteration 24, loss = 46.73997847\n",
            "Iteration 25, loss = 46.75292825\n",
            "Iteration 26, loss = 46.73831671\n",
            "Iteration 27, loss = 46.56093848\n",
            "Iteration 28, loss = 46.42988277\n",
            "Iteration 29, loss = 46.37639351\n",
            "Iteration 30, loss = 46.37942345\n",
            "Iteration 31, loss = 46.16967265\n",
            "Iteration 32, loss = 46.16266257\n",
            "Iteration 33, loss = 46.25419570\n",
            "Iteration 34, loss = 45.89087693\n",
            "Iteration 35, loss = 45.94870844\n",
            "Iteration 36, loss = 45.73654586\n",
            "Iteration 37, loss = 45.58318938\n",
            "Iteration 38, loss = 45.65228275\n",
            "Iteration 39, loss = 45.50645804\n",
            "Iteration 40, loss = 45.46386508\n",
            "Iteration 41, loss = 45.34101179\n",
            "Iteration 42, loss = 45.22404485\n",
            "Iteration 43, loss = 45.20471666\n",
            "Iteration 44, loss = 45.46677041\n",
            "Iteration 45, loss = 45.13500839\n",
            "Iteration 46, loss = 45.02737555\n",
            "Iteration 47, loss = 45.12942229\n",
            "Iteration 48, loss = 45.22629621\n",
            "Iteration 49, loss = 44.90757373\n",
            "Iteration 50, loss = 44.96338216\n",
            "Iteration 51, loss = 44.78899350\n",
            "Iteration 52, loss = 44.62480549\n",
            "Iteration 53, loss = 44.71493457\n",
            "Iteration 54, loss = 44.55504368\n",
            "Iteration 55, loss = 44.52431546\n",
            "Iteration 56, loss = 44.52299624\n",
            "Iteration 57, loss = 44.50169070\n",
            "Iteration 58, loss = 44.60086040\n",
            "Iteration 59, loss = 44.31125514\n",
            "Iteration 60, loss = 44.35878157\n",
            "Iteration 61, loss = 44.28622545\n",
            "Iteration 62, loss = 44.43422098\n",
            "Iteration 63, loss = 44.17052385\n",
            "Iteration 64, loss = 44.31133334\n",
            "Iteration 65, loss = 44.01033471\n",
            "Iteration 66, loss = 43.95484795\n",
            "Iteration 67, loss = 43.97172503\n",
            "Iteration 68, loss = 43.92879120\n",
            "Iteration 69, loss = 43.82187623\n",
            "Iteration 70, loss = 43.97708884\n",
            "Iteration 71, loss = 44.05665197\n",
            "Iteration 72, loss = 43.78775215\n",
            "Iteration 73, loss = 43.76335861\n",
            "Iteration 74, loss = 43.74566095\n",
            "Iteration 75, loss = 43.84872051\n",
            "Iteration 76, loss = 43.57872640\n",
            "Iteration 77, loss = 43.49269736\n",
            "Iteration 78, loss = 43.47512075\n",
            "Iteration 79, loss = 43.70692786\n",
            "Iteration 80, loss = 43.52906777\n",
            "Iteration 81, loss = 43.73219214\n",
            "Iteration 82, loss = 43.50975969\n",
            "Iteration 83, loss = 43.32062780\n",
            "Iteration 84, loss = 43.39020375\n",
            "Iteration 85, loss = 43.19198853\n",
            "Iteration 86, loss = 43.35516646\n",
            "Iteration 87, loss = 43.09821280\n",
            "Iteration 88, loss = 43.08613464\n",
            "Iteration 89, loss = 43.11925726\n",
            "Iteration 90, loss = 43.19554748\n",
            "Iteration 91, loss = 43.11785229\n",
            "Iteration 92, loss = 42.92077960\n",
            "Iteration 93, loss = 42.87546600\n",
            "Iteration 94, loss = 42.95546471\n",
            "Iteration 95, loss = 42.75686833\n",
            "Iteration 96, loss = 42.96293174\n",
            "Iteration 97, loss = 42.66689784\n",
            "Iteration 98, loss = 42.62499266\n",
            "Iteration 99, loss = 42.55845954\n",
            "Iteration 100, loss = 42.56102997\n",
            "Iteration 101, loss = 42.46530293\n",
            "Iteration 102, loss = 42.44248530\n",
            "Iteration 103, loss = 42.58963140\n",
            "Iteration 104, loss = 42.41435508\n",
            "Iteration 105, loss = 42.40473310\n",
            "Iteration 106, loss = 42.18094800\n",
            "Iteration 107, loss = 42.24485930\n",
            "Iteration 108, loss = 42.02005966\n",
            "Iteration 109, loss = 42.05910311\n",
            "Iteration 110, loss = 41.95501670\n",
            "Iteration 111, loss = 41.96588713\n",
            "Iteration 112, loss = 42.01464939\n",
            "Iteration 113, loss = 42.11518836\n",
            "Iteration 114, loss = 41.96931579\n",
            "Iteration 115, loss = 41.86022330\n",
            "Iteration 116, loss = 41.77668026\n",
            "Iteration 117, loss = 41.87988104\n",
            "Iteration 118, loss = 41.60204561\n",
            "Iteration 119, loss = 41.74511222\n",
            "Iteration 120, loss = 41.56667069\n",
            "Iteration 121, loss = 41.54325386\n",
            "Iteration 122, loss = 41.43701241\n",
            "Iteration 123, loss = 41.40929498\n",
            "Iteration 124, loss = 41.59601717\n",
            "Iteration 125, loss = 41.52955391\n",
            "Iteration 126, loss = 41.48520602\n",
            "Iteration 127, loss = 41.53278543\n",
            "Iteration 128, loss = 41.20277885\n",
            "Iteration 129, loss = 41.22497582\n",
            "Iteration 130, loss = 41.06742690\n",
            "Iteration 131, loss = 41.00122368\n",
            "Iteration 132, loss = 41.07882387\n",
            "Iteration 133, loss = 41.17560007\n",
            "Iteration 134, loss = 41.24895617\n",
            "Iteration 135, loss = 40.89671957\n",
            "Iteration 136, loss = 40.96996962\n",
            "Iteration 137, loss = 40.89558856\n",
            "Iteration 138, loss = 40.82627034\n",
            "Iteration 139, loss = 41.05995938\n",
            "Iteration 140, loss = 40.79108145\n",
            "Iteration 141, loss = 41.02806402\n",
            "Iteration 142, loss = 40.50506101\n",
            "Iteration 143, loss = 40.81290330\n",
            "Iteration 144, loss = 40.62365529\n",
            "Iteration 145, loss = 40.60986716\n",
            "Iteration 146, loss = 40.52784156\n",
            "Iteration 147, loss = 40.44550696\n",
            "Iteration 148, loss = 40.45153519\n",
            "Iteration 149, loss = 40.44230052\n",
            "Iteration 150, loss = 40.30687721\n",
            "Iteration 151, loss = 40.26250785\n",
            "Iteration 152, loss = 40.24004104\n",
            "Iteration 153, loss = 40.31760425\n",
            "Iteration 154, loss = 40.30956506\n",
            "Iteration 155, loss = 40.01629113\n",
            "Iteration 156, loss = 40.14795355\n",
            "Iteration 157, loss = 40.16527523\n",
            "Iteration 158, loss = 39.87871945\n",
            "Iteration 159, loss = 39.81217647\n",
            "Iteration 160, loss = 39.92618771\n",
            "Iteration 161, loss = 39.84644135\n",
            "Iteration 162, loss = 39.69495941\n",
            "Iteration 163, loss = 39.57746062\n",
            "Iteration 164, loss = 39.63266757\n",
            "Iteration 165, loss = 39.51875392\n",
            "Iteration 166, loss = 39.40312200\n",
            "Iteration 167, loss = 39.37051942\n",
            "Iteration 168, loss = 39.59519466\n",
            "Iteration 169, loss = 39.26331512\n",
            "Iteration 170, loss = 39.20419407\n",
            "Iteration 171, loss = 39.25547507\n",
            "Iteration 172, loss = 39.49347738\n",
            "Iteration 173, loss = 39.11872033\n",
            "Iteration 174, loss = 39.18205112\n",
            "Iteration 175, loss = 39.05707067\n",
            "Iteration 176, loss = 39.00911224\n",
            "Iteration 177, loss = 38.98347105\n",
            "Iteration 178, loss = 38.81881905\n",
            "Iteration 179, loss = 38.96663590\n",
            "Iteration 180, loss = 38.84002775\n",
            "Iteration 181, loss = 38.85252418\n",
            "Iteration 182, loss = 38.81637647\n",
            "Iteration 183, loss = 38.81215191\n",
            "Iteration 184, loss = 38.73068339\n",
            "Iteration 185, loss = 38.60406430\n",
            "Iteration 186, loss = 38.60580906\n",
            "Iteration 187, loss = 38.59526244\n",
            "Iteration 188, loss = 38.51303871\n",
            "Iteration 189, loss = 38.38925499\n",
            "Iteration 190, loss = 38.59312629\n",
            "Iteration 191, loss = 38.23482302\n",
            "Iteration 192, loss = 38.29714809\n",
            "Iteration 193, loss = 38.55205786\n",
            "Iteration 194, loss = 38.33805543\n",
            "Iteration 195, loss = 38.19959841\n",
            "Iteration 196, loss = 38.26901729\n",
            "Iteration 197, loss = 38.25718361\n",
            "Iteration 198, loss = 38.13990664\n",
            "Iteration 199, loss = 38.04530380\n",
            "Iteration 200, loss = 38.05667733\n",
            "Iteration 201, loss = 38.07151035\n",
            "Iteration 202, loss = 38.06192596\n",
            "Iteration 203, loss = 38.05362816\n",
            "Iteration 204, loss = 37.79995790\n",
            "Iteration 205, loss = 38.12863858\n",
            "Iteration 206, loss = 38.00281402\n",
            "Iteration 207, loss = 37.65353038\n",
            "Iteration 208, loss = 37.90156733\n",
            "Iteration 209, loss = 37.58774867\n",
            "Iteration 210, loss = 37.66412850\n",
            "Iteration 211, loss = 37.58741863\n",
            "Iteration 212, loss = 37.55143807\n",
            "Iteration 213, loss = 37.51122793\n",
            "Iteration 214, loss = 37.39490318\n",
            "Iteration 215, loss = 37.34974418\n",
            "Iteration 216, loss = 37.51726374\n",
            "Iteration 217, loss = 37.27523372\n",
            "Iteration 218, loss = 37.18502763\n",
            "Iteration 219, loss = 37.29073449\n",
            "Iteration 220, loss = 37.20752275\n",
            "Iteration 221, loss = 36.99251091\n",
            "Iteration 222, loss = 37.09417959\n",
            "Iteration 223, loss = 37.07611429\n",
            "Iteration 224, loss = 36.96574176\n",
            "Iteration 225, loss = 36.91851249\n",
            "Iteration 226, loss = 37.02958390\n",
            "Iteration 227, loss = 36.78788159\n",
            "Iteration 228, loss = 36.83029978\n",
            "Iteration 229, loss = 36.96592887\n",
            "Iteration 230, loss = 36.86552434\n",
            "Iteration 231, loss = 36.66626856\n",
            "Iteration 232, loss = 36.62692394\n",
            "Iteration 233, loss = 36.56914958\n",
            "Iteration 234, loss = 36.58188078\n",
            "Iteration 235, loss = 36.61523387\n",
            "Iteration 236, loss = 36.59198047\n",
            "Iteration 237, loss = 36.45469797\n",
            "Iteration 238, loss = 36.37854661\n",
            "Iteration 239, loss = 36.52969303\n",
            "Iteration 240, loss = 36.67395894\n",
            "Iteration 241, loss = 36.51653245\n",
            "Iteration 242, loss = 36.35530508\n",
            "Iteration 243, loss = 36.24670028\n",
            "Iteration 244, loss = 36.20179735\n",
            "Iteration 245, loss = 36.16044317\n",
            "Iteration 246, loss = 36.20236258\n",
            "Iteration 247, loss = 36.06798692\n",
            "Iteration 248, loss = 36.04030036\n",
            "Iteration 249, loss = 36.11615642\n",
            "Iteration 250, loss = 36.22720362\n",
            "Iteration 251, loss = 36.07591467\n",
            "Iteration 252, loss = 36.07723855\n",
            "Iteration 253, loss = 36.22869505\n",
            "Iteration 254, loss = 36.08404001\n",
            "Iteration 255, loss = 35.66506139\n",
            "Iteration 256, loss = 35.75446244\n",
            "Iteration 257, loss = 35.76615821\n",
            "Iteration 258, loss = 35.78710501\n",
            "Iteration 259, loss = 35.92520109\n",
            "Iteration 260, loss = 35.84871656\n",
            "Iteration 261, loss = 35.55344278\n",
            "Iteration 262, loss = 35.53124035\n",
            "Iteration 263, loss = 35.64601045\n",
            "Iteration 264, loss = 35.64828115\n",
            "Iteration 265, loss = 35.57487400\n",
            "Iteration 266, loss = 35.65019959\n",
            "Iteration 267, loss = 35.65839523\n",
            "Iteration 268, loss = 35.58873276\n",
            "Iteration 269, loss = 35.52207428\n",
            "Iteration 270, loss = 35.35584004\n",
            "Iteration 271, loss = 35.44313321\n",
            "Iteration 272, loss = 35.66068023\n",
            "Iteration 273, loss = 35.21247048\n",
            "Iteration 274, loss = 35.45680236\n",
            "Iteration 275, loss = 35.43341690\n",
            "Iteration 276, loss = 35.43187283\n",
            "Iteration 277, loss = 35.34308639\n",
            "Iteration 278, loss = 35.19065926\n",
            "Iteration 279, loss = 35.25436938\n",
            "Iteration 280, loss = 35.31933209\n",
            "Iteration 281, loss = 35.47794250\n",
            "Iteration 282, loss = 35.14628684\n",
            "Iteration 283, loss = 35.19771139\n",
            "Iteration 284, loss = 35.15113851\n",
            "Iteration 285, loss = 34.97441825\n",
            "Iteration 286, loss = 34.92994319\n",
            "Iteration 287, loss = 35.13451645\n",
            "Iteration 288, loss = 35.15340213\n",
            "Iteration 289, loss = 35.04994375\n",
            "Iteration 290, loss = 34.95296615\n",
            "Iteration 291, loss = 34.93078644\n",
            "Iteration 292, loss = 35.01221619\n",
            "Iteration 293, loss = 34.77807392\n",
            "Iteration 294, loss = 35.32108428\n",
            "Iteration 295, loss = 35.07332019\n",
            "Iteration 296, loss = 34.91812212\n",
            "Iteration 297, loss = 34.77164156\n",
            "Iteration 298, loss = 34.67840958\n",
            "Iteration 299, loss = 34.76083803\n",
            "Iteration 300, loss = 34.87280689\n",
            "Iteration 301, loss = 35.11397278\n",
            "Iteration 302, loss = 34.68878304\n",
            "Iteration 303, loss = 34.68204560\n",
            "Iteration 304, loss = 34.71136390\n",
            "Iteration 305, loss = 34.87923507\n",
            "Iteration 306, loss = 34.61312791\n",
            "Iteration 307, loss = 34.72363615\n",
            "Iteration 308, loss = 34.53763726\n",
            "Iteration 309, loss = 34.60886631\n",
            "Iteration 310, loss = 34.63886233\n",
            "Iteration 311, loss = 34.60811976\n",
            "Iteration 312, loss = 34.67347362\n",
            "Iteration 313, loss = 34.65479334\n",
            "Iteration 314, loss = 34.60579334\n",
            "Iteration 315, loss = 34.76414948\n",
            "Iteration 316, loss = 34.62676791\n",
            "Iteration 317, loss = 34.44179277\n",
            "Iteration 318, loss = 34.65864308\n",
            "Iteration 319, loss = 34.61665099\n",
            "Iteration 320, loss = 34.41051477\n",
            "Iteration 321, loss = 34.41040921\n",
            "Iteration 322, loss = 34.28068399\n",
            "Iteration 323, loss = 34.32365446\n",
            "Iteration 324, loss = 34.49365231\n",
            "Iteration 325, loss = 34.44091171\n",
            "Iteration 326, loss = 34.26895256\n",
            "Iteration 327, loss = 34.57029624\n",
            "Iteration 328, loss = 34.42812898\n",
            "Iteration 329, loss = 34.46920512\n",
            "Iteration 330, loss = 34.41332689\n",
            "Iteration 331, loss = 34.42656356\n",
            "Iteration 332, loss = 34.42411037\n",
            "Iteration 333, loss = 34.26780214\n",
            "Iteration 334, loss = 34.27526917\n",
            "Iteration 335, loss = 34.33656596\n",
            "Iteration 336, loss = 34.22966796\n",
            "Iteration 337, loss = 34.26471369\n",
            "Iteration 338, loss = 34.37076341\n",
            "Iteration 339, loss = 34.21337878\n",
            "Iteration 340, loss = 34.21880888\n",
            "Iteration 341, loss = 34.26766432\n",
            "Iteration 342, loss = 34.13827448\n",
            "Iteration 343, loss = 34.11268857\n",
            "Iteration 344, loss = 34.14575666\n",
            "Iteration 345, loss = 34.38772897\n",
            "Iteration 346, loss = 34.21463195\n",
            "Iteration 347, loss = 34.12881808\n",
            "Iteration 348, loss = 33.96240445\n",
            "Iteration 349, loss = 34.14833545\n",
            "Iteration 350, loss = 34.10053899\n",
            "Iteration 351, loss = 34.09201971\n",
            "Iteration 352, loss = 34.19891486\n",
            "Iteration 353, loss = 34.18750142\n",
            "Iteration 354, loss = 33.99904711\n",
            "Iteration 355, loss = 34.08477124\n",
            "Iteration 356, loss = 34.08581683\n",
            "Iteration 357, loss = 33.94301756\n",
            "Iteration 358, loss = 33.99313911\n",
            "Iteration 359, loss = 34.06822612\n",
            "Iteration 360, loss = 33.93822092\n",
            "Iteration 361, loss = 34.23187762\n",
            "Iteration 362, loss = 33.91416675\n",
            "Iteration 363, loss = 33.87573389\n",
            "Iteration 364, loss = 34.00674069\n",
            "Iteration 365, loss = 34.05635324\n",
            "Iteration 366, loss = 34.02735499\n",
            "Iteration 367, loss = 34.08614674\n",
            "Iteration 368, loss = 33.78626539\n",
            "Iteration 369, loss = 33.97684333\n",
            "Iteration 370, loss = 33.95404039\n",
            "Iteration 371, loss = 33.93725675\n",
            "Iteration 372, loss = 33.91772890\n",
            "Iteration 373, loss = 33.86019446\n",
            "Iteration 374, loss = 33.85305043\n",
            "Iteration 375, loss = 33.87556482\n",
            "Iteration 376, loss = 33.92367715\n",
            "Iteration 377, loss = 33.77922126\n",
            "Iteration 378, loss = 33.70633267\n",
            "Iteration 379, loss = 33.87360240\n",
            "Iteration 380, loss = 33.84003656\n",
            "Iteration 381, loss = 33.67125625\n",
            "Iteration 382, loss = 33.74781676\n",
            "Iteration 383, loss = 33.81786748\n",
            "Iteration 384, loss = 33.60171837\n",
            "Iteration 385, loss = 33.92685057\n",
            "Iteration 386, loss = 33.82234182\n",
            "Iteration 387, loss = 33.84725443\n",
            "Iteration 388, loss = 33.62869083\n",
            "Iteration 389, loss = 34.09816208\n",
            "Iteration 390, loss = 33.73476952\n",
            "Iteration 391, loss = 33.73679784\n",
            "Iteration 392, loss = 33.67041055\n",
            "Iteration 393, loss = 33.81829458\n",
            "Iteration 394, loss = 33.59374854\n",
            "Iteration 395, loss = 33.77874400\n",
            "Iteration 396, loss = 34.11713605\n",
            "Iteration 397, loss = 33.71484593\n",
            "Iteration 398, loss = 33.66120365\n",
            "Iteration 399, loss = 33.58473885\n",
            "Iteration 400, loss = 33.66637875\n",
            "Iteration 401, loss = 33.63273513\n",
            "Iteration 402, loss = 33.44553522\n",
            "Iteration 403, loss = 33.63331781\n",
            "Iteration 404, loss = 33.72073861\n",
            "Iteration 405, loss = 33.59651807\n",
            "Iteration 406, loss = 33.84586981\n",
            "Iteration 407, loss = 33.55682208\n",
            "Iteration 408, loss = 33.37858261\n",
            "Iteration 409, loss = 33.39892973\n",
            "Iteration 410, loss = 33.58272597\n",
            "Iteration 411, loss = 33.66721845\n",
            "Iteration 412, loss = 33.59754392\n",
            "Iteration 413, loss = 33.79718494\n",
            "Iteration 414, loss = 33.75558820\n",
            "Iteration 415, loss = 33.46683072\n",
            "Iteration 416, loss = 33.62462067\n",
            "Iteration 417, loss = 33.35321861\n",
            "Iteration 418, loss = 33.44308005\n",
            "Iteration 419, loss = 33.39900050\n",
            "Iteration 420, loss = 33.60587671\n",
            "Iteration 421, loss = 33.80270610\n",
            "Iteration 422, loss = 33.36790847\n",
            "Iteration 423, loss = 33.44467238\n",
            "Iteration 424, loss = 33.51595325\n",
            "Iteration 425, loss = 33.29843687\n",
            "Iteration 426, loss = 33.38474633\n",
            "Iteration 427, loss = 33.87319570\n",
            "Iteration 428, loss = 33.68430015\n",
            "Iteration 429, loss = 33.57752327\n",
            "Iteration 430, loss = 33.40946496\n",
            "Iteration 431, loss = 33.27587392\n",
            "Iteration 432, loss = 33.77455505\n",
            "Iteration 433, loss = 33.48354065\n",
            "Iteration 434, loss = 33.44611137\n",
            "Iteration 435, loss = 33.48161909\n",
            "Iteration 436, loss = 33.47150941\n",
            "Iteration 437, loss = 33.25337474\n",
            "Iteration 438, loss = 33.31618969\n",
            "Iteration 439, loss = 33.35276573\n",
            "Iteration 440, loss = 33.30389065\n",
            "Iteration 441, loss = 33.31198653\n",
            "Iteration 442, loss = 33.49326641\n",
            "Iteration 443, loss = 33.32498021\n",
            "Iteration 444, loss = 33.20900212\n",
            "Iteration 445, loss = 33.35209802\n",
            "Iteration 446, loss = 33.25456418\n",
            "Iteration 447, loss = 33.50211406\n",
            "Iteration 448, loss = 33.30667125\n",
            "Iteration 449, loss = 33.39492933\n",
            "Iteration 450, loss = 33.34933757\n",
            "Iteration 451, loss = 33.22740521\n",
            "Iteration 452, loss = 33.10807148\n",
            "Iteration 453, loss = 33.61143267\n",
            "Iteration 454, loss = 33.26880941\n",
            "Iteration 455, loss = 33.29334093\n",
            "Iteration 456, loss = 33.05117443\n",
            "Iteration 457, loss = 33.40253976\n",
            "Iteration 458, loss = 33.25608179\n",
            "Iteration 459, loss = 33.47194978\n",
            "Iteration 460, loss = 33.37005162\n",
            "Iteration 461, loss = 33.14004694\n",
            "Iteration 462, loss = 33.22645040\n",
            "Iteration 463, loss = 33.62428234\n",
            "Iteration 464, loss = 33.14798749\n",
            "Iteration 465, loss = 33.26008831\n",
            "Iteration 466, loss = 33.30484405\n",
            "Iteration 467, loss = 33.25121676\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkSNvsg6Vfrg",
        "colab_type": "code",
        "outputId": "1d820963-8d7c-4136-f36a-fda7cee02a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(wei_first_dev)\n",
        "print(wei_first_test)\n",
        "print(wei_second_dev)\n",
        "print(wei_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[133.43105170284676, 125.80799632370217, 108.57340989780583]\n",
            "[101.3566533976719, 117.56137299949785, 101.03580286297819]\n",
            "[126.38121705032674, 127.72600649339834, 158.57057756038293]\n",
            "[107.25731990044734, 121.05456791881197, 123.46315811651259]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9widszr2cNjc",
        "colab_type": "code",
        "outputId": "49bee38a-99e9-4ba2-c4e7-a3e2910c414e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "wei_test_mse = pd.DataFrame(list(zip(wei_first_test, wei_second_test)), columns=['WEIGHTED_REG','WEIGHTED_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "wei_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_c4e93940_9eed_11ea_9f06_0242ac1c0002row0_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_c4e93940_9eed_11ea_9f06_0242ac1c0002row1_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_c4e93940_9eed_11ea_9f06_0242ac1c0002row2_col0 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >WEIGHTED_REG</th>        <th class=\"col_heading level0 col1\" >WEIGHTED_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002row0_col0\" class=\"data row0 col0\" >101.356653</td>\n",
              "                        <td id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002row0_col1\" class=\"data row0 col1\" >107.257320</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002row1_col0\" class=\"data row1 col0\" >117.561373</td>\n",
              "                        <td id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002row1_col1\" class=\"data row1 col1\" >121.054568</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002row2_col0\" class=\"data row2 col0\" >101.035803</td>\n",
              "                        <td id=\"T_c4e93940_9eed_11ea_9f06_0242ac1c0002row2_col1\" class=\"data row2 col1\" >123.463158</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28ef1eac18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkBtIVGhPjkJ",
        "colab_type": "text"
      },
      "source": [
        "### PRED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI_JWLBvqeaL",
        "colab_type": "code",
        "outputId": "181ad772-7dd0-4df7-ec23-1993795b3afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#{'activation': 'identity', 'alpha': 1, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 64, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 200, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000}\n",
        "\n",
        "ma_train = pd.concat([female_train, mixed_train], ignore_index=True)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([female_train_pred, mixed_train_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "ma_tgt = male_tgt.copy().reset_index(drop=True)\n",
        "ma_tgt_pred = male_tgt_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([male_train, mixed_train], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([male_train_pred, mixed_train_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "fe_tgt = female_tgt.copy().reset_index(drop=True)\n",
        "fe_tgt_pred = female_tgt_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([male_train, female_train], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([male_train_pred, female_train_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "mi_tgt = mixed_tgt.copy().reset_index(drop=True)\n",
        "mi_tgt_pred = mixed_tgt_pred.copy().reset_index(drop=True)\n",
        "\n",
        "#d = {'male':[ma_train, ma_dev, ma_test, male_tgt], 'female':[fe_train, fe_dev, fe_test, female_tgt], 'mixed':[mi_train, mi_dev, mi_test, mixed_tgt]}\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred, ma_tgt, ma_tgt_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred, fe_tgt, fe_tgt_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred, mi_tgt, mi_tgt_pred]}\n",
        "\n",
        "pre_first_dev = []\n",
        "pre_second_dev = []\n",
        "pre_first_test = []\n",
        "pre_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  X_tgt = df[6]\n",
        "  Y_tgt = df[7]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #print(type(X_train))\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "\n",
        "  #Training and running the SRCONLY model first\n",
        "  src_linr = LinearRegression() #normalize=True\n",
        "  src_linr.fit(X_train, Y_train)\n",
        "  #X_tgt['Pred'] = src_linr.predict(X_tgt)\n",
        "  Y_hat_tgt = src_linr.predict(X_tgt)\n",
        "  X_tgt_new = X_tgt.copy()\n",
        "  X_tgt_new['Pred'] = Y_hat_tgt\n",
        "  #X_tgt_new = X_tgt_new.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "  #X_dev['Pred'] = src_linr.predict(X_dev)\n",
        "  Y_hat_dev = src_linr.predict(X_dev)\n",
        "  X_dev_new = X_dev.copy()\n",
        "  X_dev_new['Pred'] = Y_hat_dev\n",
        "  #X_dev_new = X_dev_new.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "  #X_test['Pred'] = src_linr.predict(X_test)\n",
        "  Y_hat_test = src_linr.predict(X_test)\n",
        "  X_test_new = X_test.copy()\n",
        "  X_test_new['Pred'] = Y_hat_test\n",
        "  #X_test_new = X_test_new.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "  tgt_linr = LinearRegression() #normalize=True\n",
        "  tgt_linr.fit(X_tgt_new, Y_tgt)\n",
        "  pre_first_dev.append(mean_squared_error(Y_dev, tgt_linr.predict(X_dev_new)))\n",
        "  pre_first_test.append(mean_squared_error(Y_test, tgt_linr.predict(X_test_new)))\n",
        "\n",
        "  #Training and running the TGTONLY model now\n",
        "  src_mlp = MLPRegressor(activation='relu', alpha=0.0001, batch_size=128, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  src_mlp.fit(X_train, Y_train)\n",
        "  #X_tgt['Pred'] = src_linr.predict(X_tgt)\n",
        "  Y_hat_tgt = src_mlp.predict(X_tgt)\n",
        "  X_tgt_new = X_tgt.copy()\n",
        "  X_tgt_new['Pred'] = Y_hat_tgt\n",
        "  #X_tgt_new = X_tgt_new.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "  #X_dev['Pred'] = src_linr.predict(X_dev)\n",
        "  Y_hat_dev = src_mlp.predict(X_dev)\n",
        "  X_dev_new = X_dev.copy()\n",
        "  X_dev_new['Pred'] = Y_hat_dev\n",
        "  #X_dev_new = X_dev_new.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "  #X_test['Pred'] = src_linr.predict(X_test)\n",
        "  Y_hat_test = src_mlp.predict(X_test)\n",
        "  X_test_new = X_test.copy()\n",
        "  X_test_new['Pred'] = Y_hat_test\n",
        "  #X_test_new = X_test_new.sample(n=100, random_state = 24).reset_index(drop=True)\n",
        "  tgt_mlp = MLPRegressor(activation='relu', alpha=1, batch_size=64, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  tgt_mlp.fit(X_tgt_new, Y_tgt)\n",
        "  #tgt_mlp = MLPRegressor(random_state=24)\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30), (25,25,25,25)], 'activation':['relu', 'identity'], 'alpha':[0.0001, 0.01, 0.1, 1, 10], 'batch_size':[200, 128, 64, 32], 'max_iter':[1000], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30)], 'alpha':[0.0001, 0.01, 1], 'activation':['relu', 'identity'], 'max_iter':[1000], 'batch_size':[200, 128, 64], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200], 'learning_rate_init':[0.001, 0.01]}\n",
        "  #mse = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False\n",
        "  #clf = GridSearchCV(tgt_mlp, parameters, scoring = 'neg_mean_squared_error', n_jobs = -1)#, verbose=True, scoring = mse)\n",
        "  #clf.fit(X_tgt_new, Y_tgt)\n",
        "  #print(clf.best_params_)\n",
        "\n",
        "  pre_second_dev.append(mean_squared_error(Y_dev, tgt_mlp.predict(X_dev_new)))\n",
        "  pre_second_test.append(mean_squared_error(Y_test, tgt_mlp.predict(X_test_new)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 108.66570139\n",
            "Iteration 2, loss = 75.15090289\n",
            "Iteration 3, loss = 71.20661604\n",
            "Iteration 4, loss = 68.32243477\n",
            "Iteration 5, loss = 65.50988555\n",
            "Iteration 6, loss = 62.76317216\n",
            "Iteration 7, loss = 60.27149179\n",
            "Iteration 8, loss = 58.24121499\n",
            "Iteration 9, loss = 56.96747127\n",
            "Iteration 10, loss = 55.87717995\n",
            "Iteration 11, loss = 55.59715619\n",
            "Iteration 12, loss = 54.87448394\n",
            "Iteration 13, loss = 54.47425668\n",
            "Iteration 14, loss = 54.10991681\n",
            "Iteration 15, loss = 53.94279295\n",
            "Iteration 16, loss = 54.05404891\n",
            "Iteration 17, loss = 53.73277701\n",
            "Iteration 18, loss = 53.57075254\n",
            "Iteration 19, loss = 53.30802557\n",
            "Iteration 20, loss = 53.28916486\n",
            "Iteration 21, loss = 53.13240656\n",
            "Iteration 22, loss = 53.09121649\n",
            "Iteration 23, loss = 53.24988559\n",
            "Iteration 24, loss = 52.96685552\n",
            "Iteration 25, loss = 52.71211885\n",
            "Iteration 26, loss = 52.82176748\n",
            "Iteration 27, loss = 52.60537781\n",
            "Iteration 28, loss = 52.69057856\n",
            "Iteration 29, loss = 52.50889566\n",
            "Iteration 30, loss = 52.55507271\n",
            "Iteration 31, loss = 52.55317143\n",
            "Iteration 32, loss = 52.44675448\n",
            "Iteration 33, loss = 52.36817643\n",
            "Iteration 34, loss = 52.32275508\n",
            "Iteration 35, loss = 52.17152898\n",
            "Iteration 36, loss = 52.10855921\n",
            "Iteration 37, loss = 52.16158485\n",
            "Iteration 38, loss = 52.25573866\n",
            "Iteration 39, loss = 52.14177327\n",
            "Iteration 40, loss = 52.38356810\n",
            "Iteration 41, loss = 52.12278488\n",
            "Iteration 42, loss = 52.01733633\n",
            "Iteration 43, loss = 52.14998828\n",
            "Iteration 44, loss = 52.20801274\n",
            "Iteration 45, loss = 51.95123623\n",
            "Iteration 46, loss = 52.08779449\n",
            "Iteration 47, loss = 52.03848659\n",
            "Iteration 48, loss = 51.90216913\n",
            "Iteration 49, loss = 51.87196924\n",
            "Iteration 50, loss = 51.87813545\n",
            "Iteration 51, loss = 52.01788948\n",
            "Iteration 52, loss = 52.06085293\n",
            "Iteration 53, loss = 51.97538947\n",
            "Iteration 54, loss = 52.02298558\n",
            "Iteration 55, loss = 51.82953861\n",
            "Iteration 56, loss = 52.03045333\n",
            "Iteration 57, loss = 52.00050137\n",
            "Iteration 58, loss = 52.01707606\n",
            "Iteration 59, loss = 51.80971425\n",
            "Iteration 60, loss = 51.94596136\n",
            "Iteration 61, loss = 51.84460472\n",
            "Iteration 62, loss = 51.79987992\n",
            "Iteration 63, loss = 51.97751055\n",
            "Iteration 64, loss = 51.76760593\n",
            "Iteration 65, loss = 51.71353282\n",
            "Iteration 66, loss = 51.74596281\n",
            "Iteration 67, loss = 51.70875248\n",
            "Iteration 68, loss = 51.79182209\n",
            "Iteration 69, loss = 51.64894918\n",
            "Iteration 70, loss = 51.79611759\n",
            "Iteration 71, loss = 52.11632468\n",
            "Iteration 72, loss = 51.88980474\n",
            "Iteration 73, loss = 51.81215776\n",
            "Iteration 74, loss = 51.70763663\n",
            "Iteration 75, loss = 51.56613499\n",
            "Iteration 76, loss = 51.69420798\n",
            "Iteration 77, loss = 51.78356901\n",
            "Iteration 78, loss = 51.63069205\n",
            "Iteration 79, loss = 51.70287196\n",
            "Iteration 80, loss = 51.74207812\n",
            "Iteration 81, loss = 51.81889144\n",
            "Iteration 82, loss = 51.75607301\n",
            "Iteration 83, loss = 51.71991753\n",
            "Iteration 84, loss = 51.59510753\n",
            "Iteration 85, loss = 51.73265339\n",
            "Iteration 86, loss = 51.93874611\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 425.34137768\n",
            "Iteration 2, loss = 372.12499664\n",
            "Iteration 3, loss = 325.09593831\n",
            "Iteration 4, loss = 282.15184200\n",
            "Iteration 5, loss = 245.83566912\n",
            "Iteration 6, loss = 212.80118163\n",
            "Iteration 7, loss = 184.59565140\n",
            "Iteration 8, loss = 162.76282837\n",
            "Iteration 9, loss = 143.85158204\n",
            "Iteration 10, loss = 128.71268348\n",
            "Iteration 11, loss = 118.15344832\n",
            "Iteration 12, loss = 109.86880764\n",
            "Iteration 13, loss = 105.07974591\n",
            "Iteration 14, loss = 100.71130789\n",
            "Iteration 15, loss = 98.55562507\n",
            "Iteration 16, loss = 98.07507535\n",
            "Iteration 17, loss = 97.37620797\n",
            "Iteration 18, loss = 97.43095092\n",
            "Iteration 19, loss = 97.14464923\n",
            "Iteration 20, loss = 96.85557823\n",
            "Iteration 21, loss = 96.33069117\n",
            "Iteration 22, loss = 95.22247965\n",
            "Iteration 23, loss = 93.86515819\n",
            "Iteration 24, loss = 92.23206945\n",
            "Iteration 25, loss = 90.77366025\n",
            "Iteration 26, loss = 88.99111418\n",
            "Iteration 27, loss = 88.01367645\n",
            "Iteration 28, loss = 86.55845274\n",
            "Iteration 29, loss = 85.49512946\n",
            "Iteration 30, loss = 84.62444642\n",
            "Iteration 31, loss = 83.80139158\n",
            "Iteration 32, loss = 83.15831173\n",
            "Iteration 33, loss = 82.61701087\n",
            "Iteration 34, loss = 81.90694203\n",
            "Iteration 35, loss = 81.43939693\n",
            "Iteration 36, loss = 80.99172594\n",
            "Iteration 37, loss = 80.39746990\n",
            "Iteration 38, loss = 80.06003890\n",
            "Iteration 39, loss = 79.49823566\n",
            "Iteration 40, loss = 79.09910095\n",
            "Iteration 41, loss = 78.66621907\n",
            "Iteration 42, loss = 78.28163094\n",
            "Iteration 43, loss = 77.94982764\n",
            "Iteration 44, loss = 77.56966925\n",
            "Iteration 45, loss = 77.20166723\n",
            "Iteration 46, loss = 76.95273544\n",
            "Iteration 47, loss = 76.75280043\n",
            "Iteration 48, loss = 76.41185356\n",
            "Iteration 49, loss = 76.19363373\n",
            "Iteration 50, loss = 75.95847071\n",
            "Iteration 51, loss = 75.73081759\n",
            "Iteration 52, loss = 75.54927889\n",
            "Iteration 53, loss = 75.30863302\n",
            "Iteration 54, loss = 75.17286802\n",
            "Iteration 55, loss = 74.92555839\n",
            "Iteration 56, loss = 74.75811648\n",
            "Iteration 57, loss = 74.54890960\n",
            "Iteration 58, loss = 74.36537426\n",
            "Iteration 59, loss = 74.20978741\n",
            "Iteration 60, loss = 74.03199225\n",
            "Iteration 61, loss = 73.84520465\n",
            "Iteration 62, loss = 73.68199524\n",
            "Iteration 63, loss = 73.47609694\n",
            "Iteration 64, loss = 73.33138473\n",
            "Iteration 65, loss = 73.14161378\n",
            "Iteration 66, loss = 72.96266889\n",
            "Iteration 67, loss = 72.77649643\n",
            "Iteration 68, loss = 72.65772197\n",
            "Iteration 69, loss = 72.46287417\n",
            "Iteration 70, loss = 72.31513130\n",
            "Iteration 71, loss = 72.13774720\n",
            "Iteration 72, loss = 71.95530671\n",
            "Iteration 73, loss = 71.81016947\n",
            "Iteration 74, loss = 71.68224965\n",
            "Iteration 75, loss = 71.53561129\n",
            "Iteration 76, loss = 71.38201476\n",
            "Iteration 77, loss = 71.24668845\n",
            "Iteration 78, loss = 71.17881167\n",
            "Iteration 79, loss = 71.02161739\n",
            "Iteration 80, loss = 70.90017788\n",
            "Iteration 81, loss = 70.77563028\n",
            "Iteration 82, loss = 70.65781041\n",
            "Iteration 83, loss = 70.54180476\n",
            "Iteration 84, loss = 70.42362413\n",
            "Iteration 85, loss = 70.33172153\n",
            "Iteration 86, loss = 70.29296163\n",
            "Iteration 87, loss = 70.10746437\n",
            "Iteration 88, loss = 69.94203264\n",
            "Iteration 89, loss = 69.83170367\n",
            "Iteration 90, loss = 69.72470914\n",
            "Iteration 91, loss = 69.60400905\n",
            "Iteration 92, loss = 69.48466562\n",
            "Iteration 93, loss = 69.36054645\n",
            "Iteration 94, loss = 69.24039368\n",
            "Iteration 95, loss = 69.06916336\n",
            "Iteration 96, loss = 69.02891056\n",
            "Iteration 97, loss = 68.95347405\n",
            "Iteration 98, loss = 68.89842300\n",
            "Iteration 99, loss = 68.83335104\n",
            "Iteration 100, loss = 68.65753559\n",
            "Iteration 101, loss = 68.54559675\n",
            "Iteration 102, loss = 68.41437356\n",
            "Iteration 103, loss = 68.30282712\n",
            "Iteration 104, loss = 68.22205340\n",
            "Iteration 105, loss = 68.10999252\n",
            "Iteration 106, loss = 68.01523371\n",
            "Iteration 107, loss = 67.85889225\n",
            "Iteration 108, loss = 67.82113494\n",
            "Iteration 109, loss = 67.69854423\n",
            "Iteration 110, loss = 67.61413257\n",
            "Iteration 111, loss = 67.52416030\n",
            "Iteration 112, loss = 67.37668426\n",
            "Iteration 113, loss = 67.25873475\n",
            "Iteration 114, loss = 67.16293399\n",
            "Iteration 115, loss = 67.10949958\n",
            "Iteration 116, loss = 67.01019440\n",
            "Iteration 117, loss = 66.89730656\n",
            "Iteration 118, loss = 66.81276901\n",
            "Iteration 119, loss = 66.70661353\n",
            "Iteration 120, loss = 66.62846825\n",
            "Iteration 121, loss = 66.53334843\n",
            "Iteration 122, loss = 66.46698791\n",
            "Iteration 123, loss = 66.31720881\n",
            "Iteration 124, loss = 66.24023197\n",
            "Iteration 125, loss = 66.16351686\n",
            "Iteration 126, loss = 66.11575320\n",
            "Iteration 127, loss = 66.07728243\n",
            "Iteration 128, loss = 66.02071707\n",
            "Iteration 129, loss = 65.95403350\n",
            "Iteration 130, loss = 65.85913111\n",
            "Iteration 131, loss = 65.69580914\n",
            "Iteration 132, loss = 65.58889374\n",
            "Iteration 133, loss = 65.48427284\n",
            "Iteration 134, loss = 65.38809636\n",
            "Iteration 135, loss = 65.31278055\n",
            "Iteration 136, loss = 65.22610924\n",
            "Iteration 137, loss = 65.19098889\n",
            "Iteration 138, loss = 65.21204926\n",
            "Iteration 139, loss = 65.04364824\n",
            "Iteration 140, loss = 64.92119070\n",
            "Iteration 141, loss = 65.02308263\n",
            "Iteration 142, loss = 64.81981958\n",
            "Iteration 143, loss = 65.01710014\n",
            "Iteration 144, loss = 64.70418741\n",
            "Iteration 145, loss = 64.50407067\n",
            "Iteration 146, loss = 64.48445763\n",
            "Iteration 147, loss = 64.50228518\n",
            "Iteration 148, loss = 64.37974925\n",
            "Iteration 149, loss = 64.26287782\n",
            "Iteration 150, loss = 64.17616206\n",
            "Iteration 151, loss = 64.10608522\n",
            "Iteration 152, loss = 64.02918740\n",
            "Iteration 153, loss = 64.02309178\n",
            "Iteration 154, loss = 63.90160851\n",
            "Iteration 155, loss = 63.83985201\n",
            "Iteration 156, loss = 63.73534133\n",
            "Iteration 157, loss = 63.59787626\n",
            "Iteration 158, loss = 63.56496265\n",
            "Iteration 159, loss = 63.51628917\n",
            "Iteration 160, loss = 63.46628741\n",
            "Iteration 161, loss = 63.37086166\n",
            "Iteration 162, loss = 63.26831243\n",
            "Iteration 163, loss = 63.20493185\n",
            "Iteration 164, loss = 63.16009405\n",
            "Iteration 165, loss = 63.12734437\n",
            "Iteration 166, loss = 63.04391603\n",
            "Iteration 167, loss = 62.98145519\n",
            "Iteration 168, loss = 62.88689853\n",
            "Iteration 169, loss = 62.80327250\n",
            "Iteration 170, loss = 62.73375431\n",
            "Iteration 171, loss = 62.70119299\n",
            "Iteration 172, loss = 62.59363569\n",
            "Iteration 173, loss = 62.54573259\n",
            "Iteration 174, loss = 62.56181642\n",
            "Iteration 175, loss = 62.44055489\n",
            "Iteration 176, loss = 62.31830442\n",
            "Iteration 177, loss = 62.23488395\n",
            "Iteration 178, loss = 62.22013009\n",
            "Iteration 179, loss = 62.11813442\n",
            "Iteration 180, loss = 62.05576360\n",
            "Iteration 181, loss = 61.98424129\n",
            "Iteration 182, loss = 61.90589887\n",
            "Iteration 183, loss = 61.85699316\n",
            "Iteration 184, loss = 61.99712848\n",
            "Iteration 185, loss = 61.80679718\n",
            "Iteration 186, loss = 61.71538110\n",
            "Iteration 187, loss = 61.62680456\n",
            "Iteration 188, loss = 61.54223198\n",
            "Iteration 189, loss = 61.49106244\n",
            "Iteration 190, loss = 61.42454584\n",
            "Iteration 191, loss = 61.36953765\n",
            "Iteration 192, loss = 61.30998977\n",
            "Iteration 193, loss = 61.25362840\n",
            "Iteration 194, loss = 61.25793825\n",
            "Iteration 195, loss = 61.11162478\n",
            "Iteration 196, loss = 61.04612036\n",
            "Iteration 197, loss = 61.07716805\n",
            "Iteration 198, loss = 60.87836831\n",
            "Iteration 199, loss = 60.93265208\n",
            "Iteration 200, loss = 60.88114416\n",
            "Iteration 201, loss = 60.84295312\n",
            "Iteration 202, loss = 60.76278403\n",
            "Iteration 203, loss = 60.63086395\n",
            "Iteration 204, loss = 60.63198155\n",
            "Iteration 205, loss = 60.64513747\n",
            "Iteration 206, loss = 60.54868765\n",
            "Iteration 207, loss = 60.56144083\n",
            "Iteration 208, loss = 60.56802887\n",
            "Iteration 209, loss = 60.30428391\n",
            "Iteration 210, loss = 60.52216015\n",
            "Iteration 211, loss = 60.18924642\n",
            "Iteration 212, loss = 60.36537507\n",
            "Iteration 213, loss = 60.08442609\n",
            "Iteration 214, loss = 60.04382334\n",
            "Iteration 215, loss = 60.07262346\n",
            "Iteration 216, loss = 59.94216277\n",
            "Iteration 217, loss = 59.84091671\n",
            "Iteration 218, loss = 59.77231727\n",
            "Iteration 219, loss = 59.70879404\n",
            "Iteration 220, loss = 59.65800352\n",
            "Iteration 221, loss = 59.66304379\n",
            "Iteration 222, loss = 59.53610550\n",
            "Iteration 223, loss = 59.52187168\n",
            "Iteration 224, loss = 59.49301073\n",
            "Iteration 225, loss = 59.42869059\n",
            "Iteration 226, loss = 59.43453377\n",
            "Iteration 227, loss = 59.39873803\n",
            "Iteration 228, loss = 59.21565035\n",
            "Iteration 229, loss = 59.14826906\n",
            "Iteration 230, loss = 59.11898336\n",
            "Iteration 231, loss = 59.07999717\n",
            "Iteration 232, loss = 59.11056230\n",
            "Iteration 233, loss = 59.05013412\n",
            "Iteration 234, loss = 58.89005064\n",
            "Iteration 235, loss = 58.90077885\n",
            "Iteration 236, loss = 58.82158747\n",
            "Iteration 237, loss = 58.76855792\n",
            "Iteration 238, loss = 58.69741635\n",
            "Iteration 239, loss = 58.67633411\n",
            "Iteration 240, loss = 58.57650927\n",
            "Iteration 241, loss = 58.51957678\n",
            "Iteration 242, loss = 58.45205323\n",
            "Iteration 243, loss = 58.40548350\n",
            "Iteration 244, loss = 58.41652919\n",
            "Iteration 245, loss = 58.43816966\n",
            "Iteration 246, loss = 58.39482993\n",
            "Iteration 247, loss = 58.24101489\n",
            "Iteration 248, loss = 58.28834003\n",
            "Iteration 249, loss = 58.12245321\n",
            "Iteration 250, loss = 58.15250766\n",
            "Iteration 251, loss = 58.02661584\n",
            "Iteration 252, loss = 57.97395313\n",
            "Iteration 253, loss = 57.93169098\n",
            "Iteration 254, loss = 58.00418578\n",
            "Iteration 255, loss = 57.86017596\n",
            "Iteration 256, loss = 57.81142806\n",
            "Iteration 257, loss = 57.76349231\n",
            "Iteration 258, loss = 57.72215638\n",
            "Iteration 259, loss = 57.68214395\n",
            "Iteration 260, loss = 57.60936669\n",
            "Iteration 261, loss = 57.55659779\n",
            "Iteration 262, loss = 57.50892709\n",
            "Iteration 263, loss = 57.50607423\n",
            "Iteration 264, loss = 57.41235050\n",
            "Iteration 265, loss = 57.36231609\n",
            "Iteration 266, loss = 57.36252230\n",
            "Iteration 267, loss = 57.30846128\n",
            "Iteration 268, loss = 57.20661070\n",
            "Iteration 269, loss = 57.27487360\n",
            "Iteration 270, loss = 57.20629774\n",
            "Iteration 271, loss = 57.13905404\n",
            "Iteration 272, loss = 57.13580784\n",
            "Iteration 273, loss = 57.03270005\n",
            "Iteration 274, loss = 57.19190776\n",
            "Iteration 275, loss = 56.99975707\n",
            "Iteration 276, loss = 56.91518101\n",
            "Iteration 277, loss = 56.89352045\n",
            "Iteration 278, loss = 56.82786065\n",
            "Iteration 279, loss = 56.71666025\n",
            "Iteration 280, loss = 56.83865462\n",
            "Iteration 281, loss = 56.77285802\n",
            "Iteration 282, loss = 56.65473203\n",
            "Iteration 283, loss = 56.54384293\n",
            "Iteration 284, loss = 56.50273421\n",
            "Iteration 285, loss = 56.51028496\n",
            "Iteration 286, loss = 56.50052810\n",
            "Iteration 287, loss = 56.43698976\n",
            "Iteration 288, loss = 56.36973396\n",
            "Iteration 289, loss = 56.31033915\n",
            "Iteration 290, loss = 56.28841999\n",
            "Iteration 291, loss = 56.23979525\n",
            "Iteration 292, loss = 56.20171670\n",
            "Iteration 293, loss = 56.14612547\n",
            "Iteration 294, loss = 56.16712866\n",
            "Iteration 295, loss = 56.06801217\n",
            "Iteration 296, loss = 56.06434203\n",
            "Iteration 297, loss = 55.97176720\n",
            "Iteration 298, loss = 55.96883406\n",
            "Iteration 299, loss = 56.01098786\n",
            "Iteration 300, loss = 55.89729105\n",
            "Iteration 301, loss = 55.90781067\n",
            "Iteration 302, loss = 55.87924764\n",
            "Iteration 303, loss = 55.78271357\n",
            "Iteration 304, loss = 55.65518418\n",
            "Iteration 305, loss = 55.83355524\n",
            "Iteration 306, loss = 55.88149839\n",
            "Iteration 307, loss = 55.86684032\n",
            "Iteration 308, loss = 55.73325589\n",
            "Iteration 309, loss = 55.63144592\n",
            "Iteration 310, loss = 55.78913229\n",
            "Iteration 311, loss = 55.47939944\n",
            "Iteration 312, loss = 55.42962781\n",
            "Iteration 313, loss = 55.40341713\n",
            "Iteration 314, loss = 55.40032876\n",
            "Iteration 315, loss = 55.36423496\n",
            "Iteration 316, loss = 55.28243414\n",
            "Iteration 317, loss = 55.36702076\n",
            "Iteration 318, loss = 55.19838165\n",
            "Iteration 319, loss = 55.15780885\n",
            "Iteration 320, loss = 55.14807915\n",
            "Iteration 321, loss = 55.13379962\n",
            "Iteration 322, loss = 55.06951588\n",
            "Iteration 323, loss = 54.99294677\n",
            "Iteration 324, loss = 54.94837915\n",
            "Iteration 325, loss = 54.95928332\n",
            "Iteration 326, loss = 54.90032807\n",
            "Iteration 327, loss = 54.99546913\n",
            "Iteration 328, loss = 54.80540045\n",
            "Iteration 329, loss = 54.79911704\n",
            "Iteration 330, loss = 54.77546457\n",
            "Iteration 331, loss = 54.70707230\n",
            "Iteration 332, loss = 54.64896072\n",
            "Iteration 333, loss = 54.66895322\n",
            "Iteration 334, loss = 54.60557288\n",
            "Iteration 335, loss = 54.56042299\n",
            "Iteration 336, loss = 54.52573885\n",
            "Iteration 337, loss = 54.49946077\n",
            "Iteration 338, loss = 54.44825478\n",
            "Iteration 339, loss = 54.46485258\n",
            "Iteration 340, loss = 54.37326726\n",
            "Iteration 341, loss = 54.38227144\n",
            "Iteration 342, loss = 54.37373308\n",
            "Iteration 343, loss = 54.34908053\n",
            "Iteration 344, loss = 54.34218235\n",
            "Iteration 345, loss = 54.34208070\n",
            "Iteration 346, loss = 54.18411546\n",
            "Iteration 347, loss = 54.45771163\n",
            "Iteration 348, loss = 54.15378977\n",
            "Iteration 349, loss = 54.08953542\n",
            "Iteration 350, loss = 54.03326059\n",
            "Iteration 351, loss = 54.04187377\n",
            "Iteration 352, loss = 54.13141870\n",
            "Iteration 353, loss = 54.25194899\n",
            "Iteration 354, loss = 54.03498522\n",
            "Iteration 355, loss = 54.02029034\n",
            "Iteration 356, loss = 54.09109402\n",
            "Iteration 357, loss = 53.86581865\n",
            "Iteration 358, loss = 53.86723780\n",
            "Iteration 359, loss = 53.71840233\n",
            "Iteration 360, loss = 53.87738032\n",
            "Iteration 361, loss = 53.70475624\n",
            "Iteration 362, loss = 53.67919154\n",
            "Iteration 363, loss = 53.61925593\n",
            "Iteration 364, loss = 53.58852882\n",
            "Iteration 365, loss = 53.66575400\n",
            "Iteration 366, loss = 53.43896191\n",
            "Iteration 367, loss = 53.40576227\n",
            "Iteration 368, loss = 53.48363154\n",
            "Iteration 369, loss = 53.67010721\n",
            "Iteration 370, loss = 53.71469624\n",
            "Iteration 371, loss = 53.55154408\n",
            "Iteration 372, loss = 53.39323724\n",
            "Iteration 373, loss = 53.27664346\n",
            "Iteration 374, loss = 53.26818597\n",
            "Iteration 375, loss = 53.36149583\n",
            "Iteration 376, loss = 53.15165552\n",
            "Iteration 377, loss = 53.32461502\n",
            "Iteration 378, loss = 53.15503618\n",
            "Iteration 379, loss = 53.22220348\n",
            "Iteration 380, loss = 53.06098745\n",
            "Iteration 381, loss = 52.97847815\n",
            "Iteration 382, loss = 52.94223744\n",
            "Iteration 383, loss = 52.94506258\n",
            "Iteration 384, loss = 52.98539552\n",
            "Iteration 385, loss = 53.00158262\n",
            "Iteration 386, loss = 52.89741040\n",
            "Iteration 387, loss = 52.85846584\n",
            "Iteration 388, loss = 52.79023712\n",
            "Iteration 389, loss = 52.87584016\n",
            "Iteration 390, loss = 52.64419228\n",
            "Iteration 391, loss = 52.65593911\n",
            "Iteration 392, loss = 52.65111033\n",
            "Iteration 393, loss = 52.62171492\n",
            "Iteration 394, loss = 52.60657477\n",
            "Iteration 395, loss = 52.64951883\n",
            "Iteration 396, loss = 52.59142498\n",
            "Iteration 397, loss = 52.39774068\n",
            "Iteration 398, loss = 52.59690945\n",
            "Iteration 399, loss = 52.75667607\n",
            "Iteration 400, loss = 52.58392546\n",
            "Iteration 401, loss = 52.50127339\n",
            "Iteration 402, loss = 52.35805091\n",
            "Iteration 403, loss = 52.40414818\n",
            "Iteration 404, loss = 52.22624682\n",
            "Iteration 405, loss = 52.26484454\n",
            "Iteration 406, loss = 52.24178114\n",
            "Iteration 407, loss = 52.22639117\n",
            "Iteration 408, loss = 52.12622115\n",
            "Iteration 409, loss = 52.15674849\n",
            "Iteration 410, loss = 52.17027904\n",
            "Iteration 411, loss = 52.00671747\n",
            "Iteration 412, loss = 51.92001263\n",
            "Iteration 413, loss = 52.06982067\n",
            "Iteration 414, loss = 52.19445300\n",
            "Iteration 415, loss = 51.91821689\n",
            "Iteration 416, loss = 51.86605918\n",
            "Iteration 417, loss = 51.90113293\n",
            "Iteration 418, loss = 51.94110892\n",
            "Iteration 419, loss = 51.88527821\n",
            "Iteration 420, loss = 51.73497987\n",
            "Iteration 421, loss = 51.78402555\n",
            "Iteration 422, loss = 51.94587281\n",
            "Iteration 423, loss = 51.75198724\n",
            "Iteration 424, loss = 51.70415080\n",
            "Iteration 425, loss = 51.57403833\n",
            "Iteration 426, loss = 51.78284436\n",
            "Iteration 427, loss = 51.59410702\n",
            "Iteration 428, loss = 51.50365473\n",
            "Iteration 429, loss = 51.48673104\n",
            "Iteration 430, loss = 51.45153070\n",
            "Iteration 431, loss = 51.40419227\n",
            "Iteration 432, loss = 51.33002805\n",
            "Iteration 433, loss = 51.47566825\n",
            "Iteration 434, loss = 51.33604436\n",
            "Iteration 435, loss = 51.41565398\n",
            "Iteration 436, loss = 51.25302868\n",
            "Iteration 437, loss = 51.20279888\n",
            "Iteration 438, loss = 51.12332897\n",
            "Iteration 439, loss = 51.33859274\n",
            "Iteration 440, loss = 51.10581914\n",
            "Iteration 441, loss = 51.19721298\n",
            "Iteration 442, loss = 51.17212807\n",
            "Iteration 443, loss = 51.06807604\n",
            "Iteration 444, loss = 51.06912006\n",
            "Iteration 445, loss = 50.98261564\n",
            "Iteration 446, loss = 50.98212267\n",
            "Iteration 447, loss = 50.98933230\n",
            "Iteration 448, loss = 50.86310165\n",
            "Iteration 449, loss = 50.92902960\n",
            "Iteration 450, loss = 50.83350405\n",
            "Iteration 451, loss = 50.78104226\n",
            "Iteration 452, loss = 50.76253695\n",
            "Iteration 453, loss = 50.75589968\n",
            "Iteration 454, loss = 50.71191650\n",
            "Iteration 455, loss = 50.72570631\n",
            "Iteration 456, loss = 50.64171467\n",
            "Iteration 457, loss = 50.72177773\n",
            "Iteration 458, loss = 50.65675769\n",
            "Iteration 459, loss = 50.62197808\n",
            "Iteration 460, loss = 50.60119648\n",
            "Iteration 461, loss = 50.59736554\n",
            "Iteration 462, loss = 50.54868506\n",
            "Iteration 463, loss = 50.54488332\n",
            "Iteration 464, loss = 50.72791482\n",
            "Iteration 465, loss = 50.58620598\n",
            "Iteration 466, loss = 50.38730038\n",
            "Iteration 467, loss = 50.31622992\n",
            "Iteration 468, loss = 50.50480626\n",
            "Iteration 469, loss = 50.58376877\n",
            "Iteration 470, loss = 50.39363063\n",
            "Iteration 471, loss = 50.47066049\n",
            "Iteration 472, loss = 50.39660877\n",
            "Iteration 473, loss = 50.26925059\n",
            "Iteration 474, loss = 50.16689312\n",
            "Iteration 475, loss = 50.16723633\n",
            "Iteration 476, loss = 50.12360717\n",
            "Iteration 477, loss = 50.29670727\n",
            "Iteration 478, loss = 50.16851698\n",
            "Iteration 479, loss = 50.07809256\n",
            "Iteration 480, loss = 49.99852065\n",
            "Iteration 481, loss = 50.07277052\n",
            "Iteration 482, loss = 50.08219078\n",
            "Iteration 483, loss = 50.13893932\n",
            "Iteration 484, loss = 49.93112991\n",
            "Iteration 485, loss = 49.81099377\n",
            "Iteration 486, loss = 49.88010734\n",
            "Iteration 487, loss = 50.05096505\n",
            "Iteration 488, loss = 50.20306863\n",
            "Iteration 489, loss = 50.02502031\n",
            "Iteration 490, loss = 49.72083078\n",
            "Iteration 491, loss = 49.84255444\n",
            "Iteration 492, loss = 49.98948394\n",
            "Iteration 493, loss = 50.13345082\n",
            "Iteration 494, loss = 49.70307026\n",
            "Iteration 495, loss = 49.60418412\n",
            "Iteration 496, loss = 49.60082540\n",
            "Iteration 497, loss = 49.72370889\n",
            "Iteration 498, loss = 49.73478537\n",
            "Iteration 499, loss = 49.53041257\n",
            "Iteration 500, loss = 49.43229768\n",
            "Iteration 501, loss = 49.66638129\n",
            "Iteration 502, loss = 49.76359748\n",
            "Iteration 503, loss = 49.92802900\n",
            "Iteration 504, loss = 49.45960168\n",
            "Iteration 505, loss = 49.39372859\n",
            "Iteration 506, loss = 49.36188099\n",
            "Iteration 507, loss = 49.32446225\n",
            "Iteration 508, loss = 49.23842035\n",
            "Iteration 509, loss = 49.29401663\n",
            "Iteration 510, loss = 49.24645635\n",
            "Iteration 511, loss = 49.18557960\n",
            "Iteration 512, loss = 49.12496739\n",
            "Iteration 513, loss = 49.11132004\n",
            "Iteration 514, loss = 49.11004269\n",
            "Iteration 515, loss = 49.35956297\n",
            "Iteration 516, loss = 49.04684644\n",
            "Iteration 517, loss = 48.98676127\n",
            "Iteration 518, loss = 49.03241653\n",
            "Iteration 519, loss = 49.05289914\n",
            "Iteration 520, loss = 49.16787708\n",
            "Iteration 521, loss = 49.01406123\n",
            "Iteration 522, loss = 49.00903173\n",
            "Iteration 523, loss = 48.95890133\n",
            "Iteration 524, loss = 48.93325110\n",
            "Iteration 525, loss = 49.01851987\n",
            "Iteration 526, loss = 48.85681319\n",
            "Iteration 527, loss = 48.78360725\n",
            "Iteration 528, loss = 49.08215904\n",
            "Iteration 529, loss = 49.15350438\n",
            "Iteration 530, loss = 48.76868060\n",
            "Iteration 531, loss = 48.99987240\n",
            "Iteration 532, loss = 48.87664111\n",
            "Iteration 533, loss = 48.83275689\n",
            "Iteration 534, loss = 48.58646262\n",
            "Iteration 535, loss = 48.59595652\n",
            "Iteration 536, loss = 48.71317041\n",
            "Iteration 537, loss = 48.76609431\n",
            "Iteration 538, loss = 48.70336091\n",
            "Iteration 539, loss = 48.46921789\n",
            "Iteration 540, loss = 48.69601692\n",
            "Iteration 541, loss = 48.47458689\n",
            "Iteration 542, loss = 48.41139047\n",
            "Iteration 543, loss = 48.48848266\n",
            "Iteration 544, loss = 48.40490121\n",
            "Iteration 545, loss = 48.43718841\n",
            "Iteration 546, loss = 48.29725772\n",
            "Iteration 547, loss = 48.51711746\n",
            "Iteration 548, loss = 48.32590173\n",
            "Iteration 549, loss = 48.22740442\n",
            "Iteration 550, loss = 48.30172083\n",
            "Iteration 551, loss = 48.29484986\n",
            "Iteration 552, loss = 48.15860158\n",
            "Iteration 553, loss = 48.14856278\n",
            "Iteration 554, loss = 48.07416752\n",
            "Iteration 555, loss = 48.19422900\n",
            "Iteration 556, loss = 48.04748129\n",
            "Iteration 557, loss = 48.04441375\n",
            "Iteration 558, loss = 48.00212795\n",
            "Iteration 559, loss = 47.99209368\n",
            "Iteration 560, loss = 47.94594896\n",
            "Iteration 561, loss = 47.92487840\n",
            "Iteration 562, loss = 48.17457084\n",
            "Iteration 563, loss = 47.91676983\n",
            "Iteration 564, loss = 47.79847612\n",
            "Iteration 565, loss = 48.00008180\n",
            "Iteration 566, loss = 48.48029491\n",
            "Iteration 567, loss = 48.42559881\n",
            "Iteration 568, loss = 47.84262918\n",
            "Iteration 569, loss = 47.85449677\n",
            "Iteration 570, loss = 48.22782740\n",
            "Iteration 571, loss = 48.24017712\n",
            "Iteration 572, loss = 47.70981725\n",
            "Iteration 573, loss = 47.46605671\n",
            "Iteration 574, loss = 47.99413239\n",
            "Iteration 575, loss = 48.37870973\n",
            "Iteration 576, loss = 48.02794063\n",
            "Iteration 577, loss = 47.58449464\n",
            "Iteration 578, loss = 48.14839470\n",
            "Iteration 579, loss = 47.98746956\n",
            "Iteration 580, loss = 47.68114640\n",
            "Iteration 581, loss = 47.47928491\n",
            "Iteration 582, loss = 47.45180281\n",
            "Iteration 583, loss = 47.43279642\n",
            "Iteration 584, loss = 47.51344468\n",
            "Iteration 585, loss = 47.35765100\n",
            "Iteration 586, loss = 47.43108704\n",
            "Iteration 587, loss = 47.57538931\n",
            "Iteration 588, loss = 47.29770000\n",
            "Iteration 589, loss = 47.34493779\n",
            "Iteration 590, loss = 47.22706505\n",
            "Iteration 591, loss = 47.35826009\n",
            "Iteration 592, loss = 47.20482369\n",
            "Iteration 593, loss = 47.13913733\n",
            "Iteration 594, loss = 47.33430237\n",
            "Iteration 595, loss = 47.24585095\n",
            "Iteration 596, loss = 47.10489898\n",
            "Iteration 597, loss = 47.24447814\n",
            "Iteration 598, loss = 47.13262653\n",
            "Iteration 599, loss = 47.12204690\n",
            "Iteration 600, loss = 46.97934811\n",
            "Iteration 601, loss = 46.95423884\n",
            "Iteration 602, loss = 46.94309032\n",
            "Iteration 603, loss = 46.92091725\n",
            "Iteration 604, loss = 46.88548027\n",
            "Iteration 605, loss = 46.88097341\n",
            "Iteration 606, loss = 47.09659136\n",
            "Iteration 607, loss = 46.80484812\n",
            "Iteration 608, loss = 46.80167985\n",
            "Iteration 609, loss = 46.89195812\n",
            "Iteration 610, loss = 46.92534556\n",
            "Iteration 611, loss = 46.92893504\n",
            "Iteration 612, loss = 46.92376735\n",
            "Iteration 613, loss = 46.73293074\n",
            "Iteration 614, loss = 46.68116787\n",
            "Iteration 615, loss = 46.66810220\n",
            "Iteration 616, loss = 46.99942369\n",
            "Iteration 617, loss = 46.72451009\n",
            "Iteration 618, loss = 46.79512222\n",
            "Iteration 619, loss = 46.68872642\n",
            "Iteration 620, loss = 46.64710352\n",
            "Iteration 621, loss = 46.51613829\n",
            "Iteration 622, loss = 46.70541777\n",
            "Iteration 623, loss = 46.59752843\n",
            "Iteration 624, loss = 46.57809361\n",
            "Iteration 625, loss = 46.43712576\n",
            "Iteration 626, loss = 46.43286379\n",
            "Iteration 627, loss = 46.55968421\n",
            "Iteration 628, loss = 46.61368861\n",
            "Iteration 629, loss = 46.38938788\n",
            "Iteration 630, loss = 46.56856757\n",
            "Iteration 631, loss = 46.45199442\n",
            "Iteration 632, loss = 46.35803854\n",
            "Iteration 633, loss = 46.28247115\n",
            "Iteration 634, loss = 46.24558374\n",
            "Iteration 635, loss = 46.24085844\n",
            "Iteration 636, loss = 46.18833836\n",
            "Iteration 637, loss = 46.12166861\n",
            "Iteration 638, loss = 46.13633952\n",
            "Iteration 639, loss = 46.17286263\n",
            "Iteration 640, loss = 46.15194764\n",
            "Iteration 641, loss = 46.18251739\n",
            "Iteration 642, loss = 46.01533602\n",
            "Iteration 643, loss = 46.03249148\n",
            "Iteration 644, loss = 46.02367163\n",
            "Iteration 645, loss = 46.13261473\n",
            "Iteration 646, loss = 46.19273129\n",
            "Iteration 647, loss = 45.93939686\n",
            "Iteration 648, loss = 45.89590277\n",
            "Iteration 649, loss = 46.07935656\n",
            "Iteration 650, loss = 45.93249933\n",
            "Iteration 651, loss = 46.11718160\n",
            "Iteration 652, loss = 45.91990553\n",
            "Iteration 653, loss = 46.20456378\n",
            "Iteration 654, loss = 45.74342380\n",
            "Iteration 655, loss = 45.80494086\n",
            "Iteration 656, loss = 46.15476657\n",
            "Iteration 657, loss = 46.08122010\n",
            "Iteration 658, loss = 45.88412000\n",
            "Iteration 659, loss = 45.69516530\n",
            "Iteration 660, loss = 45.73065262\n",
            "Iteration 661, loss = 45.68900327\n",
            "Iteration 662, loss = 45.66901778\n",
            "Iteration 663, loss = 45.59923634\n",
            "Iteration 664, loss = 45.60832152\n",
            "Iteration 665, loss = 45.60166622\n",
            "Iteration 666, loss = 45.55959028\n",
            "Iteration 667, loss = 45.62569665\n",
            "Iteration 668, loss = 45.89676591\n",
            "Iteration 669, loss = 45.59158335\n",
            "Iteration 670, loss = 45.42711196\n",
            "Iteration 671, loss = 45.59899445\n",
            "Iteration 672, loss = 45.42002673\n",
            "Iteration 673, loss = 45.46066452\n",
            "Iteration 674, loss = 45.55758388\n",
            "Iteration 675, loss = 45.42824077\n",
            "Iteration 676, loss = 45.31799593\n",
            "Iteration 677, loss = 45.28210397\n",
            "Iteration 678, loss = 45.33367138\n",
            "Iteration 679, loss = 45.57608924\n",
            "Iteration 680, loss = 45.43076938\n",
            "Iteration 681, loss = 45.34888413\n",
            "Iteration 682, loss = 45.68494581\n",
            "Iteration 683, loss = 45.19319061\n",
            "Iteration 684, loss = 45.42760537\n",
            "Iteration 685, loss = 45.33362416\n",
            "Iteration 686, loss = 45.30508561\n",
            "Iteration 687, loss = 45.09000372\n",
            "Iteration 688, loss = 45.10355363\n",
            "Iteration 689, loss = 45.18946734\n",
            "Iteration 690, loss = 45.11947594\n",
            "Iteration 691, loss = 45.33102083\n",
            "Iteration 692, loss = 45.07764700\n",
            "Iteration 693, loss = 44.98059070\n",
            "Iteration 694, loss = 45.09979924\n",
            "Iteration 695, loss = 45.05199641\n",
            "Iteration 696, loss = 45.09528802\n",
            "Iteration 697, loss = 44.93026267\n",
            "Iteration 698, loss = 44.85096427\n",
            "Iteration 699, loss = 44.88435816\n",
            "Iteration 700, loss = 44.90240204\n",
            "Iteration 701, loss = 44.98371256\n",
            "Iteration 702, loss = 44.93042397\n",
            "Iteration 703, loss = 44.84376023\n",
            "Iteration 704, loss = 45.08860419\n",
            "Iteration 705, loss = 45.04036040\n",
            "Iteration 706, loss = 44.81953690\n",
            "Iteration 707, loss = 44.83409089\n",
            "Iteration 708, loss = 44.80186375\n",
            "Iteration 709, loss = 44.76764088\n",
            "Iteration 710, loss = 44.80551612\n",
            "Iteration 711, loss = 44.70540408\n",
            "Iteration 712, loss = 44.59664156\n",
            "Iteration 713, loss = 44.57064933\n",
            "Iteration 714, loss = 44.69326719\n",
            "Iteration 715, loss = 44.62602841\n",
            "Iteration 716, loss = 44.54586427\n",
            "Iteration 717, loss = 44.54994227\n",
            "Iteration 718, loss = 44.68017847\n",
            "Iteration 719, loss = 44.55870719\n",
            "Iteration 720, loss = 44.43798395\n",
            "Iteration 721, loss = 44.68657017\n",
            "Iteration 722, loss = 44.56345227\n",
            "Iteration 723, loss = 44.42825994\n",
            "Iteration 724, loss = 44.35953036\n",
            "Iteration 725, loss = 44.39126345\n",
            "Iteration 726, loss = 44.49998563\n",
            "Iteration 727, loss = 44.43356787\n",
            "Iteration 728, loss = 44.31697150\n",
            "Iteration 729, loss = 44.28214265\n",
            "Iteration 730, loss = 44.45465678\n",
            "Iteration 731, loss = 44.30787868\n",
            "Iteration 732, loss = 44.25624702\n",
            "Iteration 733, loss = 44.31679173\n",
            "Iteration 734, loss = 44.41807218\n",
            "Iteration 735, loss = 44.25554391\n",
            "Iteration 736, loss = 44.24210854\n",
            "Iteration 737, loss = 44.20215873\n",
            "Iteration 738, loss = 44.33606554\n",
            "Iteration 739, loss = 44.42133574\n",
            "Iteration 740, loss = 44.25337218\n",
            "Iteration 741, loss = 44.04008193\n",
            "Iteration 742, loss = 44.78155067\n",
            "Iteration 743, loss = 44.41952711\n",
            "Iteration 744, loss = 44.23464789\n",
            "Iteration 745, loss = 44.07059762\n",
            "Iteration 746, loss = 44.19002590\n",
            "Iteration 747, loss = 44.05062814\n",
            "Iteration 748, loss = 43.89936758\n",
            "Iteration 749, loss = 44.11151819\n",
            "Iteration 750, loss = 44.37298076\n",
            "Iteration 751, loss = 44.35177623\n",
            "Iteration 752, loss = 44.06420296\n",
            "Iteration 753, loss = 44.27001404\n",
            "Iteration 754, loss = 44.65434071\n",
            "Iteration 755, loss = 44.06156781\n",
            "Iteration 756, loss = 44.15897916\n",
            "Iteration 757, loss = 44.26606979\n",
            "Iteration 758, loss = 44.14473302\n",
            "Iteration 759, loss = 43.93519596\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 99.01069369\n",
            "Iteration 2, loss = 73.71141505\n",
            "Iteration 3, loss = 70.86178063\n",
            "Iteration 4, loss = 68.28547412\n",
            "Iteration 5, loss = 65.61959271\n",
            "Iteration 6, loss = 63.21761429\n",
            "Iteration 7, loss = 61.14256727\n",
            "Iteration 8, loss = 59.27407805\n",
            "Iteration 9, loss = 58.09947771\n",
            "Iteration 10, loss = 57.05236085\n",
            "Iteration 11, loss = 56.44022338\n",
            "Iteration 12, loss = 56.02927777\n",
            "Iteration 13, loss = 55.64109836\n",
            "Iteration 14, loss = 55.45416143\n",
            "Iteration 15, loss = 55.34718122\n",
            "Iteration 16, loss = 55.12527849\n",
            "Iteration 17, loss = 54.79116078\n",
            "Iteration 18, loss = 54.62580020\n",
            "Iteration 19, loss = 54.56571435\n",
            "Iteration 20, loss = 54.49882143\n",
            "Iteration 21, loss = 54.20391590\n",
            "Iteration 22, loss = 54.12582521\n",
            "Iteration 23, loss = 54.15638365\n",
            "Iteration 24, loss = 54.06268325\n",
            "Iteration 25, loss = 53.90812761\n",
            "Iteration 26, loss = 53.66223635\n",
            "Iteration 27, loss = 53.65952198\n",
            "Iteration 28, loss = 53.66281267\n",
            "Iteration 29, loss = 53.67606813\n",
            "Iteration 30, loss = 53.46342592\n",
            "Iteration 31, loss = 53.50831150\n",
            "Iteration 32, loss = 53.63751599\n",
            "Iteration 33, loss = 53.38859956\n",
            "Iteration 34, loss = 53.60476255\n",
            "Iteration 35, loss = 53.31719799\n",
            "Iteration 36, loss = 53.22415155\n",
            "Iteration 37, loss = 53.36332916\n",
            "Iteration 38, loss = 53.31133415\n",
            "Iteration 39, loss = 53.45345456\n",
            "Iteration 40, loss = 53.16925397\n",
            "Iteration 41, loss = 53.18575915\n",
            "Iteration 42, loss = 53.49457705\n",
            "Iteration 43, loss = 53.26926487\n",
            "Iteration 44, loss = 53.12156723\n",
            "Iteration 45, loss = 53.07649859\n",
            "Iteration 46, loss = 53.14568524\n",
            "Iteration 47, loss = 53.02778475\n",
            "Iteration 48, loss = 53.09440023\n",
            "Iteration 49, loss = 53.35736541\n",
            "Iteration 50, loss = 52.91561258\n",
            "Iteration 51, loss = 53.08507017\n",
            "Iteration 52, loss = 53.02500899\n",
            "Iteration 53, loss = 53.07691052\n",
            "Iteration 54, loss = 52.98555416\n",
            "Iteration 55, loss = 52.91419318\n",
            "Iteration 56, loss = 53.25081736\n",
            "Iteration 57, loss = 53.09424013\n",
            "Iteration 58, loss = 53.30108149\n",
            "Iteration 59, loss = 53.03898854\n",
            "Iteration 60, loss = 53.01412155\n",
            "Iteration 61, loss = 52.90098732\n",
            "Iteration 62, loss = 52.94875823\n",
            "Iteration 63, loss = 53.04961141\n",
            "Iteration 64, loss = 52.79415991\n",
            "Iteration 65, loss = 53.24689371\n",
            "Iteration 66, loss = 52.81217523\n",
            "Iteration 67, loss = 52.97062489\n",
            "Iteration 68, loss = 52.81927808\n",
            "Iteration 69, loss = 52.91678025\n",
            "Iteration 70, loss = 52.83687073\n",
            "Iteration 71, loss = 52.71926539\n",
            "Iteration 72, loss = 52.74930435\n",
            "Iteration 73, loss = 52.80679014\n",
            "Iteration 74, loss = 52.75576375\n",
            "Iteration 75, loss = 52.96559238\n",
            "Iteration 76, loss = 53.13925695\n",
            "Iteration 77, loss = 53.02654073\n",
            "Iteration 78, loss = 52.74589817\n",
            "Iteration 79, loss = 52.78420810\n",
            "Iteration 80, loss = 52.68462370\n",
            "Iteration 81, loss = 52.78071655\n",
            "Iteration 82, loss = 52.91313925\n",
            "Iteration 83, loss = 52.73131328\n",
            "Iteration 84, loss = 52.64064893\n",
            "Iteration 85, loss = 52.78431739\n",
            "Iteration 86, loss = 53.08225866\n",
            "Iteration 87, loss = 52.63010170\n",
            "Iteration 88, loss = 52.77417009\n",
            "Iteration 89, loss = 52.77112837\n",
            "Iteration 90, loss = 52.85195421\n",
            "Iteration 91, loss = 52.75056445\n",
            "Iteration 92, loss = 52.62538539\n",
            "Iteration 93, loss = 52.60810648\n",
            "Iteration 94, loss = 52.71083631\n",
            "Iteration 95, loss = 52.72949508\n",
            "Iteration 96, loss = 52.56895101\n",
            "Iteration 97, loss = 52.68889121\n",
            "Iteration 98, loss = 52.47327926\n",
            "Iteration 99, loss = 52.76481801\n",
            "Iteration 100, loss = 52.62039370\n",
            "Iteration 101, loss = 52.62796251\n",
            "Iteration 102, loss = 52.45813682\n",
            "Iteration 103, loss = 52.48771458\n",
            "Iteration 104, loss = 52.62561322\n",
            "Iteration 105, loss = 52.46309331\n",
            "Iteration 106, loss = 52.70338134\n",
            "Iteration 107, loss = 52.45224486\n",
            "Iteration 108, loss = 52.65987827\n",
            "Iteration 109, loss = 52.63414344\n",
            "Iteration 110, loss = 52.66541641\n",
            "Iteration 111, loss = 52.49754414\n",
            "Iteration 112, loss = 52.42343267\n",
            "Iteration 113, loss = 52.45652713\n",
            "Iteration 114, loss = 52.50485007\n",
            "Iteration 115, loss = 52.53090076\n",
            "Iteration 116, loss = 52.41168090\n",
            "Iteration 117, loss = 52.31558421\n",
            "Iteration 118, loss = 52.47407732\n",
            "Iteration 119, loss = 52.48864844\n",
            "Iteration 120, loss = 52.76583180\n",
            "Iteration 121, loss = 52.47044698\n",
            "Iteration 122, loss = 52.33378017\n",
            "Iteration 123, loss = 52.30094913\n",
            "Iteration 124, loss = 52.49851085\n",
            "Iteration 125, loss = 52.39901359\n",
            "Iteration 126, loss = 52.22840089\n",
            "Iteration 127, loss = 52.39218692\n",
            "Iteration 128, loss = 52.43137212\n",
            "Iteration 129, loss = 52.35877359\n",
            "Iteration 130, loss = 52.40798823\n",
            "Iteration 131, loss = 52.34084681\n",
            "Iteration 132, loss = 52.27662718\n",
            "Iteration 133, loss = 52.52046265\n",
            "Iteration 134, loss = 52.30909623\n",
            "Iteration 135, loss = 52.28427597\n",
            "Iteration 136, loss = 52.42385146\n",
            "Iteration 137, loss = 52.32426535\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 492.85788732\n",
            "Iteration 2, loss = 437.85202390\n",
            "Iteration 3, loss = 386.29484985\n",
            "Iteration 4, loss = 340.24255124\n",
            "Iteration 5, loss = 297.77343640\n",
            "Iteration 6, loss = 260.91575741\n",
            "Iteration 7, loss = 227.33439291\n",
            "Iteration 8, loss = 198.69362448\n",
            "Iteration 9, loss = 173.71780066\n",
            "Iteration 10, loss = 153.41789892\n",
            "Iteration 11, loss = 134.54478231\n",
            "Iteration 12, loss = 120.79212604\n",
            "Iteration 13, loss = 109.65833743\n",
            "Iteration 14, loss = 100.98367257\n",
            "Iteration 15, loss = 94.43839148\n",
            "Iteration 16, loss = 89.58663141\n",
            "Iteration 17, loss = 86.24963236\n",
            "Iteration 18, loss = 84.13620291\n",
            "Iteration 19, loss = 82.09303844\n",
            "Iteration 20, loss = 81.01333880\n",
            "Iteration 21, loss = 80.13949118\n",
            "Iteration 22, loss = 79.20215831\n",
            "Iteration 23, loss = 78.12036556\n",
            "Iteration 24, loss = 77.01103663\n",
            "Iteration 25, loss = 75.81561104\n",
            "Iteration 26, loss = 74.54586017\n",
            "Iteration 27, loss = 73.01630703\n",
            "Iteration 28, loss = 71.53358770\n",
            "Iteration 29, loss = 69.95825464\n",
            "Iteration 30, loss = 68.46692166\n",
            "Iteration 31, loss = 67.00800388\n",
            "Iteration 32, loss = 65.52843966\n",
            "Iteration 33, loss = 64.26165893\n",
            "Iteration 34, loss = 63.13112715\n",
            "Iteration 35, loss = 61.92724184\n",
            "Iteration 36, loss = 61.06078185\n",
            "Iteration 37, loss = 60.11929575\n",
            "Iteration 38, loss = 59.16681650\n",
            "Iteration 39, loss = 58.32995728\n",
            "Iteration 40, loss = 57.62523696\n",
            "Iteration 41, loss = 56.86968414\n",
            "Iteration 42, loss = 56.08238249\n",
            "Iteration 43, loss = 55.35510480\n",
            "Iteration 44, loss = 54.72015518\n",
            "Iteration 45, loss = 54.08619794\n",
            "Iteration 46, loss = 53.41757175\n",
            "Iteration 47, loss = 52.82789801\n",
            "Iteration 48, loss = 52.22962352\n",
            "Iteration 49, loss = 51.69785219\n",
            "Iteration 50, loss = 51.17010371\n",
            "Iteration 51, loss = 50.76014927\n",
            "Iteration 52, loss = 50.25931950\n",
            "Iteration 53, loss = 49.73206690\n",
            "Iteration 54, loss = 49.37323045\n",
            "Iteration 55, loss = 49.00638265\n",
            "Iteration 56, loss = 48.60777989\n",
            "Iteration 57, loss = 48.25851465\n",
            "Iteration 58, loss = 47.94274992\n",
            "Iteration 59, loss = 47.65405901\n",
            "Iteration 60, loss = 47.33072328\n",
            "Iteration 61, loss = 47.09863964\n",
            "Iteration 62, loss = 46.81998342\n",
            "Iteration 63, loss = 46.61904364\n",
            "Iteration 64, loss = 46.45690363\n",
            "Iteration 65, loss = 46.18375588\n",
            "Iteration 66, loss = 46.00031637\n",
            "Iteration 67, loss = 45.77574264\n",
            "Iteration 68, loss = 45.57719827\n",
            "Iteration 69, loss = 45.43844828\n",
            "Iteration 70, loss = 45.25871006\n",
            "Iteration 71, loss = 45.15713959\n",
            "Iteration 72, loss = 45.03661163\n",
            "Iteration 73, loss = 44.91617900\n",
            "Iteration 74, loss = 44.78456370\n",
            "Iteration 75, loss = 44.59853028\n",
            "Iteration 76, loss = 44.48930540\n",
            "Iteration 77, loss = 44.36068056\n",
            "Iteration 78, loss = 44.28510451\n",
            "Iteration 79, loss = 44.15159790\n",
            "Iteration 80, loss = 44.07890794\n",
            "Iteration 81, loss = 43.95262087\n",
            "Iteration 82, loss = 43.84682002\n",
            "Iteration 83, loss = 43.74598761\n",
            "Iteration 84, loss = 43.64611625\n",
            "Iteration 85, loss = 43.57055732\n",
            "Iteration 86, loss = 43.44745614\n",
            "Iteration 87, loss = 43.33874707\n",
            "Iteration 88, loss = 43.26726270\n",
            "Iteration 89, loss = 43.14868737\n",
            "Iteration 90, loss = 43.05733874\n",
            "Iteration 91, loss = 42.95522630\n",
            "Iteration 92, loss = 42.88362596\n",
            "Iteration 93, loss = 42.79169099\n",
            "Iteration 94, loss = 42.68999661\n",
            "Iteration 95, loss = 42.58760013\n",
            "Iteration 96, loss = 42.43409264\n",
            "Iteration 97, loss = 42.34765165\n",
            "Iteration 98, loss = 42.33141250\n",
            "Iteration 99, loss = 42.23726553\n",
            "Iteration 100, loss = 42.11645094\n",
            "Iteration 101, loss = 42.01832886\n",
            "Iteration 102, loss = 41.99510877\n",
            "Iteration 103, loss = 41.84815759\n",
            "Iteration 104, loss = 41.76608624\n",
            "Iteration 105, loss = 41.69871181\n",
            "Iteration 106, loss = 41.60613141\n",
            "Iteration 107, loss = 41.52925010\n",
            "Iteration 108, loss = 41.47497851\n",
            "Iteration 109, loss = 41.39222333\n",
            "Iteration 110, loss = 41.32292859\n",
            "Iteration 111, loss = 41.26021879\n",
            "Iteration 112, loss = 41.18445744\n",
            "Iteration 113, loss = 41.11092447\n",
            "Iteration 114, loss = 41.03812191\n",
            "Iteration 115, loss = 40.97971314\n",
            "Iteration 116, loss = 40.91555832\n",
            "Iteration 117, loss = 40.88734673\n",
            "Iteration 118, loss = 40.86741313\n",
            "Iteration 119, loss = 40.76848033\n",
            "Iteration 120, loss = 40.72943075\n",
            "Iteration 121, loss = 40.65939408\n",
            "Iteration 122, loss = 40.57117349\n",
            "Iteration 123, loss = 40.56561986\n",
            "Iteration 124, loss = 40.43878798\n",
            "Iteration 125, loss = 40.38501983\n",
            "Iteration 126, loss = 40.34348663\n",
            "Iteration 127, loss = 40.25730348\n",
            "Iteration 128, loss = 40.19719550\n",
            "Iteration 129, loss = 40.14224628\n",
            "Iteration 130, loss = 40.12594166\n",
            "Iteration 131, loss = 40.04447431\n",
            "Iteration 132, loss = 39.97001898\n",
            "Iteration 133, loss = 39.92654013\n",
            "Iteration 134, loss = 39.85552886\n",
            "Iteration 135, loss = 39.84148251\n",
            "Iteration 136, loss = 39.78221252\n",
            "Iteration 137, loss = 39.73595330\n",
            "Iteration 138, loss = 39.65108619\n",
            "Iteration 139, loss = 39.59557130\n",
            "Iteration 140, loss = 39.55093657\n",
            "Iteration 141, loss = 39.51232990\n",
            "Iteration 142, loss = 39.49448229\n",
            "Iteration 143, loss = 39.48295564\n",
            "Iteration 144, loss = 39.44351853\n",
            "Iteration 145, loss = 39.33234983\n",
            "Iteration 146, loss = 39.24735426\n",
            "Iteration 147, loss = 39.20283920\n",
            "Iteration 148, loss = 39.16355528\n",
            "Iteration 149, loss = 39.18484194\n",
            "Iteration 150, loss = 39.03864320\n",
            "Iteration 151, loss = 39.05733838\n",
            "Iteration 152, loss = 38.96600679\n",
            "Iteration 153, loss = 38.89749680\n",
            "Iteration 154, loss = 38.85495207\n",
            "Iteration 155, loss = 38.80682887\n",
            "Iteration 156, loss = 38.75714517\n",
            "Iteration 157, loss = 38.70778577\n",
            "Iteration 158, loss = 38.66993770\n",
            "Iteration 159, loss = 38.68700407\n",
            "Iteration 160, loss = 38.68107694\n",
            "Iteration 161, loss = 38.54186160\n",
            "Iteration 162, loss = 38.50214299\n",
            "Iteration 163, loss = 38.44134664\n",
            "Iteration 164, loss = 38.39564289\n",
            "Iteration 165, loss = 38.34618068\n",
            "Iteration 166, loss = 38.31105182\n",
            "Iteration 167, loss = 38.28065378\n",
            "Iteration 168, loss = 38.25580235\n",
            "Iteration 169, loss = 38.22199307\n",
            "Iteration 170, loss = 38.19854214\n",
            "Iteration 171, loss = 38.12865976\n",
            "Iteration 172, loss = 38.07688480\n",
            "Iteration 173, loss = 38.04210255\n",
            "Iteration 174, loss = 38.03496445\n",
            "Iteration 175, loss = 37.98085613\n",
            "Iteration 176, loss = 37.93495754\n",
            "Iteration 177, loss = 37.88854073\n",
            "Iteration 178, loss = 37.84729799\n",
            "Iteration 179, loss = 37.81435811\n",
            "Iteration 180, loss = 37.81618940\n",
            "Iteration 181, loss = 37.75505212\n",
            "Iteration 182, loss = 37.71242749\n",
            "Iteration 183, loss = 37.72821372\n",
            "Iteration 184, loss = 37.62622846\n",
            "Iteration 185, loss = 37.56242932\n",
            "Iteration 186, loss = 37.52272260\n",
            "Iteration 187, loss = 37.48641766\n",
            "Iteration 188, loss = 37.45566602\n",
            "Iteration 189, loss = 37.39537983\n",
            "Iteration 190, loss = 37.42106886\n",
            "Iteration 191, loss = 37.33513508\n",
            "Iteration 192, loss = 37.28216265\n",
            "Iteration 193, loss = 37.23632562\n",
            "Iteration 194, loss = 37.21100676\n",
            "Iteration 195, loss = 37.22772160\n",
            "Iteration 196, loss = 37.13613029\n",
            "Iteration 197, loss = 37.09265932\n",
            "Iteration 198, loss = 37.04074800\n",
            "Iteration 199, loss = 37.09455652\n",
            "Iteration 200, loss = 36.99242343\n",
            "Iteration 201, loss = 36.94411934\n",
            "Iteration 202, loss = 36.91616272\n",
            "Iteration 203, loss = 36.89134622\n",
            "Iteration 204, loss = 36.82643889\n",
            "Iteration 205, loss = 36.79077912\n",
            "Iteration 206, loss = 36.73677190\n",
            "Iteration 207, loss = 36.71310071\n",
            "Iteration 208, loss = 36.69871060\n",
            "Iteration 209, loss = 36.63133782\n",
            "Iteration 210, loss = 36.60464673\n",
            "Iteration 211, loss = 36.57173295\n",
            "Iteration 212, loss = 36.51808637\n",
            "Iteration 213, loss = 36.47087549\n",
            "Iteration 214, loss = 36.47654046\n",
            "Iteration 215, loss = 36.41190049\n",
            "Iteration 216, loss = 36.38611386\n",
            "Iteration 217, loss = 36.32558225\n",
            "Iteration 218, loss = 36.30022554\n",
            "Iteration 219, loss = 36.25296575\n",
            "Iteration 220, loss = 36.23718503\n",
            "Iteration 221, loss = 36.23348698\n",
            "Iteration 222, loss = 36.18801164\n",
            "Iteration 223, loss = 36.17623195\n",
            "Iteration 224, loss = 36.20715018\n",
            "Iteration 225, loss = 36.05409462\n",
            "Iteration 226, loss = 36.02118389\n",
            "Iteration 227, loss = 35.98444072\n",
            "Iteration 228, loss = 35.94196234\n",
            "Iteration 229, loss = 35.88966331\n",
            "Iteration 230, loss = 35.84061420\n",
            "Iteration 231, loss = 35.90895364\n",
            "Iteration 232, loss = 35.77793806\n",
            "Iteration 233, loss = 35.75522731\n",
            "Iteration 234, loss = 35.70636401\n",
            "Iteration 235, loss = 35.67861064\n",
            "Iteration 236, loss = 35.62451892\n",
            "Iteration 237, loss = 35.60755637\n",
            "Iteration 238, loss = 35.57745208\n",
            "Iteration 239, loss = 35.55346810\n",
            "Iteration 240, loss = 35.49867361\n",
            "Iteration 241, loss = 35.47530872\n",
            "Iteration 242, loss = 35.43255307\n",
            "Iteration 243, loss = 35.40755327\n",
            "Iteration 244, loss = 35.35161769\n",
            "Iteration 245, loss = 35.32614378\n",
            "Iteration 246, loss = 35.29267018\n",
            "Iteration 247, loss = 35.25617802\n",
            "Iteration 248, loss = 35.30797680\n",
            "Iteration 249, loss = 35.20129482\n",
            "Iteration 250, loss = 35.15509126\n",
            "Iteration 251, loss = 35.11106924\n",
            "Iteration 252, loss = 35.23426031\n",
            "Iteration 253, loss = 35.05017340\n",
            "Iteration 254, loss = 35.01031423\n",
            "Iteration 255, loss = 34.97075328\n",
            "Iteration 256, loss = 34.97734144\n",
            "Iteration 257, loss = 35.00216521\n",
            "Iteration 258, loss = 34.98714755\n",
            "Iteration 259, loss = 34.85585361\n",
            "Iteration 260, loss = 34.80005883\n",
            "Iteration 261, loss = 34.84820645\n",
            "Iteration 262, loss = 34.83345414\n",
            "Iteration 263, loss = 34.76880161\n",
            "Iteration 264, loss = 34.68440866\n",
            "Iteration 265, loss = 34.63354987\n",
            "Iteration 266, loss = 34.61530817\n",
            "Iteration 267, loss = 34.64508583\n",
            "Iteration 268, loss = 34.68990810\n",
            "Iteration 269, loss = 34.71153495\n",
            "Iteration 270, loss = 34.73465298\n",
            "Iteration 271, loss = 34.59564850\n",
            "Iteration 272, loss = 34.56852812\n",
            "Iteration 273, loss = 34.47502608\n",
            "Iteration 274, loss = 34.40633308\n",
            "Iteration 275, loss = 34.34427571\n",
            "Iteration 276, loss = 34.32629585\n",
            "Iteration 277, loss = 34.34637418\n",
            "Iteration 278, loss = 34.35059346\n",
            "Iteration 279, loss = 34.25288742\n",
            "Iteration 280, loss = 34.18636624\n",
            "Iteration 281, loss = 34.16456446\n",
            "Iteration 282, loss = 34.26312649\n",
            "Iteration 283, loss = 34.22366876\n",
            "Iteration 284, loss = 34.17331205\n",
            "Iteration 285, loss = 34.05704693\n",
            "Iteration 286, loss = 34.07663847\n",
            "Iteration 287, loss = 34.05429028\n",
            "Iteration 288, loss = 34.11436688\n",
            "Iteration 289, loss = 34.08065728\n",
            "Iteration 290, loss = 34.05683287\n",
            "Iteration 291, loss = 33.96623956\n",
            "Iteration 292, loss = 34.03830419\n",
            "Iteration 293, loss = 33.87171544\n",
            "Iteration 294, loss = 33.81883575\n",
            "Iteration 295, loss = 33.78972864\n",
            "Iteration 296, loss = 33.83898488\n",
            "Iteration 297, loss = 33.78068352\n",
            "Iteration 298, loss = 33.77456510\n",
            "Iteration 299, loss = 33.75350053\n",
            "Iteration 300, loss = 33.68012337\n",
            "Iteration 301, loss = 33.63480137\n",
            "Iteration 302, loss = 33.57686440\n",
            "Iteration 303, loss = 33.70874736\n",
            "Iteration 304, loss = 33.70668249\n",
            "Iteration 305, loss = 33.60787329\n",
            "Iteration 306, loss = 33.53464679\n",
            "Iteration 307, loss = 33.45286397\n",
            "Iteration 308, loss = 33.46704640\n",
            "Iteration 309, loss = 33.47611529\n",
            "Iteration 310, loss = 33.49336951\n",
            "Iteration 311, loss = 33.47107337\n",
            "Iteration 312, loss = 33.43494643\n",
            "Iteration 313, loss = 33.35713215\n",
            "Iteration 314, loss = 33.30781128\n",
            "Iteration 315, loss = 33.29818608\n",
            "Iteration 316, loss = 33.32491400\n",
            "Iteration 317, loss = 33.24383870\n",
            "Iteration 318, loss = 33.23859833\n",
            "Iteration 319, loss = 33.18915967\n",
            "Iteration 320, loss = 33.20089018\n",
            "Iteration 321, loss = 33.23799057\n",
            "Iteration 322, loss = 33.19334520\n",
            "Iteration 323, loss = 33.14441939\n",
            "Iteration 324, loss = 33.09053648\n",
            "Iteration 325, loss = 33.06789422\n",
            "Iteration 326, loss = 33.08218721\n",
            "Iteration 327, loss = 33.04398526\n",
            "Iteration 328, loss = 32.99250541\n",
            "Iteration 329, loss = 33.03042783\n",
            "Iteration 330, loss = 32.95886383\n",
            "Iteration 331, loss = 32.92961969\n",
            "Iteration 332, loss = 32.92319692\n",
            "Iteration 333, loss = 32.90620244\n",
            "Iteration 334, loss = 32.89034870\n",
            "Iteration 335, loss = 32.86082805\n",
            "Iteration 336, loss = 32.81059819\n",
            "Iteration 337, loss = 32.81022614\n",
            "Iteration 338, loss = 32.75777953\n",
            "Iteration 339, loss = 32.77508743\n",
            "Iteration 340, loss = 32.71157072\n",
            "Iteration 341, loss = 32.66657158\n",
            "Iteration 342, loss = 32.68067794\n",
            "Iteration 343, loss = 32.69523258\n",
            "Iteration 344, loss = 32.74141323\n",
            "Iteration 345, loss = 32.73749756\n",
            "Iteration 346, loss = 32.73271162\n",
            "Iteration 347, loss = 32.67985466\n",
            "Iteration 348, loss = 32.76022295\n",
            "Iteration 349, loss = 32.54097991\n",
            "Iteration 350, loss = 32.52743682\n",
            "Iteration 351, loss = 32.48587939\n",
            "Iteration 352, loss = 32.51420761\n",
            "Iteration 353, loss = 32.45922099\n",
            "Iteration 354, loss = 32.41499251\n",
            "Iteration 355, loss = 32.38731836\n",
            "Iteration 356, loss = 32.39534123\n",
            "Iteration 357, loss = 32.40792882\n",
            "Iteration 358, loss = 32.42993936\n",
            "Iteration 359, loss = 32.37896035\n",
            "Iteration 360, loss = 32.32391032\n",
            "Iteration 361, loss = 32.34861153\n",
            "Iteration 362, loss = 32.28427330\n",
            "Iteration 363, loss = 32.27712750\n",
            "Iteration 364, loss = 32.24529713\n",
            "Iteration 365, loss = 32.22444744\n",
            "Iteration 366, loss = 32.21946842\n",
            "Iteration 367, loss = 32.18483569\n",
            "Iteration 368, loss = 32.17626098\n",
            "Iteration 369, loss = 32.25756455\n",
            "Iteration 370, loss = 32.26069275\n",
            "Iteration 371, loss = 32.31233118\n",
            "Iteration 372, loss = 32.18443853\n",
            "Iteration 373, loss = 32.12000453\n",
            "Iteration 374, loss = 32.13990311\n",
            "Iteration 375, loss = 32.16993479\n",
            "Iteration 376, loss = 32.19359291\n",
            "Iteration 377, loss = 32.11708694\n",
            "Iteration 378, loss = 32.08616303\n",
            "Iteration 379, loss = 32.15468891\n",
            "Iteration 380, loss = 32.06111046\n",
            "Iteration 381, loss = 32.01690965\n",
            "Iteration 382, loss = 31.95107549\n",
            "Iteration 383, loss = 31.96694130\n",
            "Iteration 384, loss = 31.95021902\n",
            "Iteration 385, loss = 31.93662731\n",
            "Iteration 386, loss = 31.88572399\n",
            "Iteration 387, loss = 31.85570202\n",
            "Iteration 388, loss = 31.86180662\n",
            "Iteration 389, loss = 31.92167956\n",
            "Iteration 390, loss = 31.87848026\n",
            "Iteration 391, loss = 31.80342823\n",
            "Iteration 392, loss = 31.77801668\n",
            "Iteration 393, loss = 31.76728765\n",
            "Iteration 394, loss = 31.79464300\n",
            "Iteration 395, loss = 31.75364682\n",
            "Iteration 396, loss = 31.74377941\n",
            "Iteration 397, loss = 31.72968788\n",
            "Iteration 398, loss = 31.72228774\n",
            "Iteration 399, loss = 31.68016854\n",
            "Iteration 400, loss = 31.65900217\n",
            "Iteration 401, loss = 31.68076275\n",
            "Iteration 402, loss = 31.65206659\n",
            "Iteration 403, loss = 31.67108392\n",
            "Iteration 404, loss = 31.63974208\n",
            "Iteration 405, loss = 31.77772925\n",
            "Iteration 406, loss = 31.59237206\n",
            "Iteration 407, loss = 31.58276312\n",
            "Iteration 408, loss = 31.54303391\n",
            "Iteration 409, loss = 31.56311311\n",
            "Iteration 410, loss = 31.55282438\n",
            "Iteration 411, loss = 31.55402041\n",
            "Iteration 412, loss = 31.54967012\n",
            "Iteration 413, loss = 31.47723744\n",
            "Iteration 414, loss = 31.53111536\n",
            "Iteration 415, loss = 31.53761586\n",
            "Iteration 416, loss = 31.42682952\n",
            "Iteration 417, loss = 31.47507678\n",
            "Iteration 418, loss = 31.53560372\n",
            "Iteration 419, loss = 31.52511565\n",
            "Iteration 420, loss = 31.42178660\n",
            "Iteration 421, loss = 31.36809108\n",
            "Iteration 422, loss = 31.36242630\n",
            "Iteration 423, loss = 31.38845408\n",
            "Iteration 424, loss = 31.41022991\n",
            "Iteration 425, loss = 31.30137343\n",
            "Iteration 426, loss = 31.35623523\n",
            "Iteration 427, loss = 31.47266382\n",
            "Iteration 428, loss = 31.36608773\n",
            "Iteration 429, loss = 31.27907552\n",
            "Iteration 430, loss = 31.26279774\n",
            "Iteration 431, loss = 31.31803063\n",
            "Iteration 432, loss = 31.45038052\n",
            "Iteration 433, loss = 31.34027835\n",
            "Iteration 434, loss = 31.25968217\n",
            "Iteration 435, loss = 31.22168022\n",
            "Iteration 436, loss = 31.19495866\n",
            "Iteration 437, loss = 31.24112987\n",
            "Iteration 438, loss = 31.17012153\n",
            "Iteration 439, loss = 31.20122069\n",
            "Iteration 440, loss = 31.16581993\n",
            "Iteration 441, loss = 31.14512385\n",
            "Iteration 442, loss = 31.20137338\n",
            "Iteration 443, loss = 31.10491203\n",
            "Iteration 444, loss = 31.08033518\n",
            "Iteration 445, loss = 31.11422169\n",
            "Iteration 446, loss = 31.19248606\n",
            "Iteration 447, loss = 31.16162576\n",
            "Iteration 448, loss = 31.17047384\n",
            "Iteration 449, loss = 31.04754821\n",
            "Iteration 450, loss = 31.04092757\n",
            "Iteration 451, loss = 31.02490188\n",
            "Iteration 452, loss = 31.02847237\n",
            "Iteration 453, loss = 31.01505225\n",
            "Iteration 454, loss = 30.99248172\n",
            "Iteration 455, loss = 30.99186465\n",
            "Iteration 456, loss = 30.95248938\n",
            "Iteration 457, loss = 31.03018370\n",
            "Iteration 458, loss = 30.91790131\n",
            "Iteration 459, loss = 31.01188475\n",
            "Iteration 460, loss = 31.15617176\n",
            "Iteration 461, loss = 31.16111034\n",
            "Iteration 462, loss = 31.12291945\n",
            "Iteration 463, loss = 30.94162536\n",
            "Iteration 464, loss = 30.88469217\n",
            "Iteration 465, loss = 30.94057210\n",
            "Iteration 466, loss = 31.04628855\n",
            "Iteration 467, loss = 31.07024503\n",
            "Iteration 468, loss = 30.96141443\n",
            "Iteration 469, loss = 30.85344373\n",
            "Iteration 470, loss = 30.87072691\n",
            "Iteration 471, loss = 30.96054319\n",
            "Iteration 472, loss = 30.88287631\n",
            "Iteration 473, loss = 30.78894129\n",
            "Iteration 474, loss = 30.78129234\n",
            "Iteration 475, loss = 30.90418701\n",
            "Iteration 476, loss = 30.97094252\n",
            "Iteration 477, loss = 30.89361072\n",
            "Iteration 478, loss = 30.81511668\n",
            "Iteration 479, loss = 30.78812558\n",
            "Iteration 480, loss = 30.84217831\n",
            "Iteration 481, loss = 30.77109313\n",
            "Iteration 482, loss = 30.74476516\n",
            "Iteration 483, loss = 30.72526504\n",
            "Iteration 484, loss = 30.74420403\n",
            "Iteration 485, loss = 30.71122696\n",
            "Iteration 486, loss = 30.71089798\n",
            "Iteration 487, loss = 30.71205385\n",
            "Iteration 488, loss = 30.75651161\n",
            "Iteration 489, loss = 30.77414959\n",
            "Iteration 490, loss = 30.66548547\n",
            "Iteration 491, loss = 30.71941213\n",
            "Iteration 492, loss = 30.69371555\n",
            "Iteration 493, loss = 30.78147870\n",
            "Iteration 494, loss = 30.65858273\n",
            "Iteration 495, loss = 30.68988224\n",
            "Iteration 496, loss = 30.64142211\n",
            "Iteration 497, loss = 30.65514319\n",
            "Iteration 498, loss = 30.67386181\n",
            "Iteration 499, loss = 30.60126987\n",
            "Iteration 500, loss = 30.57445472\n",
            "Iteration 501, loss = 30.58870276\n",
            "Iteration 502, loss = 30.70128871\n",
            "Iteration 503, loss = 30.61729633\n",
            "Iteration 504, loss = 30.65803825\n",
            "Iteration 505, loss = 30.79119366\n",
            "Iteration 506, loss = 30.48456185\n",
            "Iteration 507, loss = 30.52126457\n",
            "Iteration 508, loss = 30.63103870\n",
            "Iteration 509, loss = 30.60123719\n",
            "Iteration 510, loss = 30.64418048\n",
            "Iteration 511, loss = 30.62425475\n",
            "Iteration 512, loss = 30.49397097\n",
            "Iteration 513, loss = 30.62360117\n",
            "Iteration 514, loss = 30.48755902\n",
            "Iteration 515, loss = 30.45477927\n",
            "Iteration 516, loss = 30.44416928\n",
            "Iteration 517, loss = 30.48316866\n",
            "Iteration 518, loss = 30.43307852\n",
            "Iteration 519, loss = 30.44418528\n",
            "Iteration 520, loss = 30.42682843\n",
            "Iteration 521, loss = 30.54130219\n",
            "Iteration 522, loss = 30.40686900\n",
            "Iteration 523, loss = 30.64058146\n",
            "Iteration 524, loss = 30.35978874\n",
            "Iteration 525, loss = 30.43161740\n",
            "Iteration 526, loss = 30.44965371\n",
            "Iteration 527, loss = 30.42733699\n",
            "Iteration 528, loss = 30.40490519\n",
            "Iteration 529, loss = 30.37366583\n",
            "Iteration 530, loss = 30.34838562\n",
            "Iteration 531, loss = 30.36073214\n",
            "Iteration 532, loss = 30.35969608\n",
            "Iteration 533, loss = 30.41001669\n",
            "Iteration 534, loss = 30.35282093\n",
            "Iteration 535, loss = 30.35079133\n",
            "Iteration 536, loss = 30.32705707\n",
            "Iteration 537, loss = 30.31646373\n",
            "Iteration 538, loss = 30.29196293\n",
            "Iteration 539, loss = 30.29007912\n",
            "Iteration 540, loss = 30.27959499\n",
            "Iteration 541, loss = 30.33924088\n",
            "Iteration 542, loss = 30.28447610\n",
            "Iteration 543, loss = 30.26429600\n",
            "Iteration 544, loss = 30.26043336\n",
            "Iteration 545, loss = 30.24488223\n",
            "Iteration 546, loss = 30.22781817\n",
            "Iteration 547, loss = 30.31793464\n",
            "Iteration 548, loss = 30.25266134\n",
            "Iteration 549, loss = 30.21900274\n",
            "Iteration 550, loss = 30.23502794\n",
            "Iteration 551, loss = 30.31759243\n",
            "Iteration 552, loss = 30.18347146\n",
            "Iteration 553, loss = 30.17197676\n",
            "Iteration 554, loss = 30.31687918\n",
            "Iteration 555, loss = 30.36014840\n",
            "Iteration 556, loss = 30.20631686\n",
            "Iteration 557, loss = 30.16091618\n",
            "Iteration 558, loss = 30.12909523\n",
            "Iteration 559, loss = 30.18713007\n",
            "Iteration 560, loss = 30.20006147\n",
            "Iteration 561, loss = 30.16631839\n",
            "Iteration 562, loss = 30.13769571\n",
            "Iteration 563, loss = 30.14590293\n",
            "Iteration 564, loss = 30.21613598\n",
            "Iteration 565, loss = 30.15629825\n",
            "Iteration 566, loss = 30.12232171\n",
            "Iteration 567, loss = 30.20615852\n",
            "Iteration 568, loss = 30.17994693\n",
            "Iteration 569, loss = 30.16874894\n",
            "Iteration 570, loss = 30.14133636\n",
            "Iteration 571, loss = 30.05704501\n",
            "Iteration 572, loss = 30.10343375\n",
            "Iteration 573, loss = 30.10184621\n",
            "Iteration 574, loss = 30.10410975\n",
            "Iteration 575, loss = 30.06101097\n",
            "Iteration 576, loss = 30.00336679\n",
            "Iteration 577, loss = 29.99987032\n",
            "Iteration 578, loss = 30.05133426\n",
            "Iteration 579, loss = 30.10426401\n",
            "Iteration 580, loss = 30.10743192\n",
            "Iteration 581, loss = 30.05721438\n",
            "Iteration 582, loss = 29.95384000\n",
            "Iteration 583, loss = 30.12720132\n",
            "Iteration 584, loss = 30.16712938\n",
            "Iteration 585, loss = 30.05569609\n",
            "Iteration 586, loss = 30.00736021\n",
            "Iteration 587, loss = 29.97337639\n",
            "Iteration 588, loss = 29.99774319\n",
            "Iteration 589, loss = 29.96335347\n",
            "Iteration 590, loss = 29.93075712\n",
            "Iteration 591, loss = 29.93707657\n",
            "Iteration 592, loss = 29.93663873\n",
            "Iteration 593, loss = 30.05261310\n",
            "Iteration 594, loss = 29.95741109\n",
            "Iteration 595, loss = 29.91743107\n",
            "Iteration 596, loss = 29.96260857\n",
            "Iteration 597, loss = 29.89391550\n",
            "Iteration 598, loss = 29.92706413\n",
            "Iteration 599, loss = 29.97052181\n",
            "Iteration 600, loss = 29.89767735\n",
            "Iteration 601, loss = 29.87047019\n",
            "Iteration 602, loss = 29.87451224\n",
            "Iteration 603, loss = 29.89427635\n",
            "Iteration 604, loss = 29.89351940\n",
            "Iteration 605, loss = 29.91422829\n",
            "Iteration 606, loss = 29.95224963\n",
            "Iteration 607, loss = 29.97275927\n",
            "Iteration 608, loss = 29.84767467\n",
            "Iteration 609, loss = 29.84990140\n",
            "Iteration 610, loss = 29.89335789\n",
            "Iteration 611, loss = 29.94832823\n",
            "Iteration 612, loss = 29.81889220\n",
            "Iteration 613, loss = 29.87605420\n",
            "Iteration 614, loss = 29.82011091\n",
            "Iteration 615, loss = 29.80699728\n",
            "Iteration 616, loss = 29.78714078\n",
            "Iteration 617, loss = 29.78769659\n",
            "Iteration 618, loss = 29.77892355\n",
            "Iteration 619, loss = 29.85447264\n",
            "Iteration 620, loss = 29.78013765\n",
            "Iteration 621, loss = 29.79997774\n",
            "Iteration 622, loss = 29.84721851\n",
            "Iteration 623, loss = 29.84711762\n",
            "Iteration 624, loss = 29.73189918\n",
            "Iteration 625, loss = 29.87187299\n",
            "Iteration 626, loss = 29.70960453\n",
            "Iteration 627, loss = 29.75231983\n",
            "Iteration 628, loss = 29.82287379\n",
            "Iteration 629, loss = 29.82292492\n",
            "Iteration 630, loss = 29.73372487\n",
            "Iteration 631, loss = 29.72427784\n",
            "Iteration 632, loss = 29.69332111\n",
            "Iteration 633, loss = 29.71719828\n",
            "Iteration 634, loss = 29.66908827\n",
            "Iteration 635, loss = 29.72927768\n",
            "Iteration 636, loss = 29.84647007\n",
            "Iteration 637, loss = 29.67891175\n",
            "Iteration 638, loss = 29.65432791\n",
            "Iteration 639, loss = 29.79058922\n",
            "Iteration 640, loss = 29.70089831\n",
            "Iteration 641, loss = 29.63986598\n",
            "Iteration 642, loss = 29.72369325\n",
            "Iteration 643, loss = 29.65595000\n",
            "Iteration 644, loss = 29.61309085\n",
            "Iteration 645, loss = 29.65491679\n",
            "Iteration 646, loss = 29.66086274\n",
            "Iteration 647, loss = 29.62450051\n",
            "Iteration 648, loss = 29.66793075\n",
            "Iteration 649, loss = 29.59976152\n",
            "Iteration 650, loss = 29.71367427\n",
            "Iteration 651, loss = 29.56801544\n",
            "Iteration 652, loss = 29.60566853\n",
            "Iteration 653, loss = 29.64769984\n",
            "Iteration 654, loss = 29.63543586\n",
            "Iteration 655, loss = 29.55637148\n",
            "Iteration 656, loss = 29.52903178\n",
            "Iteration 657, loss = 29.61494484\n",
            "Iteration 658, loss = 29.66600095\n",
            "Iteration 659, loss = 29.66214606\n",
            "Iteration 660, loss = 29.54438236\n",
            "Iteration 661, loss = 29.44545466\n",
            "Iteration 662, loss = 29.57376691\n",
            "Iteration 663, loss = 29.73241461\n",
            "Iteration 664, loss = 29.91203780\n",
            "Iteration 665, loss = 29.82375382\n",
            "Iteration 666, loss = 29.56857088\n",
            "Iteration 667, loss = 29.44390680\n",
            "Iteration 668, loss = 29.54975118\n",
            "Iteration 669, loss = 29.73459585\n",
            "Iteration 670, loss = 29.80103418\n",
            "Iteration 671, loss = 29.60469095\n",
            "Iteration 672, loss = 29.54039429\n",
            "Iteration 673, loss = 29.44950329\n",
            "Iteration 674, loss = 29.47630916\n",
            "Iteration 675, loss = 29.48869547\n",
            "Iteration 676, loss = 29.59315834\n",
            "Iteration 677, loss = 29.53804714\n",
            "Iteration 678, loss = 29.50958801\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 127.74384173\n",
            "Iteration 2, loss = 83.94601727\n",
            "Iteration 3, loss = 79.98543357\n",
            "Iteration 4, loss = 77.58639587\n",
            "Iteration 5, loss = 75.55203860\n",
            "Iteration 6, loss = 73.62517772\n",
            "Iteration 7, loss = 71.90101031\n",
            "Iteration 8, loss = 69.98603312\n",
            "Iteration 9, loss = 68.26312508\n",
            "Iteration 10, loss = 66.72885517\n",
            "Iteration 11, loss = 65.24609803\n",
            "Iteration 12, loss = 64.04897889\n",
            "Iteration 13, loss = 62.83093399\n",
            "Iteration 14, loss = 61.85246632\n",
            "Iteration 15, loss = 61.07155259\n",
            "Iteration 16, loss = 60.62446341\n",
            "Iteration 17, loss = 59.93892857\n",
            "Iteration 18, loss = 59.65501626\n",
            "Iteration 19, loss = 59.24610441\n",
            "Iteration 20, loss = 59.00681351\n",
            "Iteration 21, loss = 58.82614833\n",
            "Iteration 22, loss = 58.64710627\n",
            "Iteration 23, loss = 58.82520426\n",
            "Iteration 24, loss = 58.27061148\n",
            "Iteration 25, loss = 58.19630507\n",
            "Iteration 26, loss = 57.91325328\n",
            "Iteration 27, loss = 57.82351370\n",
            "Iteration 28, loss = 58.04975643\n",
            "Iteration 29, loss = 57.59243025\n",
            "Iteration 30, loss = 57.49515801\n",
            "Iteration 31, loss = 57.32298032\n",
            "Iteration 32, loss = 57.40734065\n",
            "Iteration 33, loss = 57.26091486\n",
            "Iteration 34, loss = 57.17815681\n",
            "Iteration 35, loss = 56.89894127\n",
            "Iteration 36, loss = 56.98015829\n",
            "Iteration 37, loss = 56.82889020\n",
            "Iteration 38, loss = 57.00266880\n",
            "Iteration 39, loss = 56.79278704\n",
            "Iteration 40, loss = 56.62418131\n",
            "Iteration 41, loss = 56.41455455\n",
            "Iteration 42, loss = 56.57753266\n",
            "Iteration 43, loss = 56.47885734\n",
            "Iteration 44, loss = 56.34654266\n",
            "Iteration 45, loss = 56.17005680\n",
            "Iteration 46, loss = 56.27963942\n",
            "Iteration 47, loss = 56.28245818\n",
            "Iteration 48, loss = 56.01703776\n",
            "Iteration 49, loss = 56.06689556\n",
            "Iteration 50, loss = 56.21034683\n",
            "Iteration 51, loss = 55.95087209\n",
            "Iteration 52, loss = 55.88450060\n",
            "Iteration 53, loss = 55.85253428\n",
            "Iteration 54, loss = 55.98422882\n",
            "Iteration 55, loss = 55.83056065\n",
            "Iteration 56, loss = 55.76084594\n",
            "Iteration 57, loss = 55.76752719\n",
            "Iteration 58, loss = 55.67892206\n",
            "Iteration 59, loss = 55.66255419\n",
            "Iteration 60, loss = 55.52777405\n",
            "Iteration 61, loss = 55.60642427\n",
            "Iteration 62, loss = 55.52136441\n",
            "Iteration 63, loss = 55.54412429\n",
            "Iteration 64, loss = 55.61900528\n",
            "Iteration 65, loss = 55.62035440\n",
            "Iteration 66, loss = 55.57699220\n",
            "Iteration 67, loss = 55.39964445\n",
            "Iteration 68, loss = 55.90314393\n",
            "Iteration 69, loss = 55.58732791\n",
            "Iteration 70, loss = 55.45968194\n",
            "Iteration 71, loss = 55.41645419\n",
            "Iteration 72, loss = 55.64616345\n",
            "Iteration 73, loss = 55.21369085\n",
            "Iteration 74, loss = 55.43590562\n",
            "Iteration 75, loss = 55.24396439\n",
            "Iteration 76, loss = 55.27177086\n",
            "Iteration 77, loss = 55.17406039\n",
            "Iteration 78, loss = 55.29177362\n",
            "Iteration 79, loss = 55.29510996\n",
            "Iteration 80, loss = 55.63754585\n",
            "Iteration 81, loss = 55.22755910\n",
            "Iteration 82, loss = 55.25519281\n",
            "Iteration 83, loss = 55.01709401\n",
            "Iteration 84, loss = 55.10641109\n",
            "Iteration 85, loss = 55.02343355\n",
            "Iteration 86, loss = 55.12504029\n",
            "Iteration 87, loss = 55.17032179\n",
            "Iteration 88, loss = 55.00143726\n",
            "Iteration 89, loss = 54.93287873\n",
            "Iteration 90, loss = 55.06521843\n",
            "Iteration 91, loss = 55.11728877\n",
            "Iteration 92, loss = 54.92778801\n",
            "Iteration 93, loss = 54.84381146\n",
            "Iteration 94, loss = 55.04961875\n",
            "Iteration 95, loss = 54.84874147\n",
            "Iteration 96, loss = 54.76404347\n",
            "Iteration 97, loss = 55.07849211\n",
            "Iteration 98, loss = 54.69077439\n",
            "Iteration 99, loss = 54.79679717\n",
            "Iteration 100, loss = 54.69822233\n",
            "Iteration 101, loss = 54.80428517\n",
            "Iteration 102, loss = 55.22368050\n",
            "Iteration 103, loss = 54.67912908\n",
            "Iteration 104, loss = 54.69278019\n",
            "Iteration 105, loss = 54.71875550\n",
            "Iteration 106, loss = 54.59099888\n",
            "Iteration 107, loss = 54.81358398\n",
            "Iteration 108, loss = 54.69871435\n",
            "Iteration 109, loss = 54.64558260\n",
            "Iteration 110, loss = 54.72474232\n",
            "Iteration 111, loss = 54.59854445\n",
            "Iteration 112, loss = 54.70242508\n",
            "Iteration 113, loss = 54.56711489\n",
            "Iteration 114, loss = 54.63170240\n",
            "Iteration 115, loss = 54.55278828\n",
            "Iteration 116, loss = 54.52670082\n",
            "Iteration 117, loss = 54.69840741\n",
            "Iteration 118, loss = 54.55946256\n",
            "Iteration 119, loss = 54.38698617\n",
            "Iteration 120, loss = 54.89859622\n",
            "Iteration 121, loss = 54.46105554\n",
            "Iteration 122, loss = 54.50272179\n",
            "Iteration 123, loss = 54.38610671\n",
            "Iteration 124, loss = 54.46100640\n",
            "Iteration 125, loss = 54.40381777\n",
            "Iteration 126, loss = 54.43681269\n",
            "Iteration 127, loss = 54.35432354\n",
            "Iteration 128, loss = 54.51734425\n",
            "Iteration 129, loss = 55.06217853\n",
            "Iteration 130, loss = 54.47861417\n",
            "Iteration 131, loss = 54.32666796\n",
            "Iteration 132, loss = 54.29832778\n",
            "Iteration 133, loss = 54.34380326\n",
            "Iteration 134, loss = 54.27376376\n",
            "Iteration 135, loss = 54.21818533\n",
            "Iteration 136, loss = 54.26681323\n",
            "Iteration 137, loss = 54.30816461\n",
            "Iteration 138, loss = 54.52155437\n",
            "Iteration 139, loss = 54.31550861\n",
            "Iteration 140, loss = 54.41670808\n",
            "Iteration 141, loss = 54.30817937\n",
            "Iteration 142, loss = 54.23816654\n",
            "Iteration 143, loss = 54.40172135\n",
            "Iteration 144, loss = 54.40084860\n",
            "Iteration 145, loss = 54.29044230\n",
            "Iteration 146, loss = 54.29773147\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 415.96835860\n",
            "Iteration 2, loss = 364.73864924\n",
            "Iteration 3, loss = 318.63907639\n",
            "Iteration 4, loss = 277.18173356\n",
            "Iteration 5, loss = 238.89637535\n",
            "Iteration 6, loss = 207.42304505\n",
            "Iteration 7, loss = 178.66411500\n",
            "Iteration 8, loss = 153.88101751\n",
            "Iteration 9, loss = 134.22368283\n",
            "Iteration 10, loss = 117.95131492\n",
            "Iteration 11, loss = 106.05155612\n",
            "Iteration 12, loss = 96.21151414\n",
            "Iteration 13, loss = 90.00340687\n",
            "Iteration 14, loss = 84.95921602\n",
            "Iteration 15, loss = 81.79395459\n",
            "Iteration 16, loss = 79.84523533\n",
            "Iteration 17, loss = 79.21027336\n",
            "Iteration 18, loss = 78.82875079\n",
            "Iteration 19, loss = 78.58772486\n",
            "Iteration 20, loss = 77.98025284\n",
            "Iteration 21, loss = 77.63223010\n",
            "Iteration 22, loss = 76.94315194\n",
            "Iteration 23, loss = 76.12930998\n",
            "Iteration 24, loss = 75.05323216\n",
            "Iteration 25, loss = 73.82760124\n",
            "Iteration 26, loss = 72.60740592\n",
            "Iteration 27, loss = 71.23177522\n",
            "Iteration 28, loss = 70.13559019\n",
            "Iteration 29, loss = 68.88595032\n",
            "Iteration 30, loss = 67.91790887\n",
            "Iteration 31, loss = 66.78831368\n",
            "Iteration 32, loss = 65.89683176\n",
            "Iteration 33, loss = 65.06698515\n",
            "Iteration 34, loss = 64.38435339\n",
            "Iteration 35, loss = 63.69114404\n",
            "Iteration 36, loss = 62.88282069\n",
            "Iteration 37, loss = 62.24646465\n",
            "Iteration 38, loss = 61.60489051\n",
            "Iteration 39, loss = 60.88717049\n",
            "Iteration 40, loss = 60.25156211\n",
            "Iteration 41, loss = 59.57763931\n",
            "Iteration 42, loss = 58.89797118\n",
            "Iteration 43, loss = 58.39061423\n",
            "Iteration 44, loss = 57.75428095\n",
            "Iteration 45, loss = 57.24569659\n",
            "Iteration 46, loss = 56.85615449\n",
            "Iteration 47, loss = 56.30387388\n",
            "Iteration 48, loss = 55.87092847\n",
            "Iteration 49, loss = 55.52010318\n",
            "Iteration 50, loss = 55.10476182\n",
            "Iteration 51, loss = 54.76269918\n",
            "Iteration 52, loss = 54.44559296\n",
            "Iteration 53, loss = 54.13413008\n",
            "Iteration 54, loss = 53.90438136\n",
            "Iteration 55, loss = 53.53146199\n",
            "Iteration 56, loss = 53.27841298\n",
            "Iteration 57, loss = 53.02634666\n",
            "Iteration 58, loss = 52.71247884\n",
            "Iteration 59, loss = 52.48305040\n",
            "Iteration 60, loss = 52.26348170\n",
            "Iteration 61, loss = 51.99432495\n",
            "Iteration 62, loss = 51.79235250\n",
            "Iteration 63, loss = 51.52473157\n",
            "Iteration 64, loss = 51.33493678\n",
            "Iteration 65, loss = 51.06857866\n",
            "Iteration 66, loss = 50.92587089\n",
            "Iteration 67, loss = 50.69710026\n",
            "Iteration 68, loss = 50.49712933\n",
            "Iteration 69, loss = 50.32630861\n",
            "Iteration 70, loss = 50.13897992\n",
            "Iteration 71, loss = 49.97884988\n",
            "Iteration 72, loss = 49.82033978\n",
            "Iteration 73, loss = 49.65162636\n",
            "Iteration 74, loss = 49.49468549\n",
            "Iteration 75, loss = 49.30729476\n",
            "Iteration 76, loss = 49.14163751\n",
            "Iteration 77, loss = 49.04067088\n",
            "Iteration 78, loss = 48.88450747\n",
            "Iteration 79, loss = 48.77170907\n",
            "Iteration 80, loss = 48.62948141\n",
            "Iteration 81, loss = 48.47388500\n",
            "Iteration 82, loss = 48.35843084\n",
            "Iteration 83, loss = 48.19565890\n",
            "Iteration 84, loss = 48.07545228\n",
            "Iteration 85, loss = 48.01482609\n",
            "Iteration 86, loss = 47.92020988\n",
            "Iteration 87, loss = 47.82836799\n",
            "Iteration 88, loss = 47.74964044\n",
            "Iteration 89, loss = 47.64419556\n",
            "Iteration 90, loss = 47.58930178\n",
            "Iteration 91, loss = 47.39297534\n",
            "Iteration 92, loss = 47.29111106\n",
            "Iteration 93, loss = 47.17777909\n",
            "Iteration 94, loss = 47.06482638\n",
            "Iteration 95, loss = 46.97735760\n",
            "Iteration 96, loss = 46.91254172\n",
            "Iteration 97, loss = 46.82631788\n",
            "Iteration 98, loss = 46.73862396\n",
            "Iteration 99, loss = 46.61879029\n",
            "Iteration 100, loss = 46.51698456\n",
            "Iteration 101, loss = 46.47575387\n",
            "Iteration 102, loss = 46.38595593\n",
            "Iteration 103, loss = 46.32372781\n",
            "Iteration 104, loss = 46.27749103\n",
            "Iteration 105, loss = 46.16391311\n",
            "Iteration 106, loss = 46.09692750\n",
            "Iteration 107, loss = 46.00779770\n",
            "Iteration 108, loss = 45.93516269\n",
            "Iteration 109, loss = 45.87523466\n",
            "Iteration 110, loss = 45.81202077\n",
            "Iteration 111, loss = 45.75385762\n",
            "Iteration 112, loss = 45.68761552\n",
            "Iteration 113, loss = 45.64323245\n",
            "Iteration 114, loss = 45.60368829\n",
            "Iteration 115, loss = 45.54539662\n",
            "Iteration 116, loss = 45.58423688\n",
            "Iteration 117, loss = 45.40191946\n",
            "Iteration 118, loss = 45.34171386\n",
            "Iteration 119, loss = 45.28620576\n",
            "Iteration 120, loss = 45.23552015\n",
            "Iteration 121, loss = 45.17307699\n",
            "Iteration 122, loss = 45.17217627\n",
            "Iteration 123, loss = 45.05528116\n",
            "Iteration 124, loss = 45.04106809\n",
            "Iteration 125, loss = 45.04072301\n",
            "Iteration 126, loss = 45.02727975\n",
            "Iteration 127, loss = 44.97363163\n",
            "Iteration 128, loss = 44.92844426\n",
            "Iteration 129, loss = 44.82920822\n",
            "Iteration 130, loss = 44.71813026\n",
            "Iteration 131, loss = 44.64417298\n",
            "Iteration 132, loss = 44.65981739\n",
            "Iteration 133, loss = 44.57537329\n",
            "Iteration 134, loss = 44.53486048\n",
            "Iteration 135, loss = 44.51152271\n",
            "Iteration 136, loss = 44.43952161\n",
            "Iteration 137, loss = 44.37749898\n",
            "Iteration 138, loss = 44.35187183\n",
            "Iteration 139, loss = 44.27742659\n",
            "Iteration 140, loss = 44.24145937\n",
            "Iteration 141, loss = 44.19424462\n",
            "Iteration 142, loss = 44.14700176\n",
            "Iteration 143, loss = 44.12321985\n",
            "Iteration 144, loss = 44.07253276\n",
            "Iteration 145, loss = 44.05452763\n",
            "Iteration 146, loss = 43.99762406\n",
            "Iteration 147, loss = 43.95636081\n",
            "Iteration 148, loss = 43.90736544\n",
            "Iteration 149, loss = 43.89164984\n",
            "Iteration 150, loss = 43.83106045\n",
            "Iteration 151, loss = 43.83017583\n",
            "Iteration 152, loss = 43.76151966\n",
            "Iteration 153, loss = 43.72664925\n",
            "Iteration 154, loss = 43.68286892\n",
            "Iteration 155, loss = 43.64148429\n",
            "Iteration 156, loss = 43.60188189\n",
            "Iteration 157, loss = 43.59356607\n",
            "Iteration 158, loss = 43.54815707\n",
            "Iteration 159, loss = 43.52733901\n",
            "Iteration 160, loss = 43.48308355\n",
            "Iteration 161, loss = 43.39144241\n",
            "Iteration 162, loss = 43.37805932\n",
            "Iteration 163, loss = 43.37939682\n",
            "Iteration 164, loss = 43.35973514\n",
            "Iteration 165, loss = 43.31180033\n",
            "Iteration 166, loss = 43.26179975\n",
            "Iteration 167, loss = 43.37338044\n",
            "Iteration 168, loss = 43.16481248\n",
            "Iteration 169, loss = 43.13706422\n",
            "Iteration 170, loss = 43.08577169\n",
            "Iteration 171, loss = 43.06564653\n",
            "Iteration 172, loss = 43.03481875\n",
            "Iteration 173, loss = 42.99990546\n",
            "Iteration 174, loss = 42.99854993\n",
            "Iteration 175, loss = 42.94164043\n",
            "Iteration 176, loss = 42.90098282\n",
            "Iteration 177, loss = 42.89035692\n",
            "Iteration 178, loss = 42.86893533\n",
            "Iteration 179, loss = 42.81345285\n",
            "Iteration 180, loss = 42.79550470\n",
            "Iteration 181, loss = 42.75468573\n",
            "Iteration 182, loss = 42.71857598\n",
            "Iteration 183, loss = 42.68566147\n",
            "Iteration 184, loss = 42.68850814\n",
            "Iteration 185, loss = 42.61877354\n",
            "Iteration 186, loss = 42.59793298\n",
            "Iteration 187, loss = 42.57289674\n",
            "Iteration 188, loss = 42.59232597\n",
            "Iteration 189, loss = 42.53616776\n",
            "Iteration 190, loss = 42.62223453\n",
            "Iteration 191, loss = 42.45620802\n",
            "Iteration 192, loss = 42.49048364\n",
            "Iteration 193, loss = 42.38160149\n",
            "Iteration 194, loss = 42.51529195\n",
            "Iteration 195, loss = 42.44111597\n",
            "Iteration 196, loss = 42.36278095\n",
            "Iteration 197, loss = 42.30105299\n",
            "Iteration 198, loss = 42.24711539\n",
            "Iteration 199, loss = 42.26912933\n",
            "Iteration 200, loss = 42.18394695\n",
            "Iteration 201, loss = 42.18806729\n",
            "Iteration 202, loss = 42.19039166\n",
            "Iteration 203, loss = 42.10927729\n",
            "Iteration 204, loss = 42.09200538\n",
            "Iteration 205, loss = 42.03792829\n",
            "Iteration 206, loss = 41.99453462\n",
            "Iteration 207, loss = 41.97550288\n",
            "Iteration 208, loss = 41.96242046\n",
            "Iteration 209, loss = 41.98161603\n",
            "Iteration 210, loss = 41.88498597\n",
            "Iteration 211, loss = 41.87959140\n",
            "Iteration 212, loss = 41.84492394\n",
            "Iteration 213, loss = 41.82532129\n",
            "Iteration 214, loss = 41.79948212\n",
            "Iteration 215, loss = 41.75942044\n",
            "Iteration 216, loss = 41.71879073\n",
            "Iteration 217, loss = 41.71958833\n",
            "Iteration 218, loss = 41.66541604\n",
            "Iteration 219, loss = 41.65934729\n",
            "Iteration 220, loss = 41.60735000\n",
            "Iteration 221, loss = 41.60583359\n",
            "Iteration 222, loss = 41.59124809\n",
            "Iteration 223, loss = 41.52714263\n",
            "Iteration 224, loss = 41.50829223\n",
            "Iteration 225, loss = 41.62747963\n",
            "Iteration 226, loss = 41.46067681\n",
            "Iteration 227, loss = 41.43682633\n",
            "Iteration 228, loss = 41.43078814\n",
            "Iteration 229, loss = 41.37890390\n",
            "Iteration 230, loss = 41.32719730\n",
            "Iteration 231, loss = 41.28760937\n",
            "Iteration 232, loss = 41.26587954\n",
            "Iteration 233, loss = 41.25101496\n",
            "Iteration 234, loss = 41.28536633\n",
            "Iteration 235, loss = 41.27098151\n",
            "Iteration 236, loss = 41.20297573\n",
            "Iteration 237, loss = 41.14585797\n",
            "Iteration 238, loss = 41.13091042\n",
            "Iteration 239, loss = 41.09193885\n",
            "Iteration 240, loss = 41.05878947\n",
            "Iteration 241, loss = 41.06846977\n",
            "Iteration 242, loss = 41.08954948\n",
            "Iteration 243, loss = 40.98433636\n",
            "Iteration 244, loss = 41.02241083\n",
            "Iteration 245, loss = 40.92739795\n",
            "Iteration 246, loss = 40.90860893\n",
            "Iteration 247, loss = 40.88647917\n",
            "Iteration 248, loss = 40.88307135\n",
            "Iteration 249, loss = 40.89403672\n",
            "Iteration 250, loss = 40.85761055\n",
            "Iteration 251, loss = 40.84783696\n",
            "Iteration 252, loss = 40.82398933\n",
            "Iteration 253, loss = 40.74163402\n",
            "Iteration 254, loss = 40.66882106\n",
            "Iteration 255, loss = 40.71132491\n",
            "Iteration 256, loss = 40.78132557\n",
            "Iteration 257, loss = 40.69746764\n",
            "Iteration 258, loss = 40.58041115\n",
            "Iteration 259, loss = 40.53076359\n",
            "Iteration 260, loss = 40.50531572\n",
            "Iteration 261, loss = 40.51468514\n",
            "Iteration 262, loss = 40.52994511\n",
            "Iteration 263, loss = 40.48357923\n",
            "Iteration 264, loss = 40.42053016\n",
            "Iteration 265, loss = 40.36476411\n",
            "Iteration 266, loss = 40.36721046\n",
            "Iteration 267, loss = 40.30647671\n",
            "Iteration 268, loss = 40.32977388\n",
            "Iteration 269, loss = 40.31954365\n",
            "Iteration 270, loss = 40.23380226\n",
            "Iteration 271, loss = 40.27358205\n",
            "Iteration 272, loss = 40.21949622\n",
            "Iteration 273, loss = 40.13944998\n",
            "Iteration 274, loss = 40.10042441\n",
            "Iteration 275, loss = 40.06473828\n",
            "Iteration 276, loss = 40.02466024\n",
            "Iteration 277, loss = 40.05968223\n",
            "Iteration 278, loss = 39.97777077\n",
            "Iteration 279, loss = 39.93876965\n",
            "Iteration 280, loss = 39.90356577\n",
            "Iteration 281, loss = 39.90985221\n",
            "Iteration 282, loss = 39.86435034\n",
            "Iteration 283, loss = 39.84779665\n",
            "Iteration 284, loss = 39.83479477\n",
            "Iteration 285, loss = 39.79890523\n",
            "Iteration 286, loss = 39.77375845\n",
            "Iteration 287, loss = 39.72049329\n",
            "Iteration 288, loss = 39.73199413\n",
            "Iteration 289, loss = 39.67956686\n",
            "Iteration 290, loss = 39.65834868\n",
            "Iteration 291, loss = 39.62051403\n",
            "Iteration 292, loss = 39.57278531\n",
            "Iteration 293, loss = 39.60448746\n",
            "Iteration 294, loss = 39.56577969\n",
            "Iteration 295, loss = 39.52452320\n",
            "Iteration 296, loss = 39.48826358\n",
            "Iteration 297, loss = 39.43792482\n",
            "Iteration 298, loss = 39.39317120\n",
            "Iteration 299, loss = 39.37240804\n",
            "Iteration 300, loss = 39.40304971\n",
            "Iteration 301, loss = 39.42563118\n",
            "Iteration 302, loss = 39.34345552\n",
            "Iteration 303, loss = 39.31819982\n",
            "Iteration 304, loss = 39.25190965\n",
            "Iteration 305, loss = 39.22131477\n",
            "Iteration 306, loss = 39.11667041\n",
            "Iteration 307, loss = 39.15291606\n",
            "Iteration 308, loss = 39.19593310\n",
            "Iteration 309, loss = 39.21444650\n",
            "Iteration 310, loss = 39.14739183\n",
            "Iteration 311, loss = 39.01689107\n",
            "Iteration 312, loss = 39.11015500\n",
            "Iteration 313, loss = 39.07573125\n",
            "Iteration 314, loss = 39.05405415\n",
            "Iteration 315, loss = 38.96625118\n",
            "Iteration 316, loss = 38.89501116\n",
            "Iteration 317, loss = 38.85475605\n",
            "Iteration 318, loss = 38.85382159\n",
            "Iteration 319, loss = 38.79668386\n",
            "Iteration 320, loss = 38.76626017\n",
            "Iteration 321, loss = 38.80884043\n",
            "Iteration 322, loss = 38.88862505\n",
            "Iteration 323, loss = 38.69579893\n",
            "Iteration 324, loss = 38.63626960\n",
            "Iteration 325, loss = 38.67977745\n",
            "Iteration 326, loss = 38.73452538\n",
            "Iteration 327, loss = 38.73422414\n",
            "Iteration 328, loss = 38.65168090\n",
            "Iteration 329, loss = 38.52896272\n",
            "Iteration 330, loss = 38.46615052\n",
            "Iteration 331, loss = 38.62421567\n",
            "Iteration 332, loss = 38.67525114\n",
            "Iteration 333, loss = 38.58248523\n",
            "Iteration 334, loss = 38.35959616\n",
            "Iteration 335, loss = 38.49111097\n",
            "Iteration 336, loss = 38.51590476\n",
            "Iteration 337, loss = 38.57409648\n",
            "Iteration 338, loss = 38.40600020\n",
            "Iteration 339, loss = 38.27850875\n",
            "Iteration 340, loss = 38.40898930\n",
            "Iteration 341, loss = 38.27867048\n",
            "Iteration 342, loss = 38.34246215\n",
            "Iteration 343, loss = 38.17736509\n",
            "Iteration 344, loss = 38.14409490\n",
            "Iteration 345, loss = 38.09810874\n",
            "Iteration 346, loss = 38.07399596\n",
            "Iteration 347, loss = 38.09589320\n",
            "Iteration 348, loss = 38.07107978\n",
            "Iteration 349, loss = 38.02661290\n",
            "Iteration 350, loss = 37.94343025\n",
            "Iteration 351, loss = 37.94838898\n",
            "Iteration 352, loss = 37.90743641\n",
            "Iteration 353, loss = 37.87503692\n",
            "Iteration 354, loss = 37.84572698\n",
            "Iteration 355, loss = 37.87320210\n",
            "Iteration 356, loss = 37.82693080\n",
            "Iteration 357, loss = 37.81006117\n",
            "Iteration 358, loss = 37.74802912\n",
            "Iteration 359, loss = 37.70806266\n",
            "Iteration 360, loss = 37.69997265\n",
            "Iteration 361, loss = 37.71682992\n",
            "Iteration 362, loss = 37.67230354\n",
            "Iteration 363, loss = 37.61587776\n",
            "Iteration 364, loss = 37.59411910\n",
            "Iteration 365, loss = 37.70939268\n",
            "Iteration 366, loss = 37.69356401\n",
            "Iteration 367, loss = 37.63712818\n",
            "Iteration 368, loss = 37.59507691\n",
            "Iteration 369, loss = 37.46698660\n",
            "Iteration 370, loss = 37.45964231\n",
            "Iteration 371, loss = 37.52061299\n",
            "Iteration 372, loss = 37.47414423\n",
            "Iteration 373, loss = 37.44838501\n",
            "Iteration 374, loss = 37.38152268\n",
            "Iteration 375, loss = 37.37740644\n",
            "Iteration 376, loss = 37.36287574\n",
            "Iteration 377, loss = 37.28768145\n",
            "Iteration 378, loss = 37.24008470\n",
            "Iteration 379, loss = 37.24835748\n",
            "Iteration 380, loss = 37.25252736\n",
            "Iteration 381, loss = 37.31552286\n",
            "Iteration 382, loss = 37.31908780\n",
            "Iteration 383, loss = 37.13013474\n",
            "Iteration 384, loss = 37.11464857\n",
            "Iteration 385, loss = 37.14444801\n",
            "Iteration 386, loss = 37.06118107\n",
            "Iteration 387, loss = 37.06473697\n",
            "Iteration 388, loss = 37.05489669\n",
            "Iteration 389, loss = 37.16099315\n",
            "Iteration 390, loss = 37.23060125\n",
            "Iteration 391, loss = 37.01572265\n",
            "Iteration 392, loss = 36.99484514\n",
            "Iteration 393, loss = 36.97944973\n",
            "Iteration 394, loss = 36.91062497\n",
            "Iteration 395, loss = 36.86280176\n",
            "Iteration 396, loss = 36.89185463\n",
            "Iteration 397, loss = 36.89930158\n",
            "Iteration 398, loss = 36.85582835\n",
            "Iteration 399, loss = 36.76433209\n",
            "Iteration 400, loss = 36.78742851\n",
            "Iteration 401, loss = 36.85824336\n",
            "Iteration 402, loss = 36.76741256\n",
            "Iteration 403, loss = 36.69525475\n",
            "Iteration 404, loss = 36.60172000\n",
            "Iteration 405, loss = 36.88673276\n",
            "Iteration 406, loss = 36.83011508\n",
            "Iteration 407, loss = 36.75266513\n",
            "Iteration 408, loss = 36.56087234\n",
            "Iteration 409, loss = 36.70427969\n",
            "Iteration 410, loss = 36.58431495\n",
            "Iteration 411, loss = 36.51271503\n",
            "Iteration 412, loss = 36.52809221\n",
            "Iteration 413, loss = 36.53208094\n",
            "Iteration 414, loss = 36.48613776\n",
            "Iteration 415, loss = 36.40888756\n",
            "Iteration 416, loss = 36.37177570\n",
            "Iteration 417, loss = 36.36276549\n",
            "Iteration 418, loss = 36.37785942\n",
            "Iteration 419, loss = 36.34492135\n",
            "Iteration 420, loss = 36.34105221\n",
            "Iteration 421, loss = 36.32319049\n",
            "Iteration 422, loss = 36.27856714\n",
            "Iteration 423, loss = 36.27015838\n",
            "Iteration 424, loss = 36.25081623\n",
            "Iteration 425, loss = 36.23181031\n",
            "Iteration 426, loss = 36.17025865\n",
            "Iteration 427, loss = 36.19784738\n",
            "Iteration 428, loss = 36.22242730\n",
            "Iteration 429, loss = 36.17462267\n",
            "Iteration 430, loss = 36.09757585\n",
            "Iteration 431, loss = 36.03866587\n",
            "Iteration 432, loss = 36.04587020\n",
            "Iteration 433, loss = 36.08455549\n",
            "Iteration 434, loss = 36.11994801\n",
            "Iteration 435, loss = 36.03381570\n",
            "Iteration 436, loss = 36.17930248\n",
            "Iteration 437, loss = 35.95643406\n",
            "Iteration 438, loss = 35.93940438\n",
            "Iteration 439, loss = 35.90169435\n",
            "Iteration 440, loss = 35.87245877\n",
            "Iteration 441, loss = 35.86255279\n",
            "Iteration 442, loss = 35.85113866\n",
            "Iteration 443, loss = 35.88035608\n",
            "Iteration 444, loss = 35.95734838\n",
            "Iteration 445, loss = 35.85018675\n",
            "Iteration 446, loss = 35.75166298\n",
            "Iteration 447, loss = 35.71126895\n",
            "Iteration 448, loss = 35.66282737\n",
            "Iteration 449, loss = 35.66141060\n",
            "Iteration 450, loss = 35.76585488\n",
            "Iteration 451, loss = 35.73689964\n",
            "Iteration 452, loss = 35.68153266\n",
            "Iteration 453, loss = 35.61008171\n",
            "Iteration 454, loss = 35.65272167\n",
            "Iteration 455, loss = 35.66610129\n",
            "Iteration 456, loss = 35.52223348\n",
            "Iteration 457, loss = 35.51353636\n",
            "Iteration 458, loss = 35.63021609\n",
            "Iteration 459, loss = 35.66422730\n",
            "Iteration 460, loss = 35.58349022\n",
            "Iteration 461, loss = 35.39720278\n",
            "Iteration 462, loss = 35.43648601\n",
            "Iteration 463, loss = 35.44690761\n",
            "Iteration 464, loss = 35.43172228\n",
            "Iteration 465, loss = 35.39993670\n",
            "Iteration 466, loss = 35.49415181\n",
            "Iteration 467, loss = 35.35113208\n",
            "Iteration 468, loss = 35.31696451\n",
            "Iteration 469, loss = 35.30015691\n",
            "Iteration 470, loss = 35.23842047\n",
            "Iteration 471, loss = 35.47231397\n",
            "Iteration 472, loss = 35.27483155\n",
            "Iteration 473, loss = 35.19627189\n",
            "Iteration 474, loss = 35.16213066\n",
            "Iteration 475, loss = 35.17385476\n",
            "Iteration 476, loss = 35.13212353\n",
            "Iteration 477, loss = 35.08836891\n",
            "Iteration 478, loss = 35.10098381\n",
            "Iteration 479, loss = 35.08520210\n",
            "Iteration 480, loss = 35.07153915\n",
            "Iteration 481, loss = 35.02602504\n",
            "Iteration 482, loss = 35.04261184\n",
            "Iteration 483, loss = 35.04746337\n",
            "Iteration 484, loss = 34.98706481\n",
            "Iteration 485, loss = 34.97029937\n",
            "Iteration 486, loss = 34.95015618\n",
            "Iteration 487, loss = 34.94849439\n",
            "Iteration 488, loss = 34.95289481\n",
            "Iteration 489, loss = 34.91752288\n",
            "Iteration 490, loss = 34.85099261\n",
            "Iteration 491, loss = 34.93735057\n",
            "Iteration 492, loss = 34.82215853\n",
            "Iteration 493, loss = 34.84774800\n",
            "Iteration 494, loss = 34.84376833\n",
            "Iteration 495, loss = 34.78473237\n",
            "Iteration 496, loss = 34.74078424\n",
            "Iteration 497, loss = 34.75744711\n",
            "Iteration 498, loss = 34.88900048\n",
            "Iteration 499, loss = 34.73645993\n",
            "Iteration 500, loss = 34.66540263\n",
            "Iteration 501, loss = 34.72146815\n",
            "Iteration 502, loss = 34.74819105\n",
            "Iteration 503, loss = 34.65900522\n",
            "Iteration 504, loss = 34.66665582\n",
            "Iteration 505, loss = 34.80277716\n",
            "Iteration 506, loss = 34.54237315\n",
            "Iteration 507, loss = 34.68051510\n",
            "Iteration 508, loss = 34.75706472\n",
            "Iteration 509, loss = 34.73008274\n",
            "Iteration 510, loss = 34.53683419\n",
            "Iteration 511, loss = 34.67564058\n",
            "Iteration 512, loss = 34.50393255\n",
            "Iteration 513, loss = 34.53445660\n",
            "Iteration 514, loss = 34.48271554\n",
            "Iteration 515, loss = 34.46006692\n",
            "Iteration 516, loss = 34.35796802\n",
            "Iteration 517, loss = 34.35139631\n",
            "Iteration 518, loss = 34.34410534\n",
            "Iteration 519, loss = 34.35035480\n",
            "Iteration 520, loss = 34.31737031\n",
            "Iteration 521, loss = 34.27620976\n",
            "Iteration 522, loss = 34.29972300\n",
            "Iteration 523, loss = 34.23464500\n",
            "Iteration 524, loss = 34.42850469\n",
            "Iteration 525, loss = 34.27405448\n",
            "Iteration 526, loss = 34.15033794\n",
            "Iteration 527, loss = 34.12833435\n",
            "Iteration 528, loss = 34.26029532\n",
            "Iteration 529, loss = 34.26613558\n",
            "Iteration 530, loss = 34.12991312\n",
            "Iteration 531, loss = 34.06942469\n",
            "Iteration 532, loss = 34.20010603\n",
            "Iteration 533, loss = 34.09402932\n",
            "Iteration 534, loss = 34.02563591\n",
            "Iteration 535, loss = 34.00161365\n",
            "Iteration 536, loss = 34.05231205\n",
            "Iteration 537, loss = 34.21676194\n",
            "Iteration 538, loss = 34.00769416\n",
            "Iteration 539, loss = 34.06660805\n",
            "Iteration 540, loss = 34.01105771\n",
            "Iteration 541, loss = 33.99541363\n",
            "Iteration 542, loss = 33.91224097\n",
            "Iteration 543, loss = 33.91631233\n",
            "Iteration 544, loss = 33.85828037\n",
            "Iteration 545, loss = 33.85591868\n",
            "Iteration 546, loss = 33.86413304\n",
            "Iteration 547, loss = 33.79631007\n",
            "Iteration 548, loss = 33.83433188\n",
            "Iteration 549, loss = 33.82549713\n",
            "Iteration 550, loss = 33.69588682\n",
            "Iteration 551, loss = 33.72702756\n",
            "Iteration 552, loss = 33.88262361\n",
            "Iteration 553, loss = 33.98931904\n",
            "Iteration 554, loss = 33.85665316\n",
            "Iteration 555, loss = 33.82549317\n",
            "Iteration 556, loss = 33.64822040\n",
            "Iteration 557, loss = 33.70641634\n",
            "Iteration 558, loss = 33.62607199\n",
            "Iteration 559, loss = 33.59738873\n",
            "Iteration 560, loss = 33.77193320\n",
            "Iteration 561, loss = 33.60904623\n",
            "Iteration 562, loss = 33.51027094\n",
            "Iteration 563, loss = 33.56245333\n",
            "Iteration 564, loss = 33.78416709\n",
            "Iteration 565, loss = 33.51943386\n",
            "Iteration 566, loss = 33.48154748\n",
            "Iteration 567, loss = 33.53808114\n",
            "Iteration 568, loss = 33.56643951\n",
            "Iteration 569, loss = 33.54357466\n",
            "Iteration 570, loss = 33.49229096\n",
            "Iteration 571, loss = 33.42495911\n",
            "Iteration 572, loss = 33.37983998\n",
            "Iteration 573, loss = 33.40647473\n",
            "Iteration 574, loss = 33.41312620\n",
            "Iteration 575, loss = 33.50283504\n",
            "Iteration 576, loss = 33.39810986\n",
            "Iteration 577, loss = 33.42505154\n",
            "Iteration 578, loss = 33.43661132\n",
            "Iteration 579, loss = 33.35173804\n",
            "Iteration 580, loss = 33.35896982\n",
            "Iteration 581, loss = 33.42810958\n",
            "Iteration 582, loss = 33.32114359\n",
            "Iteration 583, loss = 33.37346803\n",
            "Iteration 584, loss = 33.25585416\n",
            "Iteration 585, loss = 33.22579038\n",
            "Iteration 586, loss = 33.30524580\n",
            "Iteration 587, loss = 33.32263117\n",
            "Iteration 588, loss = 33.26306681\n",
            "Iteration 589, loss = 33.17203951\n",
            "Iteration 590, loss = 33.13810006\n",
            "Iteration 591, loss = 33.23492579\n",
            "Iteration 592, loss = 33.29093290\n",
            "Iteration 593, loss = 33.08460603\n",
            "Iteration 594, loss = 33.07628933\n",
            "Iteration 595, loss = 33.21163993\n",
            "Iteration 596, loss = 32.97591292\n",
            "Iteration 597, loss = 33.02711695\n",
            "Iteration 598, loss = 33.14159958\n",
            "Iteration 599, loss = 33.09304601\n",
            "Iteration 600, loss = 32.96881581\n",
            "Iteration 601, loss = 32.91954080\n",
            "Iteration 602, loss = 32.93618402\n",
            "Iteration 603, loss = 33.02090379\n",
            "Iteration 604, loss = 32.88501053\n",
            "Iteration 605, loss = 32.83945853\n",
            "Iteration 606, loss = 32.98032261\n",
            "Iteration 607, loss = 33.02205504\n",
            "Iteration 608, loss = 32.93844106\n",
            "Iteration 609, loss = 32.83268173\n",
            "Iteration 610, loss = 32.76743942\n",
            "Iteration 611, loss = 32.78879244\n",
            "Iteration 612, loss = 32.88719207\n",
            "Iteration 613, loss = 32.82732417\n",
            "Iteration 614, loss = 32.80400483\n",
            "Iteration 615, loss = 32.82412544\n",
            "Iteration 616, loss = 32.88473614\n",
            "Iteration 617, loss = 32.81102157\n",
            "Iteration 618, loss = 32.64661795\n",
            "Iteration 619, loss = 32.62646930\n",
            "Iteration 620, loss = 32.95142561\n",
            "Iteration 621, loss = 32.94966914\n",
            "Iteration 622, loss = 32.94072370\n",
            "Iteration 623, loss = 32.60072287\n",
            "Iteration 624, loss = 32.70893550\n",
            "Iteration 625, loss = 32.70472033\n",
            "Iteration 626, loss = 32.73044133\n",
            "Iteration 627, loss = 32.62806338\n",
            "Iteration 628, loss = 32.39851914\n",
            "Iteration 629, loss = 32.82838191\n",
            "Iteration 630, loss = 33.05444945\n",
            "Iteration 631, loss = 32.80673100\n",
            "Iteration 632, loss = 32.45199310\n",
            "Iteration 633, loss = 32.63938011\n",
            "Iteration 634, loss = 32.77120781\n",
            "Iteration 635, loss = 32.65553060\n",
            "Iteration 636, loss = 32.38691847\n",
            "Iteration 637, loss = 32.81690675\n",
            "Iteration 638, loss = 32.75122133\n",
            "Iteration 639, loss = 32.53063614\n",
            "Iteration 640, loss = 32.37100638\n",
            "Iteration 641, loss = 32.32301848\n",
            "Iteration 642, loss = 32.50627891\n",
            "Iteration 643, loss = 32.54286575\n",
            "Iteration 644, loss = 32.37217047\n",
            "Iteration 645, loss = 32.31128120\n",
            "Iteration 646, loss = 32.34911837\n",
            "Iteration 647, loss = 32.35546315\n",
            "Iteration 648, loss = 32.29039595\n",
            "Iteration 649, loss = 32.28949779\n",
            "Iteration 650, loss = 32.29114811\n",
            "Iteration 651, loss = 32.25568171\n",
            "Iteration 652, loss = 32.20206862\n",
            "Iteration 653, loss = 32.17318397\n",
            "Iteration 654, loss = 32.15842722\n",
            "Iteration 655, loss = 32.18035805\n",
            "Iteration 656, loss = 32.15526567\n",
            "Iteration 657, loss = 32.13383378\n",
            "Iteration 658, loss = 32.14677267\n",
            "Iteration 659, loss = 32.28105589\n",
            "Iteration 660, loss = 32.21201978\n",
            "Iteration 661, loss = 32.10223163\n",
            "Iteration 662, loss = 32.10717680\n",
            "Iteration 663, loss = 32.06595901\n",
            "Iteration 664, loss = 32.08985775\n",
            "Iteration 665, loss = 32.08677829\n",
            "Iteration 666, loss = 32.00366873\n",
            "Iteration 667, loss = 32.01664760\n",
            "Iteration 668, loss = 32.06823614\n",
            "Iteration 669, loss = 31.97841520\n",
            "Iteration 670, loss = 32.17463242\n",
            "Iteration 671, loss = 31.95572144\n",
            "Iteration 672, loss = 31.88678038\n",
            "Iteration 673, loss = 31.92362527\n",
            "Iteration 674, loss = 31.94144792\n",
            "Iteration 675, loss = 31.97729989\n",
            "Iteration 676, loss = 31.82617880\n",
            "Iteration 677, loss = 31.84751896\n",
            "Iteration 678, loss = 31.94153944\n",
            "Iteration 679, loss = 31.99002964\n",
            "Iteration 680, loss = 31.94673832\n",
            "Iteration 681, loss = 31.85474816\n",
            "Iteration 682, loss = 31.79130446\n",
            "Iteration 683, loss = 31.80575880\n",
            "Iteration 684, loss = 31.77384073\n",
            "Iteration 685, loss = 31.75873780\n",
            "Iteration 686, loss = 31.78326953\n",
            "Iteration 687, loss = 31.91903465\n",
            "Iteration 688, loss = 31.72320456\n",
            "Iteration 689, loss = 31.91085346\n",
            "Iteration 690, loss = 31.74031683\n",
            "Iteration 691, loss = 31.68978047\n",
            "Iteration 692, loss = 31.67322075\n",
            "Iteration 693, loss = 31.66418157\n",
            "Iteration 694, loss = 31.72613930\n",
            "Iteration 695, loss = 31.62391442\n",
            "Iteration 696, loss = 31.61823893\n",
            "Iteration 697, loss = 31.59802560\n",
            "Iteration 698, loss = 31.60202419\n",
            "Iteration 699, loss = 31.57055067\n",
            "Iteration 700, loss = 31.67518998\n",
            "Iteration 701, loss = 31.63088264\n",
            "Iteration 702, loss = 31.55315521\n",
            "Iteration 703, loss = 31.55254878\n",
            "Iteration 704, loss = 31.51858611\n",
            "Iteration 705, loss = 31.47935790\n",
            "Iteration 706, loss = 31.47871402\n",
            "Iteration 707, loss = 31.51647680\n",
            "Iteration 708, loss = 31.57543991\n",
            "Iteration 709, loss = 31.44754796\n",
            "Iteration 710, loss = 31.47841724\n",
            "Iteration 711, loss = 31.45795039\n",
            "Iteration 712, loss = 31.45915490\n",
            "Iteration 713, loss = 31.43217575\n",
            "Iteration 714, loss = 31.38868035\n",
            "Iteration 715, loss = 31.42702152\n",
            "Iteration 716, loss = 31.40870616\n",
            "Iteration 717, loss = 31.50492528\n",
            "Iteration 718, loss = 31.33170297\n",
            "Iteration 719, loss = 31.36671087\n",
            "Iteration 720, loss = 31.45566493\n",
            "Iteration 721, loss = 31.44410895\n",
            "Iteration 722, loss = 31.28884429\n",
            "Iteration 723, loss = 31.40297974\n",
            "Iteration 724, loss = 31.21440512\n",
            "Iteration 725, loss = 31.27820498\n",
            "Iteration 726, loss = 31.34496055\n",
            "Iteration 727, loss = 31.31784416\n",
            "Iteration 728, loss = 31.26715559\n",
            "Iteration 729, loss = 31.20614541\n",
            "Iteration 730, loss = 31.23463169\n",
            "Iteration 731, loss = 31.28690637\n",
            "Iteration 732, loss = 31.19574236\n",
            "Iteration 733, loss = 31.25260286\n",
            "Iteration 734, loss = 31.13936922\n",
            "Iteration 735, loss = 31.12363253\n",
            "Iteration 736, loss = 31.22149241\n",
            "Iteration 737, loss = 31.26250407\n",
            "Iteration 738, loss = 31.12253589\n",
            "Iteration 739, loss = 31.09669403\n",
            "Iteration 740, loss = 31.17857516\n",
            "Iteration 741, loss = 31.18636767\n",
            "Iteration 742, loss = 31.09472150\n",
            "Iteration 743, loss = 31.03747707\n",
            "Iteration 744, loss = 30.99536567\n",
            "Iteration 745, loss = 30.98827040\n",
            "Iteration 746, loss = 30.98853173\n",
            "Iteration 747, loss = 30.99346210\n",
            "Iteration 748, loss = 30.95321660\n",
            "Iteration 749, loss = 30.95840919\n",
            "Iteration 750, loss = 30.94694899\n",
            "Iteration 751, loss = 30.96100837\n",
            "Iteration 752, loss = 30.91265033\n",
            "Iteration 753, loss = 30.89285352\n",
            "Iteration 754, loss = 30.89164873\n",
            "Iteration 755, loss = 30.88951889\n",
            "Iteration 756, loss = 30.93118529\n",
            "Iteration 757, loss = 30.87954446\n",
            "Iteration 758, loss = 30.85236540\n",
            "Iteration 759, loss = 30.82337957\n",
            "Iteration 760, loss = 30.83980876\n",
            "Iteration 761, loss = 30.81383200\n",
            "Iteration 762, loss = 30.86266441\n",
            "Iteration 763, loss = 30.80784501\n",
            "Iteration 764, loss = 31.14456260\n",
            "Iteration 765, loss = 30.86491266\n",
            "Iteration 766, loss = 30.82856848\n",
            "Iteration 767, loss = 31.04453400\n",
            "Iteration 768, loss = 30.86725573\n",
            "Iteration 769, loss = 30.85272093\n",
            "Iteration 770, loss = 30.75092181\n",
            "Iteration 771, loss = 30.75980853\n",
            "Iteration 772, loss = 30.68411592\n",
            "Iteration 773, loss = 30.74160045\n",
            "Iteration 774, loss = 30.73127287\n",
            "Iteration 775, loss = 30.67682586\n",
            "Iteration 776, loss = 30.72223768\n",
            "Iteration 777, loss = 30.64955249\n",
            "Iteration 778, loss = 30.67440475\n",
            "Iteration 779, loss = 30.65749539\n",
            "Iteration 780, loss = 30.58650830\n",
            "Iteration 781, loss = 30.73762278\n",
            "Iteration 782, loss = 30.56996504\n",
            "Iteration 783, loss = 30.69793704\n",
            "Iteration 784, loss = 30.63143556\n",
            "Iteration 785, loss = 30.59173442\n",
            "Iteration 786, loss = 30.52646314\n",
            "Iteration 787, loss = 30.51147854\n",
            "Iteration 788, loss = 30.48697161\n",
            "Iteration 789, loss = 30.54266240\n",
            "Iteration 790, loss = 30.54620055\n",
            "Iteration 791, loss = 30.57362965\n",
            "Iteration 792, loss = 30.53256368\n",
            "Iteration 793, loss = 30.45046424\n",
            "Iteration 794, loss = 30.40066487\n",
            "Iteration 795, loss = 30.57899522\n",
            "Iteration 796, loss = 30.57566299\n",
            "Iteration 797, loss = 30.40218958\n",
            "Iteration 798, loss = 30.66255487\n",
            "Iteration 799, loss = 30.68082734\n",
            "Iteration 800, loss = 30.44694970\n",
            "Iteration 801, loss = 30.37176021\n",
            "Iteration 802, loss = 30.50679734\n",
            "Iteration 803, loss = 30.38339398\n",
            "Iteration 804, loss = 30.32920938\n",
            "Iteration 805, loss = 30.35855794\n",
            "Iteration 806, loss = 30.35288019\n",
            "Iteration 807, loss = 30.25536115\n",
            "Iteration 808, loss = 30.30846566\n",
            "Iteration 809, loss = 30.41301098\n",
            "Iteration 810, loss = 30.40246339\n",
            "Iteration 811, loss = 30.27808053\n",
            "Iteration 812, loss = 30.24023652\n",
            "Iteration 813, loss = 30.20525121\n",
            "Iteration 814, loss = 30.27051942\n",
            "Iteration 815, loss = 30.27078747\n",
            "Iteration 816, loss = 30.19890625\n",
            "Iteration 817, loss = 30.32204530\n",
            "Iteration 818, loss = 30.17399498\n",
            "Iteration 819, loss = 30.24512987\n",
            "Iteration 820, loss = 30.39283287\n",
            "Iteration 821, loss = 30.17707059\n",
            "Iteration 822, loss = 30.15672358\n",
            "Iteration 823, loss = 30.15606747\n",
            "Iteration 824, loss = 30.14560450\n",
            "Iteration 825, loss = 30.29303564\n",
            "Iteration 826, loss = 30.16547784\n",
            "Iteration 827, loss = 30.15674473\n",
            "Iteration 828, loss = 30.22071756\n",
            "Iteration 829, loss = 30.12854513\n",
            "Iteration 830, loss = 30.23264669\n",
            "Iteration 831, loss = 30.00978078\n",
            "Iteration 832, loss = 30.20443628\n",
            "Iteration 833, loss = 30.12078539\n",
            "Iteration 834, loss = 30.04630540\n",
            "Iteration 835, loss = 30.04197517\n",
            "Iteration 836, loss = 30.00122480\n",
            "Iteration 837, loss = 29.98971760\n",
            "Iteration 838, loss = 29.99068365\n",
            "Iteration 839, loss = 29.96715832\n",
            "Iteration 840, loss = 29.94346208\n",
            "Iteration 841, loss = 29.94621092\n",
            "Iteration 842, loss = 29.94467903\n",
            "Iteration 843, loss = 29.93453901\n",
            "Iteration 844, loss = 29.94199435\n",
            "Iteration 845, loss = 29.97705513\n",
            "Iteration 846, loss = 29.87457762\n",
            "Iteration 847, loss = 29.86186671\n",
            "Iteration 848, loss = 29.91363560\n",
            "Iteration 849, loss = 29.92076559\n",
            "Iteration 850, loss = 29.88439985\n",
            "Iteration 851, loss = 29.81231796\n",
            "Iteration 852, loss = 29.79197910\n",
            "Iteration 853, loss = 29.80909380\n",
            "Iteration 854, loss = 29.83527936\n",
            "Iteration 855, loss = 29.78297477\n",
            "Iteration 856, loss = 29.76763359\n",
            "Iteration 857, loss = 29.85684086\n",
            "Iteration 858, loss = 29.72756729\n",
            "Iteration 859, loss = 29.76859869\n",
            "Iteration 860, loss = 29.82726415\n",
            "Iteration 861, loss = 29.74711415\n",
            "Iteration 862, loss = 29.75176492\n",
            "Iteration 863, loss = 29.73739869\n",
            "Iteration 864, loss = 29.74135775\n",
            "Iteration 865, loss = 29.73289182\n",
            "Iteration 866, loss = 29.67163188\n",
            "Iteration 867, loss = 29.63891953\n",
            "Iteration 868, loss = 29.90952071\n",
            "Iteration 869, loss = 29.57931366\n",
            "Iteration 870, loss = 29.81947673\n",
            "Iteration 871, loss = 29.96051492\n",
            "Iteration 872, loss = 29.93916330\n",
            "Iteration 873, loss = 29.58653967\n",
            "Iteration 874, loss = 29.67229681\n",
            "Iteration 875, loss = 29.79829111\n",
            "Iteration 876, loss = 29.64512939\n",
            "Iteration 877, loss = 29.61914393\n",
            "Iteration 878, loss = 29.74493832\n",
            "Iteration 879, loss = 29.65146469\n",
            "Iteration 880, loss = 29.59365150\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCE6WcqCqeW2",
        "colab_type": "code",
        "outputId": "ec1d36fe-7655-45eb-8731-58cf08adb5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(pre_first_dev)\n",
        "print(pre_first_test)\n",
        "print(pre_second_dev)\n",
        "print(pre_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[178.44935124396926, 145.1573212329774, 118.81744496060324]\n",
            "[132.91106596637502, 126.49427581568047, 106.81075073833799]\n",
            "[158.81831618574105, 167.59368397359776, 151.33936917910464]\n",
            "[125.09476674347545, 133.02428005637884, 109.60629978420262]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItUi187e0Wp",
        "colab_type": "code",
        "outputId": "c8da9def-34f4-44dd-bf59-e74ea166bca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "pre_test_mse = pd.DataFrame(list(zip(pre_first_test, pre_second_test)), columns=['PRED_REG','PRED_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "pre_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row1_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row2_col0 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >PRED_REG</th>        <th class=\"col_heading level0 col1\" >PRED_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row0_col0\" class=\"data row0 col0\" >132.911066</td>\n",
              "                        <td id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row0_col1\" class=\"data row0 col1\" >125.094767</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row1_col0\" class=\"data row1 col0\" >126.494276</td>\n",
              "                        <td id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row1_col1\" class=\"data row1 col1\" >133.024280</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row2_col0\" class=\"data row2 col0\" >106.810751</td>\n",
              "                        <td id=\"T_1a56b8a2_9ef4_11ea_9f06_0242ac1c0002row2_col1\" class=\"data row2 col1\" >109.606300</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28ef1a05f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw5A-X-ePl9z",
        "colab_type": "text"
      },
      "source": [
        "### LININT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p54ZpEZRBKFZ",
        "colab_type": "code",
        "outputId": "5a36e951-0699-46b1-f3b0-1759a6a25fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ma_train = pd.concat([female_train, mixed_train], ignore_index=True)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([female_train_pred, mixed_train_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "ma_tgt = male_tgt.copy().reset_index(drop=True)\n",
        "ma_tgt_pred = male_tgt_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([male_train, mixed_train], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([male_train_pred, mixed_train_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "fe_tgt = female_tgt.copy().reset_index(drop=True)\n",
        "fe_tgt_pred = female_tgt_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([male_train, female_train], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([male_train_pred, female_train_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "mi_tgt = mixed_tgt.copy().reset_index(drop=True)\n",
        "mi_tgt_pred = mixed_tgt_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred, ma_tgt, ma_tgt_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred, fe_tgt, fe_tgt_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred, mi_tgt, mi_tgt_pred]}\n",
        "\n",
        "lin_first_dev = []\n",
        "lin_second_dev = []\n",
        "lin_first_test = []\n",
        "lin_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  X_tgt = df[6]\n",
        "  Y_tgt = df[7]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #print(type(X_train))\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "\n",
        "  src_linr = LinearRegression() #normalize=True\n",
        "  src_linr.fit(X_train, Y_train)\n",
        "  tgt_linr = LinearRegression() #normalize=True\n",
        "  tgt_linr.fit(X_tgt, Y_tgt)\n",
        "  #print(type(src_linr.predict(X_dev)), type(tgt_linr.predict(X_dev)))\n",
        "  Y1 = src_linr.predict(X_dev)\n",
        "  Y2 = tgt_linr.predict(X_dev)\n",
        "  Y_diff = Y2-Y1\n",
        "  #w = interpolation(Y_dev, Y2, Y_diff)\n",
        "  #print(w)\n",
        "  X = pd.DataFrame(list(zip(Y1, Y_diff)), columns=['Y1', 'Y2-Y1'])\n",
        "  Y = Y_dev.copy().reset_index(drop=True)\n",
        "  reg = LinearRegression().fit(X, Y)\n",
        "  print(reg.coef_)\n",
        "  Y1_t = src_linr.predict(X_test)\n",
        "  Y2_t = tgt_linr.predict(X_test)\n",
        "  Y_diff_t = Y2_t - Y1_t\n",
        "  #print(Y_diff_t)\n",
        "  #print(w*Y_diff_t)\n",
        "  X_t = pd.DataFrame(list(zip(Y1_t, Y_diff_t)), columns=['Y1', 'Y2-Y1'])\n",
        "  Y_t = reg.predict(X_t)\n",
        "  lin_first_test.append(mean_squared_error(Y_test, Y_t))\n",
        "  #print(mean_squared_error(Y_test, Y_t))\n",
        "\n",
        "  src_mlp = MLPRegressor(activation='relu', alpha=0.0001, batch_size=128, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24)\n",
        "  src_mlp.fit(X_train, Y_train)\n",
        "  tgt_mlp = MLPRegressor(activation='identity', alpha=0.0001, batch_size=64, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24)\n",
        "  tgt_mlp.fit(X_tgt, Y_tgt)\n",
        "  #print(type(src_linr.predict(X_dev)), type(tgt_linr.predict(X_dev)))\n",
        "  Y1 = src_mlp.predict(X_dev)\n",
        "  Y2 = tgt_mlp.predict(X_dev)\n",
        "  #print(Y1, Y2)\n",
        "  Y_diff = Y2 - Y1\n",
        "  #print(Y_diff)\n",
        "  X = pd.DataFrame(list(zip(Y1, Y_diff)), columns=['Y1', 'Y2-Y1'])\n",
        "  Y = Y_dev.copy().reset_index(drop=True)\n",
        "  reg = LinearRegression().fit(X, Y)\n",
        "  print(reg.coef_)\n",
        "  Y1_t = src_mlp.predict(X_test)\n",
        "  Y2_t = tgt_mlp.predict(X_test)\n",
        "  Y_diff_t = Y2_t - Y1_t\n",
        "  #print(Y_diff_t)\n",
        "  #print(w*Y_diff_t)\n",
        "  X_t = pd.DataFrame(list(zip(Y1_t, Y_diff_t)), columns=['Y1', 'Y2-Y1'])\n",
        "  Y_t = reg.predict(X_t)\n",
        "  lin_second_test.append(mean_squared_error(Y_test, Y_t))\n",
        "  #print(mean_squared_error(Y_test, Y_hat_t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.00253779 -0.2183974 ]\n",
            "[ 0.98147893 -0.08916908]\n",
            "[ 1.1055234  -0.23520784]\n",
            "[ 1.05694119 -0.26456162]\n",
            "[1.02318455 0.41640874]\n",
            "[1.05738549 0.42004257]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrDss20TPAyi",
        "colab_type": "code",
        "outputId": "59a22986-0813-493f-9720-7bebd304f608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(lin_first_test)\n",
        "print(lin_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[98.29361651100606, 128.5344633309243, 100.09035938460765]\n",
            "[91.39444921876091, 124.0185311717055, 102.11794090647473]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkG39m3Dv4-W",
        "colab_type": "code",
        "outputId": "7d713ed1-549c-43d0-b13b-51116f94b604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "lin_test_mse = pd.DataFrame(list(zip(lin_first_test, lin_second_test)), columns=['LININT_REG','LININT_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "lin_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row1_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row2_col0 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >LININT_REG</th>        <th class=\"col_heading level0 col1\" >LININT_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row0_col0\" class=\"data row0 col0\" >98.293617</td>\n",
              "                        <td id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row0_col1\" class=\"data row0 col1\" >91.394449</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row1_col0\" class=\"data row1 col0\" >128.534463</td>\n",
              "                        <td id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row1_col1\" class=\"data row1 col1\" >124.018531</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row2_col0\" class=\"data row2 col0\" >100.090359</td>\n",
              "                        <td id=\"T_50740b6a_9ef4_11ea_9f06_0242ac1c0002row2_col1\" class=\"data row2 col1\" >102.117941</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28efb87860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJzSGAw5rCc2",
        "colab_type": "text"
      },
      "source": [
        "### Dev and Test MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn0y12unyPFQ",
        "colab_type": "code",
        "outputId": "831d2780-1ad1-4300-aa11-b5b5beb78909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "baseline_dev_mse = pd.DataFrame(list(zip(src_first_dev, src_second_dev, tgt_first_dev, tgt_second_dev, all_first_dev, all_second_dev, \n",
        "                                         wei_first_dev, wei_second_dev, pre_first_dev, pre_second_dev)), columns=['SRCONLY_REG', \n",
        "                                                                                                                  'SRCONLY_NN', 'TGTONLY REG',\n",
        "                                                                                                                  'TGTONLY NN', 'ALL_REG',\n",
        "                                                                                                                  'ALL_NN', 'WEIGHTED_REG',\n",
        "                                                                                                                  'WEIGHTED_NN', 'PRED_REG',\n",
        "                                                                                                                  'PRED_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "baseline_dev_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col5 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col6 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col6 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >SRCONLY_REG</th>        <th class=\"col_heading level0 col1\" >SRCONLY_NN</th>        <th class=\"col_heading level0 col2\" >TGTONLY REG</th>        <th class=\"col_heading level0 col3\" >TGTONLY NN</th>        <th class=\"col_heading level0 col4\" >ALL_REG</th>        <th class=\"col_heading level0 col5\" >ALL_NN</th>        <th class=\"col_heading level0 col6\" >WEIGHTED_REG</th>        <th class=\"col_heading level0 col7\" >WEIGHTED_NN</th>        <th class=\"col_heading level0 col8\" >PRED_REG</th>        <th class=\"col_heading level0 col9\" >PRED_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col0\" class=\"data row0 col0\" >113.275347</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col1\" class=\"data row0 col1\" >116.604136</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col2\" class=\"data row0 col2\" >178.449351</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col3\" class=\"data row0 col3\" >160.058774</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col4\" class=\"data row0 col4\" >113.460774</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col5\" class=\"data row0 col5\" >111.979821</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col6\" class=\"data row0 col6\" >133.431052</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col7\" class=\"data row0 col7\" >126.381217</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col8\" class=\"data row0 col8\" >178.449351</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row0_col9\" class=\"data row0 col9\" >158.818316</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col0\" class=\"data row1 col0\" >132.899382</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col1\" class=\"data row1 col1\" >154.221572</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col2\" class=\"data row1 col2\" >144.947789</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col3\" class=\"data row1 col3\" >145.845027</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col4\" class=\"data row1 col4\" >132.545860</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col5\" class=\"data row1 col5\" >137.951916</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col6\" class=\"data row1 col6\" >125.807996</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col7\" class=\"data row1 col7\" >127.726006</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col8\" class=\"data row1 col8\" >145.157321</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row1_col9\" class=\"data row1 col9\" >167.593684</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col0\" class=\"data row2 col0\" >116.861390</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col1\" class=\"data row2 col1\" >121.694158</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col2\" class=\"data row2 col2\" >119.449265</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col3\" class=\"data row2 col3\" >129.370276</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col4\" class=\"data row2 col4\" >116.453322</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col5\" class=\"data row2 col5\" >121.961310</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col6\" class=\"data row2 col6\" >108.573410</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col7\" class=\"data row2 col7\" >158.570578</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col8\" class=\"data row2 col8\" >118.817445</td>\n",
              "                        <td id=\"T_20a4d73c_9fe6_11ea_a1ad_0242ac1c0002row2_col9\" class=\"data row2 col9\" >151.339369</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ff7ad5411d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMA6qn1rF-U",
        "colab_type": "code",
        "outputId": "4baa7333-0a1b-4e83-f913-7b18e14c5ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "baseline_test_mse = pd.DataFrame(list(zip(src_first_test, src_second_test, tgt_first_test, tgt_second_test, all_first_test, all_second_test, \n",
        "                                         wei_first_test, wei_second_test, pre_first_test, pre_second_test, lin_first_test, lin_second_test)), columns=['SRCONLY_REG', \n",
        "                                                                                                                  'SRCONLY_NN', 'TGTONLY REG',\n",
        "                                                                                                                  'TGTONLY NN', 'ALL_REG',\n",
        "                                                                                                                  'ALL_NN', 'WEIGHTED_REG',\n",
        "                                                                                                                  'WEIGHTED_NN', 'PRED_REG',\n",
        "                                                                                                                  'PRED_NN', 'LININT_REG',\n",
        "                                                                                                                  'LININT_NN'])\n",
        "\n",
        "baseline_test_mse_select = pd.DataFrame(list(zip(all_first_test, all_second_test, wei_first_test, wei_second_test)), columns=['ALL_REG',\n",
        "                                                                                                                  'ALL_NN', 'WEIGHTED_REG',\n",
        "                                                                                                                  'WEIGHTED_NN'])\n",
        "\n",
        "\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "baseline_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col11 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col6 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col10 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >SRCONLY_REG</th>        <th class=\"col_heading level0 col1\" >SRCONLY_NN</th>        <th class=\"col_heading level0 col2\" >TGTONLY REG</th>        <th class=\"col_heading level0 col3\" >TGTONLY NN</th>        <th class=\"col_heading level0 col4\" >ALL_REG</th>        <th class=\"col_heading level0 col5\" >ALL_NN</th>        <th class=\"col_heading level0 col6\" >WEIGHTED_REG</th>        <th class=\"col_heading level0 col7\" >WEIGHTED_NN</th>        <th class=\"col_heading level0 col8\" >PRED_REG</th>        <th class=\"col_heading level0 col9\" >PRED_NN</th>        <th class=\"col_heading level0 col10\" >LININT_REG</th>        <th class=\"col_heading level0 col11\" >LININT_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col0\" class=\"data row0 col0\" >97.827539</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col1\" class=\"data row0 col1\" >99.073454</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col2\" class=\"data row0 col2\" >132.911066</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col3\" class=\"data row0 col3\" >121.071410</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col4\" class=\"data row0 col4\" >97.500958</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col5\" class=\"data row0 col5\" >92.235864</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col6\" class=\"data row0 col6\" >101.356653</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col7\" class=\"data row0 col7\" >107.257320</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col8\" class=\"data row0 col8\" >132.911066</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col9\" class=\"data row0 col9\" >125.094767</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col10\" class=\"data row0 col10\" >98.293617</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row0_col11\" class=\"data row0 col11\" >91.394449</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col0\" class=\"data row1 col0\" >128.922144</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col1\" class=\"data row1 col1\" >134.576572</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col2\" class=\"data row1 col2\" >126.746146</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col3\" class=\"data row1 col3\" >130.107214</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col4\" class=\"data row1 col4\" >128.544959</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col5\" class=\"data row1 col5\" >125.682735</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col6\" class=\"data row1 col6\" >117.561373</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col7\" class=\"data row1 col7\" >121.054568</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col8\" class=\"data row1 col8\" >126.494276</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col9\" class=\"data row1 col9\" >133.024280</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col10\" class=\"data row1 col10\" >128.534463</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row1_col11\" class=\"data row1 col11\" >124.018531</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col0\" class=\"data row2 col0\" >105.009285</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col1\" class=\"data row2 col1\" >105.741108</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col2\" class=\"data row2 col2\" >106.810751</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col3\" class=\"data row2 col3\" >102.258100</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col4\" class=\"data row2 col4\" >104.767162</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col5\" class=\"data row2 col5\" >101.469352</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col6\" class=\"data row2 col6\" >101.035803</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col7\" class=\"data row2 col7\" >123.463158</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col8\" class=\"data row2 col8\" >106.810751</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col9\" class=\"data row2 col9\" >109.606300</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col10\" class=\"data row2 col10\" >100.090359</td>\n",
              "                        <td id=\"T_6adc9d12_9fe6_11ea_a1ad_0242ac1c0002row2_col11\" class=\"data row2 col11\" >102.117941</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ff7aa869630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbUL3bza5zCl",
        "colab_type": "code",
        "outputId": "a38df875-6271-44b2-de92-4f26fe07a8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "print(mean(all_first_test), mean(all_second_test), mean(wei_first_test), mean(wei_second_test))\n",
        "baseline_test_mse_select.style.apply(highlight_min, axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "110.27102637602123 106.46265061491978 106.65127642004931 117.2583486452573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_626481da_9fe7_11ea_a1ad_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_626481da_9fe7_11ea_a1ad_0242ac1c0002row1_col2 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_626481da_9fe7_11ea_a1ad_0242ac1c0002row2_col2 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_REG</th>        <th class=\"col_heading level0 col1\" >ALL_NN</th>        <th class=\"col_heading level0 col2\" >WEIGHTED_REG</th>        <th class=\"col_heading level0 col3\" >WEIGHTED_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row0_col0\" class=\"data row0 col0\" >97.500958</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row0_col1\" class=\"data row0 col1\" >92.235864</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row0_col2\" class=\"data row0 col2\" >101.356653</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row0_col3\" class=\"data row0 col3\" >107.257320</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row1_col0\" class=\"data row1 col0\" >128.544959</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row1_col1\" class=\"data row1 col1\" >125.682735</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row1_col2\" class=\"data row1 col2\" >117.561373</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row1_col3\" class=\"data row1 col3\" >121.054568</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row2_col0\" class=\"data row2 col0\" >104.767162</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row2_col1\" class=\"data row2 col1\" >101.469352</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row2_col2\" class=\"data row2 col2\" >101.035803</td>\n",
              "                        <td id=\"T_626481da_9fe7_11ea_a1ad_0242ac1c0002row2_col3\" class=\"data row2 col3\" >123.463158</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ff7aa869208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SU8tR0fqfcG",
        "colab_type": "text"
      },
      "source": [
        "### Task 1.2 (Implement FEDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET2yn-LRqhZg",
        "colab_type": "code",
        "outputId": "8708bf79-5210-43c3-8667-c9455d9de2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "h,w = male_train.shape\n",
        "zero_male = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "male_train_feda = pd.concat([male_train, male_train, zero_male, zero_male], axis = 1, ignore_index=True)\n",
        "print(male_train_feda.shape)\n",
        "h,w = male_dev.shape\n",
        "zero_male = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "male_dev_feda = pd.concat([male_dev, male_dev, zero_male, zero_male], axis = 1, ignore_index=True)\n",
        "h,w = male_test.shape\n",
        "zero_male = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "male_test_feda = pd.concat([male_test, male_test, zero_male, zero_male], axis = 1, ignore_index=True)\n",
        "h,w = male_tgt.shape\n",
        "zero_male = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "male_tgt_feda = pd.concat([male_tgt, male_tgt, zero_male, zero_male], axis = 1, ignore_index=True)\n",
        "\n",
        "h, w = female_train.shape\n",
        "zero_female = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "female_train_feda = pd.concat([female_train, zero_female, female_train, zero_female], axis = 1, ignore_index=True)\n",
        "print(female_train_feda.shape)\n",
        "h, w = female_dev.shape\n",
        "zero_female = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "female_dev_feda = pd.concat([female_dev, zero_female, female_dev, zero_female], axis = 1, ignore_index=True)\n",
        "h, w = female_test.shape\n",
        "zero_female = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "female_test_feda = pd.concat([female_test, zero_female, female_test, zero_female], axis = 1, ignore_index=True)\n",
        "h, w = female_tgt.shape\n",
        "zero_female = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "female_tgt_feda = pd.concat([female_tgt, zero_female, female_tgt, zero_female], axis = 1, ignore_index=True)\n",
        "\n",
        "h, w = mixed_train.shape\n",
        "zero_mixed = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "mixed_train_feda = pd.concat([mixed_train, zero_mixed, zero_mixed, mixed_train], axis = 1, ignore_index=True)\n",
        "print(mixed_train_feda.shape)\n",
        "h, w = mixed_dev.shape\n",
        "zero_mixed = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "mixed_dev_feda = pd.concat([mixed_dev, zero_mixed, zero_mixed, mixed_dev], axis = 1, ignore_index=True)\n",
        "h, w = mixed_test.shape\n",
        "zero_mixed = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "mixed_test_feda = pd.concat([mixed_test, zero_mixed, zero_mixed, mixed_test], axis = 1, ignore_index=True)\n",
        "h, w = mixed_tgt.shape\n",
        "zero_mixed = pd.DataFrame(0, index=range(h), columns=range(w))\n",
        "mixed_tgt_feda = pd.concat([mixed_tgt, zero_mixed, zero_mixed, mixed_tgt], axis = 1, ignore_index=True)\n",
        "\n",
        "#print(mixed_feda.tail(20))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3454, 88)\n",
            "(4204, 88)\n",
            "(7089, 88)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ddsR-PuihJb",
        "colab_type": "code",
        "outputId": "1a9656dc-4a16-4878-a38f-3ef8ade1ed17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(male_tgt_feda.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 88)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODnJFyCGxs0",
        "colab_type": "text"
      },
      "source": [
        "### ALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKFOO8O7Gfzl",
        "colab_type": "code",
        "outputId": "ff803b7f-fcdf-4399-eb74-1ff04b5134a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#100\n",
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 200, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 200, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 64, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "\n",
        "#300\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 200, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 64, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'hidden_layer_sizes': (30, 30, 30), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "\n",
        "#500\n",
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 200, 'hidden_layer_sizes': (30, 30, 30), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 200, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 128, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "\n",
        "#700\n",
        "#{'activation': 'relu', 'alpha': 1, 'batch_size': 128, 'hidden_layer_sizes': (30, 30, 30), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 200, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 128, 'hidden_layer_sizes': (30, 30, 30), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "\n",
        "#900\n",
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 200, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.01, 'batch_size': 200, 'hidden_layer_sizes': (30, 30, 30), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "#{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
        "\n",
        "ma_train = pd.concat([female_train_feda, mixed_train_feda, male_tgt_feda], ignore_index=True)\n",
        "ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev_feda.copy().reset_index(drop=True)\n",
        "ma_test = male_test_feda.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([female_train_pred, mixed_train_pred, male_tgt_pred], ignore_index=True)\n",
        "ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([male_train_feda, mixed_train_feda, female_tgt_feda], ignore_index=True)\n",
        "fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev_feda.copy().reset_index(drop=True)\n",
        "fe_test = female_test_feda.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([male_train_pred, mixed_train_pred, female_tgt_pred], ignore_index=True)\n",
        "fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([male_train_feda, female_train_feda, mixed_tgt_feda], ignore_index=True)\n",
        "mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev_feda.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test_feda.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([male_train_pred, female_train_pred, mixed_tgt_pred], ignore_index=True)\n",
        "mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred]}\n",
        "\n",
        "allfeda_first_dev_100 = []\n",
        "allfeda_second_dev_100 = []\n",
        "allfeda_first_test_100 = []\n",
        "allfeda_second_test_100 = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler.fit(X_train)\n",
        "  #X_train = scaler.transform(X_train)\n",
        "  #X_dev = scaler.transform(X_dev)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "  #parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "  #xgb = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "  #xgb.fit(X_train, Y_train)\n",
        "  linr = LinearRegression() #normalize=True\n",
        "  linr.fit(X_train, Y_train)\n",
        "  allfeda_first_dev_100.append(mean_squared_error(Y_dev, linr.predict(X_dev)))\n",
        "  allfeda_first_test_100.append(mean_squared_error(Y_test, linr.predict(X_test)))\n",
        "  \n",
        "  mlp = MLPRegressor(activation='relu', alpha=1, batch_size=200, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "  mlp.fit(X_train, Y_train)\n",
        "  #mlp = MLPRegressor(random_state=24)\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30), (25,25,25,25)], 'activation':['relu', 'identity'], 'alpha':[0.0001, 0.01, 0.1, 1, 10], 'batch_size':[200, 128, 64, 32], 'max_iter':[1000], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,), (50,50), (30,30,30)], 'alpha':[0.0001, 0.01, 1], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200, 128, 64], 'learning_rate_init':[0.001, 0.01, 0.1]}\n",
        "  #parameters = {'hidden_layer_sizes':[(100,)], 'alpha':[0.0001], 'activation':['relu'], 'max_iter':[1000], 'batch_size':[200], 'learning_rate_init':[0.001, 0.01]}\n",
        "  #mse = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False\n",
        "  #clf = GridSearchCV(mlp, parameters, scoring = 'neg_mean_squared_error', n_jobs = -1)#, verbose=True, scoring = mse)\n",
        "  #clf.fit(X_train, Y_train)\n",
        "  #print(clf.best_params_)\n",
        "  allfeda_second_dev_100.append(mean_squared_error(Y_dev, mlp.predict(X_dev)))\n",
        "  allfeda_second_test_100.append(mean_squared_error(Y_test, mlp.predict(X_test)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 123.43598851\n",
            "Iteration 2, loss = 75.20827369\n",
            "Iteration 3, loss = 70.70477638\n",
            "Iteration 4, loss = 67.51675552\n",
            "Iteration 5, loss = 64.65672063\n",
            "Iteration 6, loss = 62.09307308\n",
            "Iteration 7, loss = 59.82130659\n",
            "Iteration 8, loss = 58.10812041\n",
            "Iteration 9, loss = 56.70224104\n",
            "Iteration 10, loss = 55.71267155\n",
            "Iteration 11, loss = 55.14554993\n",
            "Iteration 12, loss = 54.75312764\n",
            "Iteration 13, loss = 54.30578141\n",
            "Iteration 14, loss = 54.25111689\n",
            "Iteration 15, loss = 54.24012464\n",
            "Iteration 16, loss = 53.69347736\n",
            "Iteration 17, loss = 53.48311526\n",
            "Iteration 18, loss = 53.64370570\n",
            "Iteration 19, loss = 53.14612845\n",
            "Iteration 20, loss = 52.94441937\n",
            "Iteration 21, loss = 53.07063468\n",
            "Iteration 22, loss = 52.90928687\n",
            "Iteration 23, loss = 52.76782654\n",
            "Iteration 24, loss = 52.84212375\n",
            "Iteration 25, loss = 52.85212785\n",
            "Iteration 26, loss = 52.54980528\n",
            "Iteration 27, loss = 52.28706860\n",
            "Iteration 28, loss = 52.38476407\n",
            "Iteration 29, loss = 52.22852872\n",
            "Iteration 30, loss = 52.11088839\n",
            "Iteration 31, loss = 52.15342742\n",
            "Iteration 32, loss = 52.23847923\n",
            "Iteration 33, loss = 52.32226507\n",
            "Iteration 34, loss = 52.18735835\n",
            "Iteration 35, loss = 52.31873790\n",
            "Iteration 36, loss = 52.13361263\n",
            "Iteration 37, loss = 51.93982645\n",
            "Iteration 38, loss = 51.97620016\n",
            "Iteration 39, loss = 52.03375382\n",
            "Iteration 40, loss = 51.80403394\n",
            "Iteration 41, loss = 52.08763697\n",
            "Iteration 42, loss = 51.76991359\n",
            "Iteration 43, loss = 51.84690800\n",
            "Iteration 44, loss = 51.80580451\n",
            "Iteration 45, loss = 51.62782555\n",
            "Iteration 46, loss = 51.71140988\n",
            "Iteration 47, loss = 51.60770246\n",
            "Iteration 48, loss = 51.54298379\n",
            "Iteration 49, loss = 51.54266278\n",
            "Iteration 50, loss = 51.72500289\n",
            "Iteration 51, loss = 51.65023202\n",
            "Iteration 52, loss = 51.59552387\n",
            "Iteration 53, loss = 51.62521354\n",
            "Iteration 54, loss = 51.49862186\n",
            "Iteration 55, loss = 51.56705928\n",
            "Iteration 56, loss = 51.38523259\n",
            "Iteration 57, loss = 51.32427204\n",
            "Iteration 58, loss = 51.46168435\n",
            "Iteration 59, loss = 51.43386361\n",
            "Iteration 60, loss = 51.32150897\n",
            "Iteration 61, loss = 51.27342706\n",
            "Iteration 62, loss = 51.26122033\n",
            "Iteration 63, loss = 51.41085818\n",
            "Iteration 64, loss = 51.51330207\n",
            "Iteration 65, loss = 51.31912772\n",
            "Iteration 66, loss = 51.30315350\n",
            "Iteration 67, loss = 51.21029842\n",
            "Iteration 68, loss = 51.46658362\n",
            "Iteration 69, loss = 51.35594955\n",
            "Iteration 70, loss = 51.41487018\n",
            "Iteration 71, loss = 51.36177544\n",
            "Iteration 72, loss = 51.31349111\n",
            "Iteration 73, loss = 51.24605022\n",
            "Iteration 74, loss = 51.13352570\n",
            "Iteration 75, loss = 51.06667634\n",
            "Iteration 76, loss = 51.12970909\n",
            "Iteration 77, loss = 51.09959120\n",
            "Iteration 78, loss = 51.21694934\n",
            "Iteration 79, loss = 51.15596610\n",
            "Iteration 80, loss = 51.49579342\n",
            "Iteration 81, loss = 51.13640645\n",
            "Iteration 82, loss = 51.03362866\n",
            "Iteration 83, loss = 51.07551387\n",
            "Iteration 84, loss = 51.04612503\n",
            "Iteration 85, loss = 51.07471475\n",
            "Iteration 86, loss = 51.10408061\n",
            "Iteration 87, loss = 50.90670466\n",
            "Iteration 88, loss = 51.35735257\n",
            "Iteration 89, loss = 51.07456486\n",
            "Iteration 90, loss = 51.04158993\n",
            "Iteration 91, loss = 51.04275311\n",
            "Iteration 92, loss = 51.19838353\n",
            "Iteration 93, loss = 51.03783332\n",
            "Iteration 94, loss = 50.84982184\n",
            "Iteration 95, loss = 50.86601726\n",
            "Iteration 96, loss = 50.97568540\n",
            "Iteration 97, loss = 50.83444606\n",
            "Iteration 98, loss = 50.72473724\n",
            "Iteration 99, loss = 50.83983672\n",
            "Iteration 100, loss = 50.93564694\n",
            "Iteration 101, loss = 50.68443646\n",
            "Iteration 102, loss = 50.76412620\n",
            "Iteration 103, loss = 50.67248209\n",
            "Iteration 104, loss = 50.80054005\n",
            "Iteration 105, loss = 50.87769119\n",
            "Iteration 106, loss = 50.90617295\n",
            "Iteration 107, loss = 50.75419699\n",
            "Iteration 108, loss = 50.70777252\n",
            "Iteration 109, loss = 50.63453021\n",
            "Iteration 110, loss = 50.52387321\n",
            "Iteration 111, loss = 50.81747072\n",
            "Iteration 112, loss = 50.88626783\n",
            "Iteration 113, loss = 50.60661356\n",
            "Iteration 114, loss = 50.53277420\n",
            "Iteration 115, loss = 50.52684480\n",
            "Iteration 116, loss = 50.66684468\n",
            "Iteration 117, loss = 50.97488250\n",
            "Iteration 118, loss = 50.63679849\n",
            "Iteration 119, loss = 50.75244963\n",
            "Iteration 120, loss = 50.77256299\n",
            "Iteration 121, loss = 50.56627043\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 121.83368000\n",
            "Iteration 2, loss = 74.56793869\n",
            "Iteration 3, loss = 70.97803141\n",
            "Iteration 4, loss = 68.40417446\n",
            "Iteration 5, loss = 65.95890288\n",
            "Iteration 6, loss = 63.50257905\n",
            "Iteration 7, loss = 61.48396516\n",
            "Iteration 8, loss = 59.88159002\n",
            "Iteration 9, loss = 58.00309551\n",
            "Iteration 10, loss = 57.01356255\n",
            "Iteration 11, loss = 56.44691662\n",
            "Iteration 12, loss = 56.05215282\n",
            "Iteration 13, loss = 55.67832252\n",
            "Iteration 14, loss = 55.53692417\n",
            "Iteration 15, loss = 55.57047841\n",
            "Iteration 16, loss = 55.32254058\n",
            "Iteration 17, loss = 55.07332951\n",
            "Iteration 18, loss = 54.70138404\n",
            "Iteration 19, loss = 54.62611062\n",
            "Iteration 20, loss = 54.52195941\n",
            "Iteration 21, loss = 54.56662169\n",
            "Iteration 22, loss = 54.30569009\n",
            "Iteration 23, loss = 54.15280906\n",
            "Iteration 24, loss = 54.08124875\n",
            "Iteration 25, loss = 54.05163834\n",
            "Iteration 26, loss = 54.25694082\n",
            "Iteration 27, loss = 53.84068688\n",
            "Iteration 28, loss = 53.87454114\n",
            "Iteration 29, loss = 53.76130127\n",
            "Iteration 30, loss = 53.76037197\n",
            "Iteration 31, loss = 53.83979361\n",
            "Iteration 32, loss = 53.72703031\n",
            "Iteration 33, loss = 53.45612119\n",
            "Iteration 34, loss = 53.39851487\n",
            "Iteration 35, loss = 53.27766107\n",
            "Iteration 36, loss = 53.35074421\n",
            "Iteration 37, loss = 53.29375030\n",
            "Iteration 38, loss = 53.29667936\n",
            "Iteration 39, loss = 53.36487706\n",
            "Iteration 40, loss = 53.60218086\n",
            "Iteration 41, loss = 53.14862583\n",
            "Iteration 42, loss = 53.32508677\n",
            "Iteration 43, loss = 53.22639995\n",
            "Iteration 44, loss = 53.10453512\n",
            "Iteration 45, loss = 53.08942890\n",
            "Iteration 46, loss = 53.16720526\n",
            "Iteration 47, loss = 53.11189097\n",
            "Iteration 48, loss = 53.26556772\n",
            "Iteration 49, loss = 53.10923146\n",
            "Iteration 50, loss = 52.87209254\n",
            "Iteration 51, loss = 52.80074217\n",
            "Iteration 52, loss = 52.70242860\n",
            "Iteration 53, loss = 53.26452518\n",
            "Iteration 54, loss = 52.73195563\n",
            "Iteration 55, loss = 52.78777339\n",
            "Iteration 56, loss = 52.99071856\n",
            "Iteration 57, loss = 52.78655133\n",
            "Iteration 58, loss = 52.99406906\n",
            "Iteration 59, loss = 52.94282084\n",
            "Iteration 60, loss = 52.92908075\n",
            "Iteration 61, loss = 52.81410606\n",
            "Iteration 62, loss = 52.89650816\n",
            "Iteration 63, loss = 53.00784652\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 156.93010563\n",
            "Iteration 2, loss = 85.39906053\n",
            "Iteration 3, loss = 79.09978370\n",
            "Iteration 4, loss = 76.09120344\n",
            "Iteration 5, loss = 73.95243079\n",
            "Iteration 6, loss = 71.93589923\n",
            "Iteration 7, loss = 69.88393932\n",
            "Iteration 8, loss = 67.95273663\n",
            "Iteration 9, loss = 66.25424454\n",
            "Iteration 10, loss = 64.54730489\n",
            "Iteration 11, loss = 62.81453338\n",
            "Iteration 12, loss = 61.75187181\n",
            "Iteration 13, loss = 60.50123656\n",
            "Iteration 14, loss = 59.71720339\n",
            "Iteration 15, loss = 59.02881748\n",
            "Iteration 16, loss = 58.71835810\n",
            "Iteration 17, loss = 58.14105483\n",
            "Iteration 18, loss = 57.88479032\n",
            "Iteration 19, loss = 57.63109775\n",
            "Iteration 20, loss = 57.39782087\n",
            "Iteration 21, loss = 57.30036169\n",
            "Iteration 22, loss = 57.16026287\n",
            "Iteration 23, loss = 57.13278610\n",
            "Iteration 24, loss = 57.06960925\n",
            "Iteration 25, loss = 56.89990490\n",
            "Iteration 26, loss = 56.68537489\n",
            "Iteration 27, loss = 56.68312573\n",
            "Iteration 28, loss = 56.48042974\n",
            "Iteration 29, loss = 56.45711830\n",
            "Iteration 30, loss = 56.06143766\n",
            "Iteration 31, loss = 56.13892213\n",
            "Iteration 32, loss = 56.19579780\n",
            "Iteration 33, loss = 56.15423340\n",
            "Iteration 34, loss = 55.85323360\n",
            "Iteration 35, loss = 55.84222252\n",
            "Iteration 36, loss = 55.73089987\n",
            "Iteration 37, loss = 55.59998845\n",
            "Iteration 38, loss = 55.55570775\n",
            "Iteration 39, loss = 55.48718671\n",
            "Iteration 40, loss = 55.52696658\n",
            "Iteration 41, loss = 55.38680688\n",
            "Iteration 42, loss = 55.33561357\n",
            "Iteration 43, loss = 55.52446389\n",
            "Iteration 44, loss = 55.31715673\n",
            "Iteration 45, loss = 55.26731755\n",
            "Iteration 46, loss = 55.44932821\n",
            "Iteration 47, loss = 55.05664195\n",
            "Iteration 48, loss = 55.14405349\n",
            "Iteration 49, loss = 55.09438675\n",
            "Iteration 50, loss = 54.94519461\n",
            "Iteration 51, loss = 55.70172960\n",
            "Iteration 52, loss = 54.98926912\n",
            "Iteration 53, loss = 54.82364933\n",
            "Iteration 54, loss = 54.80106772\n",
            "Iteration 55, loss = 55.28198046\n",
            "Iteration 56, loss = 55.22583626\n",
            "Iteration 57, loss = 54.85031161\n",
            "Iteration 58, loss = 55.00110675\n",
            "Iteration 59, loss = 55.06293747\n",
            "Iteration 60, loss = 54.63612566\n",
            "Iteration 61, loss = 54.80728113\n",
            "Iteration 62, loss = 54.60939889\n",
            "Iteration 63, loss = 54.55480747\n",
            "Iteration 64, loss = 54.52486688\n",
            "Iteration 65, loss = 54.50022692\n",
            "Iteration 66, loss = 54.46507974\n",
            "Iteration 67, loss = 54.39076832\n",
            "Iteration 68, loss = 54.35582155\n",
            "Iteration 69, loss = 54.29802001\n",
            "Iteration 70, loss = 54.31865792\n",
            "Iteration 71, loss = 54.35123607\n",
            "Iteration 72, loss = 54.20289428\n",
            "Iteration 73, loss = 54.20697893\n",
            "Iteration 74, loss = 54.34564009\n",
            "Iteration 75, loss = 54.37954337\n",
            "Iteration 76, loss = 54.04293963\n",
            "Iteration 77, loss = 54.14790534\n",
            "Iteration 78, loss = 54.20483129\n",
            "Iteration 79, loss = 54.00422736\n",
            "Iteration 80, loss = 54.08977234\n",
            "Iteration 81, loss = 53.94788120\n",
            "Iteration 82, loss = 53.93230931\n",
            "Iteration 83, loss = 54.53486956\n",
            "Iteration 84, loss = 54.15068804\n",
            "Iteration 85, loss = 54.43802665\n",
            "Iteration 86, loss = 53.95797955\n",
            "Iteration 87, loss = 53.96578369\n",
            "Iteration 88, loss = 53.89890580\n",
            "Iteration 89, loss = 54.04828127\n",
            "Iteration 90, loss = 53.84999175\n",
            "Iteration 91, loss = 53.95604301\n",
            "Iteration 92, loss = 54.06441415\n",
            "Iteration 93, loss = 53.80473321\n",
            "Iteration 94, loss = 53.74787187\n",
            "Iteration 95, loss = 53.76035775\n",
            "Iteration 96, loss = 54.20178683\n",
            "Iteration 97, loss = 53.92619943\n",
            "Iteration 98, loss = 53.78001145\n",
            "Iteration 99, loss = 53.91915051\n",
            "Iteration 100, loss = 53.77930573\n",
            "Iteration 101, loss = 53.81201979\n",
            "Iteration 102, loss = 53.60159735\n",
            "Iteration 103, loss = 53.61170306\n",
            "Iteration 104, loss = 53.62193392\n",
            "Iteration 105, loss = 53.72245242\n",
            "Iteration 106, loss = 53.66517873\n",
            "Iteration 107, loss = 53.72835747\n",
            "Iteration 108, loss = 53.57502603\n",
            "Iteration 109, loss = 53.74355920\n",
            "Iteration 110, loss = 53.55382124\n",
            "Iteration 111, loss = 53.86129723\n",
            "Iteration 112, loss = 53.51116710\n",
            "Iteration 113, loss = 53.72240039\n",
            "Iteration 114, loss = 53.60668128\n",
            "Iteration 115, loss = 53.69807409\n",
            "Iteration 116, loss = 53.73426924\n",
            "Iteration 117, loss = 53.66385068\n",
            "Iteration 118, loss = 53.77282686\n",
            "Iteration 119, loss = 53.46655852\n",
            "Iteration 120, loss = 53.45752332\n",
            "Iteration 121, loss = 53.34602650\n",
            "Iteration 122, loss = 53.31863740\n",
            "Iteration 123, loss = 53.34562402\n",
            "Iteration 124, loss = 53.86853123\n",
            "Iteration 125, loss = 53.53445038\n",
            "Iteration 126, loss = 53.89471165\n",
            "Iteration 127, loss = 53.32239383\n",
            "Iteration 128, loss = 53.32065772\n",
            "Iteration 129, loss = 53.79321794\n",
            "Iteration 130, loss = 53.81757363\n",
            "Iteration 131, loss = 53.42677386\n",
            "Iteration 132, loss = 53.70671197\n",
            "Iteration 133, loss = 53.62368665\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iej5aWWzVzf7",
        "colab_type": "code",
        "outputId": "5c4051ee-72ce-4b71-9431-b92dbe434dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(allfeda_first_test_100)\n",
        "print(allfeda_second_test_100)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[132.90719813156554, 126.67705614703584, 106.8107507383397]\n",
            "[106.52746594056809, 151.18696665059076, 101.79226924446758]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlWUpAfpVzwF",
        "colab_type": "code",
        "outputId": "8af255e0-e921-4f38-ce96-2a8ee5e87ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "allfeda_test_mse_100 = pd.DataFrame(list(zip(allfeda_first_test_100, allfeda_second_test_100)), columns=['ALL_FEDA_REG','ALL_FEDA_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "allfeda_test_mse_100.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_79dd39e8_a306_11ea_837c_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_79dd39e8_a306_11ea_837c_0242ac1c0002row1_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_79dd39e8_a306_11ea_837c_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_FEDA_REG</th>        <th class=\"col_heading level0 col1\" >ALL_FEDA_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002row0_col0\" class=\"data row0 col0\" >132.907198</td>\n",
              "                        <td id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002row0_col1\" class=\"data row0 col1\" >106.527466</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002row1_col0\" class=\"data row1 col0\" >126.677056</td>\n",
              "                        <td id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002row1_col1\" class=\"data row1 col1\" >151.186967</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002row2_col0\" class=\"data row2 col0\" >106.810751</td>\n",
              "                        <td id=\"T_79dd39e8_a306_11ea_837c_0242ac1c0002row2_col1\" class=\"data row2 col1\" >101.792269</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f20aaaf9898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHWjhIKrbD9R",
        "colab_type": "code",
        "outputId": "4ffafacc-cda4-40d2-ab14-e4fd1e3f9713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(allfeda_first_test_300)\n",
        "print(allfeda_second_test_300)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[94.32761900374565, 118.369102871395, 97.05846931215302]\n",
            "[93.07226816662556, 106.99407925986675, 94.87027837361717]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDUPak19bELV",
        "colab_type": "code",
        "outputId": "33b61817-5d30-4993-c1aa-7e3465de7310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "allfeda_test_mse_300 = pd.DataFrame(list(zip(allfeda_first_test_300, allfeda_second_test_300)), columns=['ALL_FEDA_REG','ALL_FEDA_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "allfeda_test_mse_300.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_b0ad2268_a305_11ea_837c_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_b0ad2268_a305_11ea_837c_0242ac1c0002row1_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_b0ad2268_a305_11ea_837c_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_FEDA_REG</th>        <th class=\"col_heading level0 col1\" >ALL_FEDA_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002row0_col0\" class=\"data row0 col0\" >94.327619</td>\n",
              "                        <td id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002row0_col1\" class=\"data row0 col1\" >93.072268</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002row1_col0\" class=\"data row1 col0\" >118.369103</td>\n",
              "                        <td id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002row1_col1\" class=\"data row1 col1\" >106.994079</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002row2_col0\" class=\"data row2 col0\" >97.058469</td>\n",
              "                        <td id=\"T_b0ad2268_a305_11ea_837c_0242ac1c0002row2_col1\" class=\"data row2 col1\" >94.870278</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f20aa80df28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2PBRnEpbE25",
        "colab_type": "code",
        "outputId": "4e4d18f8-3bc5-4c35-e265-e4a8df294059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(allfeda_first_test_500)\n",
        "print(allfeda_second_test_500)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.3379614788007, 115.8230226751933, 95.76670896552042]\n",
            "[93.7073724409872, 112.31518351953665, 93.65537818401909]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD0n73jXbE_B",
        "colab_type": "code",
        "outputId": "de56f289-c023-4319-d341-7f1b5c7141c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "allfeda_test_mse_500 = pd.DataFrame(list(zip(allfeda_first_test_500, allfeda_second_test_500)), columns=['ALL_FEDA_REG','ALL_FEDA_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "allfeda_test_mse_500.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_3c662b52_a305_11ea_837c_0242ac1c0002row0_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_3c662b52_a305_11ea_837c_0242ac1c0002row1_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_3c662b52_a305_11ea_837c_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_FEDA_REG</th>        <th class=\"col_heading level0 col1\" >ALL_FEDA_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002row0_col0\" class=\"data row0 col0\" >92.337961</td>\n",
              "                        <td id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002row0_col1\" class=\"data row0 col1\" >93.707372</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002row1_col0\" class=\"data row1 col0\" >115.823023</td>\n",
              "                        <td id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002row1_col1\" class=\"data row1 col1\" >112.315184</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002row2_col0\" class=\"data row2 col0\" >95.766709</td>\n",
              "                        <td id=\"T_3c662b52_a305_11ea_837c_0242ac1c0002row2_col1\" class=\"data row2 col1\" >93.655378</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f20ab4d1e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfvsKccbFWb",
        "colab_type": "code",
        "outputId": "6a0ccea2-b45c-4356-b521-69631a50af9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(allfeda_first_test_700)\n",
        "print(allfeda_second_test_700)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[93.12344443406728, 115.6184885666003, 95.92172751926219]\n",
            "[92.10129437233205, 100.94481316476417, 89.39373372959298]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptmeDLQYbFej",
        "colab_type": "code",
        "outputId": "637bfef7-9ef2-46af-a478-8a0f64db8c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "allfeda_test_mse_700 = pd.DataFrame(list(zip(allfeda_first_test_700, allfeda_second_test_700)), columns=['ALL_FEDA_REG','ALL_FEDA_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "allfeda_test_mse_700.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_d339fde8_a304_11ea_837c_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_d339fde8_a304_11ea_837c_0242ac1c0002row1_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_d339fde8_a304_11ea_837c_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_FEDA_REG</th>        <th class=\"col_heading level0 col1\" >ALL_FEDA_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002row0_col0\" class=\"data row0 col0\" >93.123444</td>\n",
              "                        <td id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002row0_col1\" class=\"data row0 col1\" >92.101294</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002row1_col0\" class=\"data row1 col0\" >115.618489</td>\n",
              "                        <td id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002row1_col1\" class=\"data row1 col1\" >100.944813</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002row2_col0\" class=\"data row2 col0\" >95.921728</td>\n",
              "                        <td id=\"T_d339fde8_a304_11ea_837c_0242ac1c0002row2_col1\" class=\"data row2 col1\" >89.393734</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f20aa80b2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miS6ByKcbRJH",
        "colab_type": "code",
        "outputId": "e6ec03d1-4227-4205-9a00-6d87069b6b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(allfeda_first_test_900)\n",
        "print(allfeda_second_test_900)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[91.0174345515029, 116.53247485433006, 95.77382759053945]\n",
            "[88.71900276088115, 105.62101362817866, 93.54721656650149]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2qN0QlkbRGn",
        "colab_type": "code",
        "outputId": "c0f49e63-3746-4bdd-9276-ee529235f687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "allfeda_test_mse_900 = pd.DataFrame(list(zip(allfeda_first_test_900, allfeda_second_test_900)), columns=['ALL_FEDA_REG','ALL_FEDA_NN'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "allfeda_test_mse_900.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_e5acad94_a300_11ea_837c_0242ac1c0002row0_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_e5acad94_a300_11ea_837c_0242ac1c0002row1_col1 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_e5acad94_a300_11ea_837c_0242ac1c0002row2_col1 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ALL_FEDA_REG</th>        <th class=\"col_heading level0 col1\" >ALL_FEDA_NN</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002row0_col0\" class=\"data row0 col0\" >91.017435</td>\n",
              "                        <td id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002row0_col1\" class=\"data row0 col1\" >88.719003</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002row1_col0\" class=\"data row1 col0\" >116.532475</td>\n",
              "                        <td id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002row1_col1\" class=\"data row1 col1\" >105.621014</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002row2_col0\" class=\"data row2 col0\" >95.773828</td>\n",
              "                        <td id=\"T_e5acad94_a300_11ea_837c_0242ac1c0002row2_col1\" class=\"data row2 col1\" >93.547217</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f20aab66470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHvSwZQrs30",
        "colab_type": "text"
      },
      "source": [
        "### Task 2 (Implement your own model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWLM9zY-rwzi",
        "colab_type": "code",
        "outputId": "14077911-c316-493b-d181-304ae517226f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "ma_train = pd.concat([female_train, mixed_train, male_tgt], ignore_index=True)\n",
        "#ma_train = ma_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev = male_dev.copy().reset_index(drop=True)\n",
        "ma_test = male_test.copy().reset_index(drop=True)\n",
        "ma_train_pred = pd.concat([female_train_pred, mixed_train_pred, male_tgt_pred], ignore_index=True)\n",
        "#ma_train_pred = ma_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "ma_dev_pred = male_dev_pred.copy().reset_index(drop=True)\n",
        "ma_test_pred = male_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "fe_train = pd.concat([male_train, mixed_train, female_tgt], ignore_index=True)\n",
        "#fe_train = fe_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev = female_dev.copy().reset_index(drop=True)\n",
        "fe_test = female_test.copy().reset_index(drop=True)\n",
        "fe_train_pred = pd.concat([male_train_pred, mixed_train_pred, female_tgt_pred], ignore_index=True)\n",
        "#fe_train_pred = fe_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "fe_dev_pred = female_dev_pred.copy().reset_index(drop=True)\n",
        "fe_test_pred = female_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "mi_train = pd.concat([male_train, female_train, mixed_tgt], ignore_index=True)\n",
        "#mi_train = mi_train.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev = mixed_dev.copy().reset_index(drop=True)\n",
        "mi_test = mixed_test.copy().reset_index(drop=True)\n",
        "mi_train_pred = pd.concat([male_train_pred, female_train_pred, mixed_tgt_pred], ignore_index=True)\n",
        "#mi_train_pred = mi_train_pred.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "mi_dev_pred = mixed_dev_pred.copy().reset_index(drop=True)\n",
        "mi_test_pred = mixed_test_pred.copy().reset_index(drop=True)\n",
        "\n",
        "d = {'male':[ma_train, ma_dev, ma_test, ma_train_pred, ma_dev_pred, ma_test_pred], 'female':[fe_train, fe_dev, fe_test, fe_train_pred, fe_dev_pred, fe_test_pred], 'mixed':[mi_train, mi_dev, mi_test, mi_train_pred, mi_dev_pred, mi_test_pred]}\n",
        "\n",
        "tr_first_dev = []\n",
        "tr_second_dev = []\n",
        "tr_first_test = []\n",
        "tr_second_test = []\n",
        "\n",
        "for item in d:\n",
        "  df = d[item]\n",
        "  X_train = df[0]\n",
        "  Y_train = df[3]\n",
        "  X_dev = df[1]\n",
        "  Y_dev = df[4]\n",
        "  X_test = df[2]\n",
        "  Y_test = df[5]\n",
        "  w = np.ones(len(Y_train))#/len(Y_train)\n",
        "  #print(X_train.shape)\n",
        "  n = X_train.shape[0] - 100\n",
        "  #print(n)\n",
        "  #print(w.shape)\n",
        "  N = 3\n",
        "  for i in range(N):\n",
        "    denom = np.sum(w)\n",
        "    p = np.divide(w, denom)\n",
        "    #X_train_new = X_train.copy()\n",
        "    #X_train_new['Prob'] = p\n",
        "    linr = LinearRegression()\n",
        "    linr.fit(X_train, Y_train, sample_weight=p)\n",
        "    x_s = X_train.tail(100)\n",
        "    x_t = X_train.head(n)\n",
        "    c_s = Y_train.tail(100).values\n",
        "    #print(c_s.shape)\n",
        "    c_t = Y_train.head(n).values\n",
        "    w_m = w[-100:]\n",
        "    den = np.sum(w_m)\n",
        "    #print(x.shape, c.shape, len(w_m))\n",
        "    h_s = linr.predict(x_s)\n",
        "    h_t = linr.predict(x_t)\n",
        "    diff = np.absolute(h_s-c_s)\n",
        "    num = np.dot(w_m, diff)\n",
        "    e = np.divide(num,den)\n",
        "    if e >= 0.5:\n",
        "      e = 0.5\n",
        "    if e == 0:\n",
        "      break \n",
        "    bt = np.divide(e, 1-e)\n",
        "    b = 1 / (1 + np.sqrt(2 * np.log(n) / N))\n",
        "    for j in range(n):\n",
        "      #print(j)\n",
        "      w[j] = w[j] * np.power(b, np.absolute(h_t[j] - c_t[j]))\n",
        "    for j in range(100):\n",
        "      #print(j)\n",
        "      #print(w[n+j])\n",
        "      #print(h_s[j])\n",
        "      #print(c_s[j])\n",
        "      w[n + j] = w[n + j] * np.power(bt,(-np.absolute(h_s[j] - c_s[j])))\n",
        "  print(w)\n",
        "  tr_first_dev.append(mean_squared_error(Y_dev, linr.predict(X_dev)))\n",
        "  tr_first_test.append(mean_squared_error(Y_test, linr.predict(X_test)))\n",
        "\n",
        "  w = np.ones(len(Y_train))#/len(Y_train)\n",
        "  #print(X_train.shape)\n",
        "  n = X_train.shape[0] - 100\n",
        "  #print(n)\n",
        "  #print(w.shape)\n",
        "  N = 50\n",
        "  for i in range(N):\n",
        "    denom = np.sum(w)\n",
        "    p = np.divide(w, denom)\n",
        "    #X_train_new = X_train.copy()\n",
        "    #X_train_new['Prob'] = p\n",
        "    #clf = XGBClassifier(n_estimators=30, max_depth=10, random_state=24)\n",
        "    clf = DecisionTreeClassifier(criterion=\"gini\", max_features=\"log2\", splitter=\"random\")\n",
        "    #clf = LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=24)\n",
        "    #mlp = MLPRegressor(activation='relu', alpha=0.0001, batch_size=64, hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=1000, random_state=24, verbose=True)\n",
        "    clf.fit(X_train, Y_train, sample_weight=p)\n",
        "    x_s = X_train.tail(100)\n",
        "    x_t = X_train.head(n)\n",
        "    c_s = Y_train.tail(100).values\n",
        "    #print(c_s.shape)\n",
        "    c_t = Y_train.head(n).values\n",
        "    w_m = w[-100:]\n",
        "    den = np.sum(w_m)\n",
        "    #print(x.shape, c.shape, len(w_m))\n",
        "    h_s = linr.predict(x_s)\n",
        "    h_t = linr.predict(x_t)\n",
        "    diff = np.absolute(h_s-c_s)\n",
        "    num = np.dot(w_m, diff)\n",
        "    e = np.divide(num,den)\n",
        "    if e > 0.5:\n",
        "      e = 0.5\n",
        "    if e == 0:\n",
        "      break \n",
        "    bt = np.divide(e, 1-e)\n",
        "    b = 1 / (1 + np.sqrt(2 * np.log(n) / N))\n",
        "    for j in range(n):\n",
        "      #print(j)\n",
        "      w[j] = w[j] * np.power(b, np.absolute(h_t[j] - c_t[j]))\n",
        "    for j in range(100):\n",
        "      #print(j)\n",
        "      #print(w[n+j])\n",
        "      #print(h_s[j])\n",
        "      #print(c_s[j])\n",
        "      w[n + j] = w[n + j] * np.power(bt,(-np.absolute(h_s[j] - c_s[j])))\n",
        "  print(w)\n",
        "  tr_second_dev.append(mean_squared_error(Y_dev, clf.predict(X_dev)))\n",
        "  tr_second_test.append(mean_squared_error(Y_test, clf.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.24913785e-21 5.28093887e-11 4.44134605e-03 ... 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[2.35077587e-125 5.49553952e-070 9.88772883e-023 ... 1.00000000e+000\n",
            " 1.00000000e+000 1.00000000e+000]\n",
            "[6.87229162e-13 3.72214134e-23 2.00260435e-07 ... 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[1.37017881e-092 1.25479529e-142 5.24619778e-038 ... 1.00000000e+000\n",
            " 1.00000000e+000 1.00000000e+000]\n",
            "[1.18267965e-11 1.11740106e-20 2.36174839e-08 ... 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[7.55011827e-067 1.93171782e-131 3.56147722e-044 ... 1.00000000e+000\n",
            " 1.00000000e+000 1.00000000e+000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DucLzjeSrxDg",
        "colab_type": "code",
        "outputId": "108b62e8-06aa-41eb-de45-0fa8a65b3d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(tr_first_dev)\n",
        "print(tr_first_test)\n",
        "print(tr_second_dev)\n",
        "print(tr_second_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[136.33996635508927, 124.77459261773818, 109.17517637844475]\n",
            "[101.48663209258373, 118.46322402014235, 101.96992003092492]\n",
            "[146.77, 120.89, 168.12]\n",
            "[114.42, 147.43, 121.94]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GJrPyAhY6IN",
        "colab_type": "code",
        "outputId": "52ec3781-1e79-4bbb-a4cf-82d02decf003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "tr_test_mse = pd.DataFrame(list(zip(tr_first_test, tr_second_test)), columns=['TrADABOOST_REG','TrADABOOST_DT'])\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "tr_test_mse.style.apply(highlight_min, axis=1)\n",
        "#print(baseline_dev_mse.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_30c3baf4_a223_11ea_853a_0242ac1c0002row0_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_30c3baf4_a223_11ea_853a_0242ac1c0002row1_col0 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_30c3baf4_a223_11ea_853a_0242ac1c0002row2_col0 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >TrADABOOST_REG</th>        <th class=\"col_heading level0 col1\" >TrADABOOST_DT</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002row0_col0\" class=\"data row0 col0\" >101.486632</td>\n",
              "                        <td id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002row0_col1\" class=\"data row0 col1\" >114.420000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002row1_col0\" class=\"data row1 col0\" >118.463224</td>\n",
              "                        <td id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002row1_col1\" class=\"data row1 col1\" >147.430000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002row2_col0\" class=\"data row2 col0\" >101.969920</td>\n",
              "                        <td id=\"T_30c3baf4_a223_11ea_853a_0242ac1c0002row2_col1\" class=\"data row2 col1\" >121.940000</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f6034f6ada0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I5oPXssj85E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}